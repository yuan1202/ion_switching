{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wavenet pytorch version\n",
    "herit from @ragnar123 notebook https://www.kaggle.com/ragnar123/wavenet-with-1-more-feature\n",
    "and @brandenkmurray notebook https://www.kaggle.com/brandenkmurray/seq2seq-rnn-with-gru\n",
    "I just add wavenet with pytorch version\n",
    "hope this notebook will help \n",
    "\n",
    "\n",
    "\n",
    "V4 change wavenet from sequential model to multi layer feature parallel, thanks @cdeotte point out the difference that my old verson with keras wavenet in other kernel, which has more good result.  [detail info](https://www.kaggle.com/c/liverpool-ion-switching/discussion/145256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('../input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations and main hyperparammeters\n",
    "EPOCHS = 150\n",
    "NNBATCHSIZE = 16\n",
    "GROUP_BATCH_SIZE = 4000\n",
    "SEED = 321\n",
    "LR = 0.001\n",
    "SPLITS = 5\n",
    "\n",
    "outdir = 'wavenet_models'\n",
    "flip = False\n",
    "noise = False\n",
    "\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "def read_data():\n",
    "    train = pd.read_csv('../input/train_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n",
    "    test  = pd.read_csv('../input/test_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32})\n",
    "    sub  = pd.read_csv('../input/sample_submission.csv', dtype={'time': np.float32})\n",
    "    return train, test, sub\n",
    "\n",
    "# create batches of 4000 observations\n",
    "def batching(df, batch_size):\n",
    "    #print(df)\n",
    "    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n",
    "    df['group'] = df['group'].astype(np.uint16)\n",
    "    return df\n",
    "\n",
    "# normalize the data (standard scaler). We can also try other scalers for a better score!\n",
    "def normalize(train, test):\n",
    "    train_input_mean = train.signal.mean()\n",
    "    train_input_sigma = train.signal.std()\n",
    "    train['signal'] = (train.signal - train_input_mean) / train_input_sigma\n",
    "    test['signal'] = (test.signal - train_input_mean) / train_input_sigma\n",
    "    return train, test\n",
    "\n",
    "# get lead and lags features\n",
    "def lag_with_pct_change(df, windows):\n",
    "    for window in windows:    \n",
    "        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n",
    "        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n",
    "    return df\n",
    "\n",
    "# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\n",
    "def run_feat_engineering(df, batch_size):\n",
    "    # create batches\n",
    "    df = batching(df, batch_size = batch_size)\n",
    "    # create leads and lags (1, 2, 3 making them 6 features)\n",
    "    df = lag_with_pct_change(df, [1, 2, 3])\n",
    "    # create signal ** 2 (this is the new feature)\n",
    "    df['signal_2'] = df['signal'] ** 2\n",
    "    return df\n",
    "\n",
    "# fillna with the mean and select features for training\n",
    "def feature_selection(train, test):\n",
    "    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "    test = test.replace([np.inf, -np.inf], np.nan)\n",
    "    for feature in features:\n",
    "        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n",
    "        train[feature] = train[feature].fillna(feature_mean)\n",
    "        test[feature] = test[feature].fillna(feature_mean)\n",
    "    return train, test, features\n",
    "\n",
    "\n",
    "def split(GROUP_BATCH_SIZE=4000, SPLITS=5):\n",
    "    print('Reading Data Started...')\n",
    "    train, test, sample_submission = read_data()\n",
    "    train, test = normalize(train, test)\n",
    "    print('Reading and Normalizing Data Completed')\n",
    "    print('Creating Features')\n",
    "    print('Feature Engineering Started...')\n",
    "    train = run_feat_engineering(train, batch_size=GROUP_BATCH_SIZE)\n",
    "    test = run_feat_engineering(test, batch_size=GROUP_BATCH_SIZE)\n",
    "    train, test, features = feature_selection(train, test)\n",
    "    print(train.head())\n",
    "    print('Feature Engineering Completed...')\n",
    "\n",
    "    target = ['open_channels']\n",
    "    group = train['group']\n",
    "    kf = GroupKFold(n_splits=SPLITS)\n",
    "    splits = [x for x in kf.split(train, train[target], group)]\n",
    "    new_splits = []\n",
    "    for sp in splits:\n",
    "        new_split = []\n",
    "        new_split.append(np.unique(group[sp[0]]))\n",
    "        new_split.append(np.unique(group[sp[1]]))\n",
    "        new_split.append(sp[1])\n",
    "        new_splits.append(new_split)\n",
    "    target_cols = ['open_channels']\n",
    "    print(train.head(), train.shape)\n",
    "    train_tr = np.array(list(train.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n",
    "    train = np.array(list(train.groupby('group').apply(lambda x: x[features].values)))\n",
    "    test = np.array(list(test.groupby('group').apply(lambda x: x[features].values)))\n",
    "    print(train.shape, test.shape, train_tr.shape)\n",
    "    return train, test, train_tr, new_splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change wavenet model, this code is from [Understanding Ion-Switching with modeling](https://www.kaggle.com/mobassir/understanding-ion-switching-with-modeling)  and also change a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# wavenet \n",
    "# from https://www.kaggle.com/hanjoonchoe/wavenet-lstm-pytorch-ignite-ver\n",
    "class Wave_Block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, dilation_rates, kernel_size):\n",
    "        super(Wave_Block, self).__init__()\n",
    "        self.num_rates = dilation_rates\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "\n",
    "        self.convs.append(nn.Conv1d(in_channels, out_channels, kernel_size=1))\n",
    "        dilation_rates = [2 ** i for i in range(dilation_rates)]\n",
    "        for dilation_rate in dilation_rates:\n",
    "            self.filter_convs.append(\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate))\n",
    "            self.gate_convs.append(\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate))\n",
    "            self.convs.append(nn.Conv1d(out_channels, out_channels, kernel_size=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs[0](x)\n",
    "        res = x\n",
    "        for i in range(self.num_rates):\n",
    "            x = torch.tanh(self.filter_convs[i](x)) * torch.sigmoid(self.gate_convs[i](x))\n",
    "            x = self.convs[i + 1](x)\n",
    "            res = res + x\n",
    "        return res\n",
    "# detail \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, inch=8, kernel_size=3):\n",
    "        super().__init__()\n",
    "        #self.LSTM = nn.GRU(input_size=input_size, hidden_size=64, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.wave_block1 = Wave_Block(inch, 16, 12, kernel_size)\n",
    "        self.wave_block2 = Wave_Block(16, 32, 8, kernel_size)\n",
    "        self.wave_block3 = Wave_Block(32, 64, 4, kernel_size)\n",
    "        self.wave_block4 = Wave_Block(64, 128, 1, kernel_size)\n",
    "        self.fc = nn.Linear(128, 11)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.wave_block1(x)\n",
    "        x = self.wave_block2(x)\n",
    "        x = self.wave_block3(x)\n",
    "\n",
    "        x = self.wave_block4(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        #x, _ = self.LSTM(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n",
    "        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n",
    "        self.counter, self.best_score = 0, None\n",
    "        self.is_maximize = is_maximize\n",
    "\n",
    "\n",
    "    def load_best_weights(self, model):\n",
    "        model.load_state_dict(torch.load(self.checkpoint_path))\n",
    "\n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None or \\\n",
    "                (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n",
    "            torch.save(model.state_dict(), self.checkpoint_path)\n",
    "            self.best_score, self.counter = score, 0\n",
    "            return 1\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return 2\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class IronDataset(Dataset):\n",
    "    def __init__(self, data, labels, training=True, transform=None, seq_len=5000, flip=0.5, noise_level=0, class_split=0.0):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.training = training\n",
    "        self.flip = flip\n",
    "        self.noise_level = noise_level\n",
    "        self.class_split = class_split\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        data = self.data[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        return [data.astype(np.float32), labels.astype(int)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_splits[0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, _, _ = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = run_feat_engineering(train_df, batch_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>signal</th>\n",
       "      <th>open_channels</th>\n",
       "      <th>group</th>\n",
       "      <th>signal_shift_pos_1</th>\n",
       "      <th>signal_shift_neg_1</th>\n",
       "      <th>signal_shift_pos_2</th>\n",
       "      <th>signal_shift_neg_2</th>\n",
       "      <th>signal_shift_pos_3</th>\n",
       "      <th>signal_shift_neg_3</th>\n",
       "      <th>signal_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>-2.760655</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.848034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.424341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.130037</td>\n",
       "      <td>7.621218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000200</td>\n",
       "      <td>-2.848034</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.760655</td>\n",
       "      <td>-2.424341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.130037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.144854</td>\n",
       "      <td>8.111297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000300</td>\n",
       "      <td>-2.424341</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.848034</td>\n",
       "      <td>-3.130037</td>\n",
       "      <td>-2.760655</td>\n",
       "      <td>-3.144854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.649931</td>\n",
       "      <td>5.877428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000400</td>\n",
       "      <td>-3.130037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.424341</td>\n",
       "      <td>-3.144854</td>\n",
       "      <td>-2.848034</td>\n",
       "      <td>-2.649931</td>\n",
       "      <td>-2.760655</td>\n",
       "      <td>-2.697078</td>\n",
       "      <td>9.797131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000500</td>\n",
       "      <td>-3.144854</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.130037</td>\n",
       "      <td>-2.649931</td>\n",
       "      <td>-2.424341</td>\n",
       "      <td>-2.697078</td>\n",
       "      <td>-2.848034</td>\n",
       "      <td>-2.596101</td>\n",
       "      <td>9.890105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999995</th>\n",
       "      <td>499.999603</td>\n",
       "      <td>2.937281</td>\n",
       "      <td>7</td>\n",
       "      <td>1249</td>\n",
       "      <td>4.347293</td>\n",
       "      <td>2.727728</td>\n",
       "      <td>4.167348</td>\n",
       "      <td>4.506354</td>\n",
       "      <td>2.389828</td>\n",
       "      <td>5.619422</td>\n",
       "      <td>8.627621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999996</th>\n",
       "      <td>499.999695</td>\n",
       "      <td>2.727728</td>\n",
       "      <td>7</td>\n",
       "      <td>1249</td>\n",
       "      <td>2.937281</td>\n",
       "      <td>4.506354</td>\n",
       "      <td>4.347293</td>\n",
       "      <td>5.619422</td>\n",
       "      <td>4.167348</td>\n",
       "      <td>5.382750</td>\n",
       "      <td>7.440502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999997</th>\n",
       "      <td>499.999786</td>\n",
       "      <td>4.506354</td>\n",
       "      <td>8</td>\n",
       "      <td>1249</td>\n",
       "      <td>2.727728</td>\n",
       "      <td>5.619422</td>\n",
       "      <td>2.937281</td>\n",
       "      <td>5.382750</td>\n",
       "      <td>4.347293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.307224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999998</th>\n",
       "      <td>499.999908</td>\n",
       "      <td>5.619422</td>\n",
       "      <td>9</td>\n",
       "      <td>1249</td>\n",
       "      <td>4.506354</td>\n",
       "      <td>5.382750</td>\n",
       "      <td>2.727728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.937281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.577909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999999</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>5.382750</td>\n",
       "      <td>9</td>\n",
       "      <td>1249</td>\n",
       "      <td>5.619422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.506354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.727728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.973999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               time    signal  open_channels  group  signal_shift_pos_1  \\\n",
       "0          0.000100 -2.760655              0      0            0.000000   \n",
       "1          0.000200 -2.848034              0      0           -2.760655   \n",
       "2          0.000300 -2.424341              0      0           -2.848034   \n",
       "3          0.000400 -3.130037              0      0           -2.424341   \n",
       "4          0.000500 -3.144854              0      0           -3.130037   \n",
       "...             ...       ...            ...    ...                 ...   \n",
       "4999995  499.999603  2.937281              7   1249            4.347293   \n",
       "4999996  499.999695  2.727728              7   1249            2.937281   \n",
       "4999997  499.999786  4.506354              8   1249            2.727728   \n",
       "4999998  499.999908  5.619422              9   1249            4.506354   \n",
       "4999999  500.000000  5.382750              9   1249            5.619422   \n",
       "\n",
       "         signal_shift_neg_1  signal_shift_pos_2  signal_shift_neg_2  \\\n",
       "0                 -2.848034            0.000000           -2.424341   \n",
       "1                 -2.424341            0.000000           -3.130037   \n",
       "2                 -3.130037           -2.760655           -3.144854   \n",
       "3                 -3.144854           -2.848034           -2.649931   \n",
       "4                 -2.649931           -2.424341           -2.697078   \n",
       "...                     ...                 ...                 ...   \n",
       "4999995            2.727728            4.167348            4.506354   \n",
       "4999996            4.506354            4.347293            5.619422   \n",
       "4999997            5.619422            2.937281            5.382750   \n",
       "4999998            5.382750            2.727728            0.000000   \n",
       "4999999            0.000000            4.506354            0.000000   \n",
       "\n",
       "         signal_shift_pos_3  signal_shift_neg_3   signal_2  \n",
       "0                  0.000000           -3.130037   7.621218  \n",
       "1                  0.000000           -3.144854   8.111297  \n",
       "2                  0.000000           -2.649931   5.877428  \n",
       "3                 -2.760655           -2.697078   9.797131  \n",
       "4                 -2.848034           -2.596101   9.890105  \n",
       "...                     ...                 ...        ...  \n",
       "4999995            2.389828            5.619422   8.627621  \n",
       "4999996            4.167348            5.382750   7.440502  \n",
       "4999997            4.347293            0.000000  20.307224  \n",
       "4999998            2.937281            0.000000  31.577909  \n",
       "4999999            2.727728            0.000000  28.973999  \n",
       "\n",
       "[5000000 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data Started...\n",
      "Reading and Normalizing Data Completed\n",
      "Creating Features\n",
      "Feature Engineering Started...\n",
      "     time    signal  open_channels  group  signal_shift_pos_1  \\\n",
      "0  0.0001 -1.148772              0      0            0.000000   \n",
      "1  0.0002 -1.184075              0      0           -1.148772   \n",
      "2  0.0003 -1.012891              0      0           -1.184075   \n",
      "3  0.0004 -1.298012              0      0           -1.012891   \n",
      "4  0.0005 -1.303999              0      0           -1.298012   \n",
      "\n",
      "   signal_shift_neg_1  signal_shift_pos_2  signal_shift_neg_2  \\\n",
      "0           -1.184075            0.000000           -1.012891   \n",
      "1           -1.012891            0.000000           -1.298012   \n",
      "2           -1.298012           -1.148772           -1.303999   \n",
      "3           -1.303999           -1.184075           -1.104036   \n",
      "4           -1.104036           -1.012891           -1.123085   \n",
      "\n",
      "   signal_shift_pos_3  signal_shift_neg_3  signal_2  \n",
      "0            0.000000           -1.298012  1.319677  \n",
      "1            0.000000           -1.303999  1.402034  \n",
      "2            0.000000           -1.104036  1.025949  \n",
      "3           -1.148772           -1.123085  1.684836  \n",
      "4           -1.184075           -1.082287  1.700413  \n",
      "Feature Engineering Completed...\n",
      "     time    signal  open_channels  group  signal_shift_pos_1  \\\n",
      "0  0.0001 -1.148772              0      0            0.000000   \n",
      "1  0.0002 -1.184075              0      0           -1.148772   \n",
      "2  0.0003 -1.012891              0      0           -1.184075   \n",
      "3  0.0004 -1.298012              0      0           -1.012891   \n",
      "4  0.0005 -1.303999              0      0           -1.298012   \n",
      "\n",
      "   signal_shift_neg_1  signal_shift_pos_2  signal_shift_neg_2  \\\n",
      "0           -1.184075            0.000000           -1.012891   \n",
      "1           -1.012891            0.000000           -1.298012   \n",
      "2           -1.298012           -1.148772           -1.303999   \n",
      "3           -1.303999           -1.184075           -1.104036   \n",
      "4           -1.104036           -1.012891           -1.123085   \n",
      "\n",
      "   signal_shift_pos_3  signal_shift_neg_3  signal_2  \n",
      "0            0.000000           -1.298012  1.319677  \n",
      "1            0.000000           -1.303999  1.402034  \n",
      "2            0.000000           -1.104036  1.025949  \n",
      "3           -1.148772           -1.123085  1.684836  \n",
      "4           -1.184075           -1.082287  1.700413   (5000000, 11)\n",
      "(1250, 4000, 8) (500, 4000, 8) (1250, 4000, 1)\n"
     ]
    }
   ],
   "source": [
    "train, test, train_tr, new_splits = split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 4000, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_splits[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "(1000,)\n",
      "------\n",
      "(1000,)\n",
      "------\n",
      "(1000,)\n",
      "------\n",
      "(1000,)\n",
      "------\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "for index, (train_index, val_index, _) in enumerate(new_splits[0:], start=0):\n",
    "    print('------')\n",
    "    print(train_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.zeros([int(2000000/GROUP_BATCH_SIZE), GROUP_BATCH_SIZE, 1])\n",
    "test_dataset = IronDataset(test, test_y, flip=False)\n",
    "test_dataloader = DataLoader(test_dataset, NNBATCHSIZE, shuffle=False, num_workers=8, pin_memory=True)\n",
    "test_preds_all = np.zeros((2000000, 11))\n",
    "\n",
    "\n",
    "oof_score = []\n",
    "for index, (train_index, val_index, _) in enumerate(new_splits[0:], start=0):\n",
    "    print(\"Fold : {}\".format(index))\n",
    "    train_dataset = IronDataset(train[train_index], train_tr[train_index], seq_len=GROUP_BATCH_SIZE, flip=flip, noise_level=noise)\n",
    "    train_dataloader = DataLoader(train_dataset, NNBATCHSIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    valid_dataset = IronDataset(train[val_index], train_tr[val_index], seq_len=GROUP_BATCH_SIZE, flip=False)\n",
    "    valid_dataloader = DataLoader(valid_dataset, NNBATCHSIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    it = 0\n",
    "    model = Classifier()\n",
    "    model = model.cuda()\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=40, is_maximize=True,\n",
    "                                   checkpoint_path=os.path.join(outdir, \"gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index,\n",
    "                                                                                                             it)))\n",
    "\n",
    "    weight = None#cal_weights()\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "    schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.2)\n",
    "    avg_train_losses, avg_valid_losses = [], []\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('**********************************')\n",
    "        print(\"Folder : {} Epoch : {}\".format(index, epoch))\n",
    "        print(\"Curr learning_rate: {:0.9f}\".format(optimizer.param_groups[0]['lr']))\n",
    "        train_losses, valid_losses = [], []\n",
    "        tr_loss_cls_item, val_loss_cls_item = [], []\n",
    "\n",
    "        model.train()  # prep model for training\n",
    "        train_preds, train_true = torch.Tensor([]).cuda(), torch.LongTensor([]).cuda()#.to(device)\n",
    "\n",
    "        for x, y in tqdm(train_dataloader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(x)\n",
    "\n",
    "            predictions_ = predictions.view(-1, predictions.shape[-1])\n",
    "            y_ = y.view(-1)\n",
    "\n",
    "            loss = criterion(predictions_, y_)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            #schedular.step()\n",
    "            # record training lossa\n",
    "            train_losses.append(loss.item())\n",
    "            train_true = torch.cat([train_true, y_], 0)\n",
    "            train_preds = torch.cat([train_preds, predictions_], 0)\n",
    "\n",
    "        model.eval()  # prep model for evaluation\n",
    "        val_preds, val_true = torch.Tensor([]).cuda(), torch.LongTensor([]).cuda()\n",
    "        print('EVALUATION')\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(valid_dataloader):\n",
    "                x = x.cuda()#.to(device)\n",
    "                y = y.cuda()#..to(device)\n",
    "\n",
    "                predictions = model(x)\n",
    "                predictions_ = predictions.view(-1, predictions.shape[-1])\n",
    "                y_ = y.view(-1)\n",
    "\n",
    "                loss = criterion(predictions_, y_)\n",
    "\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "\n",
    "                val_true = torch.cat([val_true, y_], 0)\n",
    "                val_preds = torch.cat([val_preds, predictions_], 0)\n",
    "\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        print(\"train_loss: {:0.6f}, valid_loss: {:0.6f}\".format(train_loss, valid_loss))\n",
    "\n",
    "        train_score = f1_score(train_true.cpu().detach().numpy(), train_preds.cpu().detach().numpy().argmax(1),\n",
    "                               labels=list(range(11)), average='macro')\n",
    "\n",
    "        val_score = f1_score(val_true.cpu().detach().numpy(), val_preds.cpu().detach().numpy().argmax(1),\n",
    "                             labels=list(range(11)), average='macro')\n",
    "\n",
    "        schedular.step(val_score)\n",
    "        print(\"train_f1: {:0.6f}, valid_f1: {:0.6f}\".format(train_score, val_score))\n",
    "        res = early_stopping(val_score, model)\n",
    "        #print('fres:', res)\n",
    "        if  res == 2:\n",
    "            print(\"Early Stopping\")\n",
    "            print('folder %d global best val max f1 model score %f' % (index, early_stopping.best_score))\n",
    "            break\n",
    "        elif res == 1:\n",
    "            print('save folder %d global val max f1 model score %f' % (index, val_score))\n",
    "    print('Folder {} finally best global max f1 score is {}'.format(index, early_stopping.best_score))\n",
    "    oof_score.append(round(early_stopping.best_score, 6))\n",
    "    \n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(test_dataloader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            predictions = model(x)\n",
    "            predictions_ = predictions.view(-1, predictions.shape[-1]) # shape [128, 4000, 11]\n",
    "            #print(predictions.shape, F.softmax(predictions_, dim=1).cpu().numpy().shape)\n",
    "            pred_list.append(F.softmax(predictions_, dim=1).cpu().numpy()) # shape (512000, 11)\n",
    "            #a = input()\n",
    "        test_preds = np.vstack(pred_list) # shape [2000000, 11]\n",
    "        test_preds_all += test_preds\n",
    "print('all folder score is:%s'%str(oof_score))\n",
    "print('OOF mean score is: %f'% (sum(oof_score)/len(oof_score)))\n",
    "print('Generate submission.............')\n",
    "submission_csv_path = '/kaggle/input/liverpool-ion-switching/sample_submission.csv'\n",
    "ss = pd.read_csv(submission_csv_path, dtype={'time': str})\n",
    "test_preds_all = test_preds_all / np.sum(test_preds_all, axis=1)[:, None]\n",
    "test_pred_frame = pd.DataFrame({'time': ss['time'].astype(str),\n",
    "                                'open_channels': np.argmax(test_preds_all, axis=1)})\n",
    "test_pred_frame.to_csv(\"./gru_preds.csv\", index=False)\n",
    "print('over')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
