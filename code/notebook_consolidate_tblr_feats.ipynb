{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import math\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import bloscpack as bp\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.max_rows', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/batch_ids_trn.pkl', 'rb') as f:\n",
    "    batch_id_trn = pickle.load(f)\n",
    "with open('../input/batch_ids_tst.pkl', 'rb') as f:\n",
    "    batch_id_tst = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>entropy_w10</th>\n",
       "      <th>entropy_w50</th>\n",
       "      <th>entropy_w100</th>\n",
       "      <th>entropy_w200</th>\n",
       "      <th>entropy_w500</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">49</th>\n",
       "      <th>4999995</th>\n",
       "      <td>2.163956</td>\n",
       "      <td>2.681977</td>\n",
       "      <td>2.694502</td>\n",
       "      <td>2.719278</td>\n",
       "      <td>2.654519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999996</th>\n",
       "      <td>2.163956</td>\n",
       "      <td>2.699238</td>\n",
       "      <td>2.703133</td>\n",
       "      <td>2.719278</td>\n",
       "      <td>2.655319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999997</th>\n",
       "      <td>2.163956</td>\n",
       "      <td>2.699238</td>\n",
       "      <td>2.686203</td>\n",
       "      <td>2.719495</td>\n",
       "      <td>2.654097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999998</th>\n",
       "      <td>2.163956</td>\n",
       "      <td>2.676924</td>\n",
       "      <td>2.697208</td>\n",
       "      <td>2.719495</td>\n",
       "      <td>2.654988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999999</th>\n",
       "      <td>2.163956</td>\n",
       "      <td>2.790596</td>\n",
       "      <td>2.695194</td>\n",
       "      <td>2.722676</td>\n",
       "      <td>2.656593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               entropy_w10  entropy_w50  entropy_w100  entropy_w200  \\\n",
       "batch                                                                 \n",
       "0     0                NaN          NaN           NaN           NaN   \n",
       "      1                NaN          NaN           NaN           NaN   \n",
       "      2                NaN          NaN           NaN           NaN   \n",
       "      3                NaN          NaN           NaN           NaN   \n",
       "      4                NaN          NaN           NaN           NaN   \n",
       "...                    ...          ...           ...           ...   \n",
       "49    4999995     2.163956     2.681977      2.694502      2.719278   \n",
       "      4999996     2.163956     2.699238      2.703133      2.719278   \n",
       "      4999997     2.163956     2.699238      2.686203      2.719495   \n",
       "      4999998     2.163956     2.676924      2.697208      2.719495   \n",
       "      4999999     2.163956     2.790596      2.695194      2.722676   \n",
       "\n",
       "               entropy_w500  \n",
       "batch                        \n",
       "0     0                 NaN  \n",
       "      1                 NaN  \n",
       "      2                 NaN  \n",
       "      3                 NaN  \n",
       "      4                 NaN  \n",
       "...                     ...  \n",
       "49    4999995      2.654519  \n",
       "      4999996      2.655319  \n",
       "      4999997      2.654097  \n",
       "      4999998      2.654988  \n",
       "      4999999      2.656593  \n",
       "\n",
       "[5000000 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_entro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_origv2 = pd.read_pickle('../input/feats_tblr/trn_dat_orig_v2_all.pkl').drop(columns=['open_channels', 'time']).values\n",
    "trn_entro = pd.read_pickle('../input/trn_dat_refresh1_all.pkl').values\n",
    "trn_trgtenc = pd.read_pickle('../input/train_clean_encoded.pkl').drop(columns=['open_channels', 'time', 'signal']).values\n",
    "trn_dat = np.concatenate([trn_origv2, trn_entro, trn_trgtenc], axis=1)\n",
    "del trn_origv2, trn_entro, trn_trgtenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_origv2 = pd.read_pickle('../input/feats_tblr/tst_dat_orig_v2_all.pkl')#.drop(columns=['open_channels', 'time', 'signal']).values\n",
    "tst_entro = pd.read_pickle('../input/tst_dat_refresh1_all.pkl')#.values\n",
    "tst_trgtenc = pd.read_pickle('../input/test_clean_encoded.pkl')#.drop(columns=['time', 'signal']).values\n",
    "# origv2 + entropy + trgt enc\n",
    "tst_dat = np.concatenate([tst_origv2, tst_entro, tst_trgtenc], axis=1)\n",
    "del tst_origv2, tst_entro, tst_trgtenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_origv2['na_guide'] = trn_origv2.groupby('batch')['signal'].transform(lambda x: pd.qcut(x, q=100, labels=False, duplicates='drop').astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_guide = pd.read_pickle('../input/na_guide_signal.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          38.0\n",
       "1          24.0\n",
       "2          86.0\n",
       "3           3.0\n",
       "4           2.0\n",
       "           ... \n",
       "4999995    39.0\n",
       "4999996    34.0\n",
       "4999997    73.0\n",
       "4999998    89.0\n",
       "4999999    86.0\n",
       "Name: na_guide, Length: 5000000, dtype: float16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_guide = [str(a) + '_' + str(b) for a, b in zip(na_guide.values.astype(int), np.concatenate([np.ones(100000).astype(int) * i for i in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = bp.unpack_ndarray_from_file(os.path.join('../input/feats_tblr', 'trn_dat_all_origv2_w500.bp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(a).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_neighbour_quantile_feats_w10 = bp.unpack_ndarray_from_file(os.path.join('../input', 'tst_dat_neighbour_quantile_all_w10.bp'))\n",
    "trn_neighbour_quantile_feats_w30 = bp.unpack_ndarray_from_file(os.path.join('../input', 'tst_dat_neighbour_quantile_all_w30.bp'))\n",
    "trn_neighbour_quantile_feats_w50 = bp.unpack_ndarray_from_file(os.path.join('../input', 'tst_dat_neighbour_quantile_all_w50.bp'))\n",
    "trn_neighbour_quantile_feats_w100 = bp.unpack_ndarray_from_file(os.path.join('../input', 'tst_dat_neighbour_quantile_all_w100.bp'))\n",
    "trn_neighbour_quantile_feats_w200 = bp.unpack_ndarray_from_file(os.path.join('../input', 'tst_dat_neighbour_quantile_all_w200.bp'))\n",
    "trn_neighbour_quantile_feats_w500 = bp.unpack_ndarray_from_file(os.path.join('../input', 'tst_dat_neighbour_quantile_all_w500.bp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_all = np.concatenate(\n",
    "    [\n",
    "        trn_neighbour_quantile_feats_w10,\n",
    "        trn_neighbour_quantile_feats_w30,\n",
    "        trn_neighbour_quantile_feats_w50,\n",
    "        trn_neighbour_quantile_feats_w100,\n",
    "        trn_neighbour_quantile_feats_w200,\n",
    "        trn_neighbour_quantile_feats_w500,\n",
    "    ],\n",
    "    1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp.pack_ndarray_to_file(trn_all, '../input/feats_tblr/tst_dat_neighbour_quantile_all.bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_trn = pd.read_csv('../input/train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgnl = pdf_trn['signal']\n",
    "sgnl_R = pd.concat([sgnl, pd.Series([np.nan] * 9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -2.7600\n",
       "1   -2.8557\n",
       "2   -2.4074\n",
       "3   -3.1404\n",
       "4   -3.1525\n",
       "5   -2.6418\n",
       "6   -2.6993\n",
       "7   -2.5935\n",
       "8   -2.6682\n",
       "9   -2.7586\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgnl_R.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndx = 0\n",
    "slc = slice(ndx, ndx+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentileofscore(sgnl_R.iloc[slc], sgnl_R.iloc[slc].iloc[0], 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_w500 = bp.unpack_ndarray_from_file('../input/feats_tblr/trn_dat_all_w500_fixed.bp')\n",
    "trn_orig = pd.read_pickle('../input/feats_tblr/trn_dat_orig_v2_all.pkl')\n",
    "trn_orig = trn_orig.loc[:, [c for c in trn_orig.columns if c not in ('time', 'batch', 'open_channels')]]\n",
    "\n",
    "trn = np.concatenate([trn_orig.values, trn_w500], axis=1)\n",
    "del trn_orig, trn_w500\n",
    "\n",
    "lbl = pd.read_pickle('../input/feats_tblr/trn_lbl_orig_v2_all.pkl').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_w500 = bp.unpack_ndarray_from_file(\n",
    "    os.path.join(\n",
    "        '../input/feats_tblr',\n",
    "        [f for f in os.listdir('../input/feats_tblr') if ('tst' in f) and ('w500' in f)][0]\n",
    "    )\n",
    ")\n",
    "tst_orig = pd.read_pickle('../input/feats_tblr/tst_dat_orig_v2_all.pkl')\n",
    "tst_orig = tst_orig.loc[:, [c for c in tst_orig.columns if c not in ('time', 'batch', 'open_channels')]]\n",
    "\n",
    "tst = np.concatenate([tst_orig.values, tst_w500], axis=1)\n",
    "del tst_orig, tst_w500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 66)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 column has std of zero.\n",
      "067 / 067\r"
     ]
    }
   ],
   "source": [
    "for c in range(trn.shape[1]):\n",
    "    # drop useless column\n",
    "    if np.nanstd(trn[:, c]) == 0:\n",
    "        print('{} column has std of zero.'.format(c))\n",
    "        continue\n",
    "        \n",
    "    # process infinite value\n",
    "    isinf = ~np.isfinite(trn[:, c])\n",
    "    trn[:, c][trn[:, c] == np.inf] = np.nanmax(trn[:, c][~isinf])\n",
    "    trn[:, c][trn[:, c] == -np.inf] = np.nanmin(trn[:, c][~isinf])\n",
    "    \n",
    "    #isinf = ~np.isfinite(tst[:, c])\n",
    "    #tst[:, c][tst[:, c] == np.inf] = np.nanmax(tst[:, c][~isinf])\n",
    "    #tst[:, c][tst[:, c] == -np.inf] = np.nanmin(tst[:, c][~isinf])\n",
    "    \n",
    "    # process nan\n",
    "    isnan_trn = np.isnan(trn[:, c])\n",
    "    c_avg = np.nanmean(trn[:, c])\n",
    "    c_std = np.nanstd(trn[:, c])\n",
    "    \n",
    "    if isnan_trn.sum() > 0:\n",
    "        trn[:, c][isnan_trn] = c_avg\n",
    "    \n",
    "    #isnan_tst = np.isnan(tst[:, c])\n",
    "    #if isnan_tst.sum() > 0:\n",
    "    #    tst[:, c][isnan_tst] = c_avg\n",
    "    \n",
    "    # finally scale\n",
    "    trn[:, c] = (trn[:, c] - c_avg) / c_std\n",
    "    #tst[:, c] = (tst[:, c] - c_avg) / c_std\n",
    "    \n",
    "    # show progress\n",
    "    print('{:03d} / {:03d}'.format(c+1, trn.shape[1]), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "for c in [79, 89, 99, 109, 119, 129, 139, 149, 159, 169, 179, 189, 199, 209, 219, 229, 700, 759, 768, 1046, 1105, 1114]:\n",
    "    print(np.nanstd(trn[:, c]), np.nanstd(tst[:, c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = np.delete(trn, [79, 89, 99, 109, 119, 129, 139, 149, 159, 169, 179, 189, 199, 209, 219, 229, 700, 759, 768, 1046, 1105, 1114], axis=1)\n",
    "tst = np.delete(tst, [79, 89, 99, 109, 119, 129, 139, 149, 159, 169, 179, 189, 199, 209, 219, 229, 700, 759, 768, 1046, 1105, 1114], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp.pack_ndarray_to_file(trn, '../input/feats_tblr/trn_dat_all_origv2_w500.bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp.pack_ndarray_to_file(tst, '../input/feats_tblr/tst_dat_all_origv2_w500.bp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_f_v2 = sorted([f for f in os.listdir('../input') if ('trn' in f) and ('v2' in f) and ('dat' in f) and ('w100' in f)])\n",
    "trn_v2 = np.concatenate([bp.unpack_ndarray_from_file(os.path.join('../input/', f)) for f in trn_f_v2], axis=0)\n",
    "\n",
    "tst_f_v2 = sorted([f for f in os.listdir('../input') if ('tst' in f) and ('v2' in f) and ('dat' in f) and ('w100' in f)])\n",
    "tst_f_v2 = tst_f_v2[:1] + tst_f_v2[11:] + tst_f_v2[1:11]\n",
    "tst_v2 = np.concatenate([bp.unpack_ndarray_from_file(os.path.join('../input/', f)) for f in tst_f_v2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 column has std of zero.\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for c in range(trn_v2.shape[1]):\n",
    "    # drop useless column\n",
    "    if np.nanstd(trn_v2[:, c]) == 0:\n",
    "        print('{} column has std of zero.'.format(c))\n",
    "        print(np.nanstd(tst_v2[:, c]))\n",
    "        continue\n",
    "        \n",
    "    # process infinite value\n",
    "    isinf = ~np.isfinite(trn_v2[:, c])\n",
    "    trn_v2[:, c][trn_v2[:, c] == np.inf] = np.nanmax(trn_v2[:, c][~isinf])\n",
    "    trn_v2[:, c][trn_v2[:, c] == -np.inf] = np.nanmin(trn_v2[:, c][~isinf])\n",
    "    \n",
    "    isinf = ~np.isfinite(tst_v2[:, c])\n",
    "    tst_v2[:, c][tst_v2[:, c] == np.inf] = np.nanmax(tst_v2[:, c][~isinf])\n",
    "    tst_v2[:, c][tst_v2[:, c] == -np.inf] = np.nanmin(tst_v2[:, c][~isinf])\n",
    "    \n",
    "    # process nan\n",
    "    isnan_trn = np.isnan(trn_v2[:, c])\n",
    "    c_avg = np.nanmean(trn_v2[:, c])\n",
    "    c_std = np.nanstd(trn_v2[:, c])\n",
    "    \n",
    "    if isnan_trn.sum() > 0:\n",
    "        trn_v2[:, c][isnan_trn] = c_avg\n",
    "    \n",
    "    isnan_tst = np.isnan(tst_v2[:, c])\n",
    "    if isnan_tst.sum() > 0:\n",
    "        tst_v2[:, c][isnan_tst] = c_avg\n",
    "    \n",
    "    # finally scale\n",
    "    trn_v2[:, c] = (trn_v2[:, c] - c_avg) / c_std\n",
    "    tst_v2[:, c] = (tst_v2[:, c] - c_avg) / c_std\n",
    "    \n",
    "    # show progress\n",
    "    print('{:03d} / {:03d}'.format(c+1, trn_v2.shape[1]), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_v2 = np.delete(trn_v2, [78], axis=1)\n",
    "tst_v2 = np.delete(tst_v2, [78], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp.pack_ndarray_to_file(trn_v2, '../input/trn_dat_v2_w100_welch.bp')\n",
    "bp.pack_ndarray_to_file(tst_v2, '../input/tst_dat_v2_w100_welch.bp')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trn = bp.unpack_ndarray_from_file('../input/trn_datv2_g0_w50.bp')\n",
    "feat = bp.unpack_ndarray_from_file('../input/trn_featv2_g0_w50.bp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
