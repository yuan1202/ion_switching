{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bloscpack as bp\n",
    "\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from NNs import WaveTRSFM_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "EPOCHS = 96\n",
    "BATCHSIZE = 32\n",
    "SEED = 19550423\n",
    "LR = 0.0005\n",
    "SPLITS = 5\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_fs = sorted([f for f in os.listdir('../input/') if (('trn_srs_dat' in f) and ('s500' in f) and ('w500' in f))])\n",
    "lbl_fs = sorted([f for f in os.listdir('../input/') if ('trn_srs_lbl' in f) and ('s500' in f) and ('w500' in f)])\n",
    "\n",
    "tst_fs = sorted([f for f in os.listdir('../input/') if (('tst_srs_dat' in f) and ('s500' in f) and ('w500' in f))])\n",
    "tst_fs = [tst_fs[i] for i in [0, 11, 12, 13, 14, 15, 16, 17, 18, 19]] + tst_fs[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trn_srs_dat_g0_s500_w500_feat_basic.bp',\n",
       " 'trn_srs_dat_g1_s500_w500_feat_basic.bp',\n",
       " 'trn_srs_dat_g2_s500_w500_feat_basic.bp',\n",
       " 'trn_srs_dat_g3_s500_w500_feat_basic.bp',\n",
       " 'trn_srs_dat_g4_s500_w500_feat_basic.bp',\n",
       " 'trn_srs_dat_g5_s500_w500_feat_basic.bp',\n",
       " 'trn_srs_dat_g6_s500_w500_feat_basic.bp',\n",
       " 'trn_srs_dat_g7_s500_w500_feat_basic.bp',\n",
       " 'trn_srs_dat_g8_s500_w500_feat_basic.bp',\n",
       " 'trn_srs_dat_g9_s500_w500_feat_basic.bp']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_trn = np.concatenate(\n",
    "    [bp.unpack_ndarray_from_file(os.path.join('../input/', f)) for f in trn_fs],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "series_lbl = [bp.unpack_ndarray_from_file(os.path.join('../input/', f)) for f in lbl_fs]\n",
    "\n",
    "series_grp = np.concatenate(\n",
    "    [np.ones(shape=(arr.shape[0],)) * i for i, arr in zip([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], series_lbl)],\n",
    "    axis=0\n",
    ").astype(int)\n",
    "\n",
    "series_lbl = np.concatenate(\n",
    "    series_lbl,\n",
    "    axis=0\n",
    ")[:, :, None]\n",
    "\n",
    "series_tst = np.concatenate(\n",
    "    [bp.unpack_ndarray_from_file(os.path.join('../input/', f)) for f in tst_fs],\n",
    "    axis=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_skewness = [feature_calculators.binned_entropy(lst, max_bins=20) for lst in series_trn[:, :, 0].tolist()]\n",
    "lbl_skewness = pd.qcut(pd.Series(lbl_skewness), q=10, duplicates='drop')\n",
    "\n",
    "skf_grp = [str(a) + '_' + str(b) for a, b in zip(series_grp, lbl_skewness)]\n",
    "us = np.unique(skf_grp)\n",
    "umap = {u: i for u, i in zip(us, range(len(us)))}\n",
    "skf_grp = [umap[u] for u in skf_grp]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "us, cs = np.unique(skf_grp, return_counts=True)\n",
    "for u, c in zip(us, cs):\n",
    "    print(u, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 22 / 22; \r"
     ]
    }
   ],
   "source": [
    "for i in range(series_trn.shape[-1]):\n",
    "\n",
    "    avg = series_trn[:, :, i].mean()\n",
    "    std = series_trn[:, :, i].std()\n",
    "    series_trn[:, :, i] = (series_trn[:, :, i] - avg) / std\n",
    "    series_tst[:, :, i] = (series_tst[:, :, i] - avg) / std\n",
    "    \n",
    "    #print('---------')\n",
    "    #print('{:d} - max {:.3f}; min {:.3f};'.format(i, series_trn[:, :, i].max(), series_trn[:, :, i].min()))\n",
    "    #print('{:d} - max {:.3f}; min {:.3f};'.format(i, series_tst[:, :, i].max(), series_tst[:, :, i].min()))\n",
    "    print('progress: {:02d} / {:02d}; '.format(i+1, series_trn.shape[-1]), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Waveset(Dataset):\n",
    "    def __init__(self, data, labels=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        \n",
    "        if self.labels is None:\n",
    "            return data.astype(np.float32)\n",
    "        else:\n",
    "            labels = self.labels[idx]\n",
    "            return (data.astype(np.float32), labels.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fold_train_validate(\n",
    "    model, optimizer, criterion, scheduler,\n",
    "    training_loader, validation_loader, fold_number,\n",
    "    save_path='./saved_models/wavenet_model_fold{:03d}_checkpoint.pth',\n",
    "    early_stopping=15,\n",
    "):\n",
    "\n",
    "    trn_losses = [np.nan]\n",
    "    vld_losses = [np.nan]\n",
    "    vld_f1s = [np.nan]\n",
    "    \n",
    "    last_best = 0\n",
    "\n",
    "    for epc in range(EPOCHS):\n",
    "        print('===========================================================')\n",
    "\n",
    "        epoch_trn_losses = []\n",
    "        epoch_trn_lbls = []\n",
    "        epoch_trn_prds = []\n",
    "        epoch_vld_losses = []\n",
    "        epoch_vld_lbls = []\n",
    "        epoch_vld_prds = []\n",
    "\n",
    "        # ------ training ------\n",
    "        model.train()\n",
    "        for i, (trn_batch_dat, trn_batch_lbl) in enumerate(training_loader):\n",
    "            trn_batch_dat, trn_batch_lbl = trn_batch_dat.to(DEVICE), trn_batch_lbl.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            trn_batch_prd = model(trn_batch_dat)\n",
    "            trn_batch_prd = trn_batch_prd.view(-1, trn_batch_prd.size(-1))\n",
    "            trn_batch_lbl = trn_batch_lbl.view(-1)\n",
    "            loss = criterion(trn_batch_prd, trn_batch_lbl)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_trn_losses.append(loss.item())\n",
    "            epoch_trn_lbls.append(trn_batch_lbl.detach().cpu().numpy())\n",
    "            epoch_trn_prds.append(trn_batch_prd.detach().cpu().numpy())\n",
    "\n",
    "            print(\n",
    "                'Epoch {:03d}/{:03d} - Training batch {:04d}/{:04d}: Training loss {:.6f};'.format(\n",
    "                    epc + 1, EPOCHS, i + 1, len(training_loader), epoch_trn_losses[-1],\n",
    "                ), \n",
    "                end='\\r'\n",
    "            )\n",
    "\n",
    "        # ------ validation ------\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (vld_batch_dat, vld_batch_lbl) in enumerate(validation_loader):\n",
    "                vld_batch_dat, vld_batch_lbl = vld_batch_dat.to(DEVICE), vld_batch_lbl.to(DEVICE)\n",
    "\n",
    "                vld_batch_prd = model(vld_batch_dat)\n",
    "                vld_batch_prd = vld_batch_prd.view(-1, vld_batch_prd.size(-1))\n",
    "                vld_batch_lbl = vld_batch_lbl.view(-1)\n",
    "                loss = criterion(trn_batch_prd, trn_batch_lbl)\n",
    "\n",
    "                epoch_vld_losses.append(loss.item())\n",
    "                epoch_vld_lbls.append(vld_batch_lbl.detach().cpu().numpy())\n",
    "                epoch_vld_prds.append(vld_batch_prd.detach().cpu().numpy())\n",
    "\n",
    "                print(\n",
    "                    'Epoch {:03d}/{:03d} - Validation batch {:04d}/{:04d}: Validation loss {:.6f};'.format(\n",
    "                        epc + 1, EPOCHS, i + 1, len(validation_loader), epoch_vld_losses[-1],\n",
    "                    ), \n",
    "                    end='\\r'\n",
    "                )\n",
    "\n",
    "        # ------ epoch end ------\n",
    "        f1_trn = f1_score(\n",
    "            np.concatenate(epoch_trn_lbls, axis=0), \n",
    "            np.concatenate(epoch_trn_prds, axis=0).argmax(1),\n",
    "            labels=list(range(11)), \n",
    "            average='macro'\n",
    "        )\n",
    "        f1_vld = f1_score(\n",
    "            np.concatenate(epoch_vld_lbls, axis=0), \n",
    "            np.concatenate(epoch_vld_prds, axis=0).argmax(1),\n",
    "            labels=list(range(11)), \n",
    "            average='macro'\n",
    "        )\n",
    "\n",
    "\n",
    "        print(\n",
    "            'Epoch {:03d}/{:03d} - Mean training loss {:.6f}; Mean training F1 {:.6f}; Mean validation loss {:.6f}; Mean validation F1 {:.6f}; Learning rate {:.6f};'.format(\n",
    "                epc + 1, EPOCHS, np.mean(epoch_trn_losses), f1_trn, np.mean(epoch_vld_losses), f1_vld, scheduler.get_lr()[0],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if f1_vld > np.nanmax(vld_f1s):\n",
    "            torch.save(\n",
    "                {\n",
    "                    'epoch': epc + 1,\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'f1': f1_vld,\n",
    "                    'loss': np.mean(epoch_vld_losses),\n",
    "                }, \n",
    "                save_path.format(fold_number)\n",
    "            )\n",
    "            \n",
    "            last_best = epc\n",
    "            \n",
    "        if epc - last_best > early_stopping:\n",
    "            break\n",
    "\n",
    "        vld_f1s.append(f1_vld)\n",
    "\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################\n",
      "Training/validation for fold 1/5;\n",
      "===========================================================\n",
      "Epoch 001/096 - Mean training loss 0.428720; Mean training F1 0.752741; Mean validation loss 0.156739; Mean validation F1 0.923876; Learning rate 0.000500;\n",
      "===========================================================\n",
      "Epoch 002/096 - Mean training loss 0.114477; Mean training F1 0.915637; Mean validation loss 0.098036; Mean validation F1 0.933910; Learning rate 0.000500;\n",
      "===========================================================\n",
      "Epoch 003/096 - Mean training loss 0.095097; Mean training F1 0.931418; Mean validation loss 0.089710; Mean validation F1 0.933476; Learning rate 0.000499;\n",
      "===========================================================\n",
      "Epoch 004/096 - Mean training loss 0.094520; Mean training F1 0.930872; Mean validation loss 0.117951; Mean validation F1 0.936137; Learning rate 0.000498;\n",
      "===========================================================\n",
      "Epoch 005/096 - Mean training loss 0.092231; Mean training F1 0.932452; Mean validation loss 0.087902; Mean validation F1 0.926984; Learning rate 0.000497;\n",
      "===========================================================\n",
      "Epoch 006/096 - Mean training loss 0.090602; Mean training F1 0.932902; Mean validation loss 0.105959; Mean validation F1 0.936606; Learning rate 0.000495;\n",
      "===========================================================\n",
      "Epoch 007/096 - Mean training loss 0.090709; Mean training F1 0.932815; Mean validation loss 0.154580; Mean validation F1 0.935461; Learning rate 0.000494;\n",
      "===========================================================\n",
      "Epoch 008/096 - Mean training loss 0.089754; Mean training F1 0.933314; Mean validation loss 0.085437; Mean validation F1 0.930927; Learning rate 0.000492;\n",
      "===========================================================\n",
      "Epoch 009/096 - Mean training loss 0.088714; Mean training F1 0.933904; Mean validation loss 0.079267; Mean validation F1 0.935673; Learning rate 0.000490;\n",
      "===========================================================\n",
      "Epoch 010/096 - Mean training loss 0.090677; Mean training F1 0.931838; Mean validation loss 0.038346; Mean validation F1 0.932139; Learning rate 0.000487;\n",
      "===========================================================\n",
      "Epoch 011/096 - Mean training loss 0.087392; Mean training F1 0.934732; Mean validation loss 0.067904; Mean validation F1 0.935054; Learning rate 0.000484;\n",
      "===========================================================\n",
      "Epoch 012/096 - Mean training loss 0.087704; Mean training F1 0.934473; Mean validation loss 0.069560; Mean validation F1 0.935255; Learning rate 0.000481;\n",
      "===========================================================\n",
      "Epoch 013/096 - Mean training loss 0.087482; Mean training F1 0.934095; Mean validation loss 0.075079; Mean validation F1 0.932978; Learning rate 0.000478;\n",
      "===========================================================\n",
      "Epoch 014/096 - Mean training loss 0.086982; Mean training F1 0.935034; Mean validation loss 0.057687; Mean validation F1 0.936138; Learning rate 0.000475;\n",
      "===========================================================\n",
      "Epoch 015/096 - Mean training loss 0.086651; Mean training F1 0.934806; Mean validation loss 0.074170; Mean validation F1 0.937290; Learning rate 0.000471;\n",
      "===========================================================\n",
      "Epoch 016/096 - Mean training loss 0.086535; Mean training F1 0.934909; Mean validation loss 0.062781; Mean validation F1 0.936868; Learning rate 0.000467;\n",
      "===========================================================\n",
      "Epoch 017/096 - Mean training loss 0.085981; Mean training F1 0.935406; Mean validation loss 0.085462; Mean validation F1 0.938161; Learning rate 0.000463;\n",
      "===========================================================\n",
      "Epoch 018/096 - Mean training loss 0.085264; Mean training F1 0.935864; Mean validation loss 0.062467; Mean validation F1 0.937979; Learning rate 0.000458;\n",
      "===========================================================\n",
      "Epoch 019/096 - Mean training loss 0.085554; Mean training F1 0.935682; Mean validation loss 0.065671; Mean validation F1 0.937292; Learning rate 0.000454;\n",
      "===========================================================\n",
      "Epoch 020/096 - Mean training loss 0.086080; Mean training F1 0.935141; Mean validation loss 0.079501; Mean validation F1 0.929497; Learning rate 0.000449;\n",
      "===========================================================\n",
      "Epoch 021/096 - Mean training loss 0.086078; Mean training F1 0.935332; Mean validation loss 0.081240; Mean validation F1 0.937968; Learning rate 0.000444;\n",
      "===========================================================\n",
      "Epoch 022/096 - Mean training loss 0.085158; Mean training F1 0.935614; Mean validation loss 0.087809; Mean validation F1 0.937944; Learning rate 0.000439;\n",
      "===========================================================\n",
      "Epoch 023/096 - Mean training loss 0.084643; Mean training F1 0.936158; Mean validation loss 0.094824; Mean validation F1 0.937851; Learning rate 0.000433;\n",
      "===========================================================\n",
      "Epoch 024/096 - Mean training loss 0.084517; Mean training F1 0.936310; Mean validation loss 0.085368; Mean validation F1 0.936694; Learning rate 0.000427;\n",
      "===========================================================\n",
      "Epoch 025/096 - Mean training loss 0.085086; Mean training F1 0.935469; Mean validation loss 0.059904; Mean validation F1 0.936047; Learning rate 0.000422;\n",
      "===========================================================\n",
      "Epoch 026/096 - Mean training loss 0.084304; Mean training F1 0.936377; Mean validation loss 0.102922; Mean validation F1 0.935475; Learning rate 0.000416;\n",
      "===========================================================\n",
      "Epoch 027/096 - Mean training loss 0.084790; Mean training F1 0.935469; Mean validation loss 0.117329; Mean validation F1 0.934483; Learning rate 0.000409;\n",
      "===========================================================\n",
      "Epoch 028/096 - Mean training loss 0.084139; Mean training F1 0.936195; Mean validation loss 0.114577; Mean validation F1 0.937409; Learning rate 0.000403;\n",
      "===========================================================\n",
      "Epoch 029/096 - Mean training loss 0.083706; Mean training F1 0.936648; Mean validation loss 0.114164; Mean validation F1 0.936849; Learning rate 0.000396;\n",
      "===========================================================\n",
      "Epoch 030/096 - Mean training loss 0.084148; Mean training F1 0.936083; Mean validation loss 0.067659; Mean validation F1 0.937780; Learning rate 0.000390;\n",
      "===========================================================\n",
      "Epoch 031/096 - Mean training loss 0.083241; Mean training F1 0.936817; Mean validation loss 0.105647; Mean validation F1 0.938385; Learning rate 0.000383;\n",
      "===========================================================\n",
      "Epoch 032/096 - Mean training loss 0.083419; Mean training F1 0.936825; Mean validation loss 0.063543; Mean validation F1 0.937635; Learning rate 0.000376;\n",
      "===========================================================\n",
      "Epoch 033/096 - Mean training loss 0.083116; Mean training F1 0.936741; Mean validation loss 0.078403; Mean validation F1 0.936205; Learning rate 0.000369;\n",
      "===========================================================\n",
      "Epoch 034/096 - Mean training loss 0.083133; Mean training F1 0.936851; Mean validation loss 0.096335; Mean validation F1 0.935240; Learning rate 0.000362;\n",
      "===========================================================\n",
      "Epoch 035/096 - Mean training loss 0.082841; Mean training F1 0.937211; Mean validation loss 0.086490; Mean validation F1 0.937814; Learning rate 0.000354;\n",
      "===========================================================\n",
      "Epoch 036/096 - Mean training loss 0.083160; Mean training F1 0.936585; Mean validation loss 0.103837; Mean validation F1 0.938487; Learning rate 0.000347;\n",
      "===========================================================\n",
      "Epoch 037/096 - Mean training loss 0.082107; Mean training F1 0.937491; Mean validation loss 0.092397; Mean validation F1 0.938001; Learning rate 0.000339;\n",
      "===========================================================\n",
      "Epoch 038/096 - Mean training loss 0.082623; Mean training F1 0.936931; Mean validation loss 0.051627; Mean validation F1 0.937884; Learning rate 0.000331;\n",
      "===========================================================\n",
      "Epoch 039/096 - Mean training loss 0.081979; Mean training F1 0.937635; Mean validation loss 0.119954; Mean validation F1 0.937955; Learning rate 0.000324;\n",
      "===========================================================\n",
      "Epoch 040/096 - Mean training loss 0.081678; Mean training F1 0.937751; Mean validation loss 0.093740; Mean validation F1 0.938309; Learning rate 0.000316;\n",
      "===========================================================\n",
      "Epoch 041/096 - Mean training loss 0.081707; Mean training F1 0.937535; Mean validation loss 0.063589; Mean validation F1 0.937245; Learning rate 0.000308;\n",
      "===========================================================\n",
      "Epoch 042/096 - Mean training loss 0.081365; Mean training F1 0.937815; Mean validation loss 0.072110; Mean validation F1 0.938091; Learning rate 0.000300;\n",
      "===========================================================\n",
      "Epoch 043/096 - Mean training loss 0.081451; Mean training F1 0.937619; Mean validation loss 0.063076; Mean validation F1 0.938744; Learning rate 0.000292;\n",
      "===========================================================\n",
      "Epoch 044/096 - Mean training loss 0.080964; Mean training F1 0.938047; Mean validation loss 0.087928; Mean validation F1 0.936627; Learning rate 0.000284;\n",
      "===========================================================\n",
      "Epoch 045/096 - Mean training loss 0.080960; Mean training F1 0.938123; Mean validation loss 0.103078; Mean validation F1 0.938296; Learning rate 0.000276;\n",
      "===========================================================\n",
      "Epoch 046/096 - Mean training loss 0.080483; Mean training F1 0.938351; Mean validation loss 0.089582; Mean validation F1 0.937584; Learning rate 0.000268;\n",
      "===========================================================\n",
      "Epoch 047/096 - Mean training loss 0.080404; Mean training F1 0.938251; Mean validation loss 0.082191; Mean validation F1 0.938412; Learning rate 0.000260;\n",
      "===========================================================\n",
      "Epoch 048/096 - Mean training loss 0.080064; Mean training F1 0.938551; Mean validation loss 0.112517; Mean validation F1 0.938228; Learning rate 0.000252;\n",
      "===========================================================\n",
      "Epoch 049/096 - Mean training loss 0.079768; Mean training F1 0.938445; Mean validation loss 0.066287; Mean validation F1 0.937252; Learning rate 0.000243;\n",
      "===========================================================\n",
      "Epoch 050/096 - Mean training loss 0.079567; Mean training F1 0.938709; Mean validation loss 0.075306; Mean validation F1 0.938316; Learning rate 0.000235;\n",
      "===========================================================\n",
      "Epoch 051/096 - Mean training loss 0.079303; Mean training F1 0.938810; Mean validation loss 0.064951; Mean validation F1 0.938370; Learning rate 0.000227;\n",
      "===========================================================\n",
      "Epoch 052/096 - Mean training loss 0.078904; Mean training F1 0.939262; Mean validation loss 0.099502; Mean validation F1 0.938706; Learning rate 0.000219;\n",
      "===========================================================\n",
      "Epoch 053/096 - Mean training loss 0.078491; Mean training F1 0.939341; Mean validation loss 0.123348; Mean validation F1 0.938414; Learning rate 0.000211;\n",
      "===========================================================\n",
      "Epoch 054/096 - Mean training loss 0.078217; Mean training F1 0.939374; Mean validation loss 0.075059; Mean validation F1 0.937678; Learning rate 0.000203;\n",
      "===========================================================\n",
      "Epoch 055/096 - Mean training loss 0.077843; Mean training F1 0.939686; Mean validation loss 0.112010; Mean validation F1 0.938349; Learning rate 0.000195;\n",
      "===========================================================\n",
      "Epoch 056/096 - Mean training loss 0.077338; Mean training F1 0.939729; Mean validation loss 0.056442; Mean validation F1 0.937759; Learning rate 0.000187;\n",
      "===========================================================\n",
      "Epoch 057/096 - Mean training loss 0.077106; Mean training F1 0.940026; Mean validation loss 0.047522; Mean validation F1 0.938150; Learning rate 0.000179;\n",
      "===========================================================\n",
      "Epoch 058/096 - Mean training loss 0.076575; Mean training F1 0.940194; Mean validation loss 0.080039; Mean validation F1 0.937690; Learning rate 0.000172;\n",
      "===========================================================\n",
      "Epoch 059/096 - Mean training loss 0.076068; Mean training F1 0.940502; Mean validation loss 0.084006; Mean validation F1 0.938329; Learning rate 0.000164;\n",
      "===========================================================\n",
      "Epoch 060/096 - Mean training loss 0.075633; Mean training F1 0.940600; Mean validation loss 0.096986; Mean validation F1 0.938185; Learning rate 0.000156;\n",
      "===========================================================\n",
      "Epoch 061/096 - Mean training loss 0.075224; Mean training F1 0.940917; Mean validation loss 0.058697; Mean validation F1 0.937702; Learning rate 0.000149;\n",
      "===========================================================\n",
      "Epoch 062/096 - Mean training loss 0.074743; Mean training F1 0.941141; Mean validation loss 0.071507; Mean validation F1 0.937706; Learning rate 0.000141;\n",
      "===========================================================\n",
      "Epoch 063/096 - Mean training loss 0.074164; Mean training F1 0.941387; Mean validation loss 0.092593; Mean validation F1 0.937771; Learning rate 0.000134;\n",
      "===========================================================\n",
      "Epoch 064/096 - Mean training loss 0.073584; Mean training F1 0.941767; Mean validation loss 0.104865; Mean validation F1 0.937648; Learning rate 0.000127;\n"
     ]
    }
   ],
   "source": [
    "for fld, (ndcs_trn, ndcs_vld) in enumerate(skf.split(series_trn, skf_grp)):\n",
    "    print('################################################################')\n",
    "    print('Training/validation for fold {:d}/{:d};'.format(fld+1, N_FOLDS))\n",
    "    \n",
    "    # setup fold data\n",
    "    dat_trn, lbl_trn = series_trn[ndcs_trn], series_lbl[ndcs_trn]\n",
    "    dat_vld, lbl_vld = series_trn[ndcs_vld], series_lbl[ndcs_vld]\n",
    "    \n",
    "    waveset_trn = Waveset(dat_trn, lbl_trn)\n",
    "    waveset_vld = Waveset(dat_vld, lbl_vld)\n",
    "\n",
    "    loader_trn = DataLoader(waveset_trn, BATCHSIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    loader_vld = DataLoader(waveset_vld, BATCHSIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    # setup fold model\n",
    "    mdl = WaveTRSFM_Classifier(series_trn.shape[-1]).to(DEVICE)\n",
    "    critrn = nn.CrossEntropyLoss()\n",
    "    optimzr = torch.optim.AdamW(mdl.parameters(), lr=LR)\n",
    "    schdlr = torch.optim.lr_scheduler.CosineAnnealingLR(optimzr, T_max=EPOCHS, eta_min=LR/200)\n",
    "    \n",
    "    # run\n",
    "    fold_train_validate(\n",
    "        model=mdl, optimizer=optimzr, criterion=critrn,\n",
    "        scheduler=schdlr, training_loader=loader_trn, validation_loader=loader_vld,\n",
    "        fold_number=fld,\n",
    "        save_path='./saved_models/waveTRSFM_model_basicfeats_fold{:03d}_checkpoint.pth',\n",
    "        early_stopping=20\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv', dtype={'time': str, 'open_channels': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- fold 0 --------\n",
      "model validation loss: 0.050; validation f1: 0.937;\n"
     ]
    }
   ],
   "source": [
    "submission_pred = np.zeros(shape=(submission.shape[0], 11))\n",
    "\n",
    "waveset_tst = Waveset(series_tst)\n",
    "loader_tst = DataLoader(waveset_tst, BATCHSIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "for fld in range(5):\n",
    "    print('-------- fold {:d} --------'.format(fld))\n",
    "    fld_weight = torch.load('./saved_models/waveRNN_model_fold{:03d}_checkpoint.pth'.format(fld))\n",
    "    #fld_weight = torch.load('./saved_models/waveRNN_model_fold{:03d}_checkpoint_overfittest.pth'.format(fld))\n",
    "    print('model validation loss: {:.3f}; validation f1: {:.3f};'.format(fld_weight['loss'], fld_weight['f1']))\n",
    "    mdl = WaveRNN_Classifier(series_tst.shape[-1]).to(DEVICE)\n",
    "    mdl.load_state_dict(fld_weight['model'])\n",
    "    with torch.no_grad():\n",
    "        tst_fold_prd = []\n",
    "        for tst_batch_dat in loader_tst:\n",
    "            tst_batch_prd = mdl(tst_batch_dat.to(DEVICE))\n",
    "            tst_batch_prd = tst_batch_prd.view(-1, tst_batch_prd.size(-1)).detach().cpu().numpy()\n",
    "            tst_fold_prd.append(tst_batch_prd)\n",
    "            \n",
    "        submission_pred += np.concatenate(tst_fold_prd, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['open_channels'] = submission_pred.argmax(1)\n",
    "submission.to_csv(\"../submissions/sub0_waveRNN_myfeats_cvbyentropy_meanstdnorm_testoverfit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------- fold 0 --------\n",
    "model validation loss: 0.077; validation f1: 0.939;\n",
    "-------- fold 1 --------\n",
    "model validation loss: 0.076; validation f1: 0.938;\n",
    "-------- fold 2 --------\n",
    "model validation loss: 0.093; validation f1: 0.938;\n",
    "-------- fold 3 --------\n",
    "model validation loss: 0.087; validation f1: 0.939;\n",
    "-------- fold 4 --------\n",
    "model validation loss: 0.077; validation f1: 0.938;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "################################################################\n",
    "Training/validation for fold 1/5;\n",
    "===========================================================\n",
    "Epoch 001/096 - Mean training loss 0.501867; Mean training F1 0.705879; Mean validation loss 0.138329; Mean validation F1 0.906963; Learning rate 0.001000;\n",
    "===========================================================\n",
    "Epoch 002/096 - Mean training loss 0.109726; Mean training F1 0.927435; Mean validation loss 0.056432; Mean validation F1 0.934830; Learning rate 0.000999;\n",
    "===========================================================\n",
    "Epoch 003/096 - Mean training loss 0.090683; Mean training F1 0.935156; Mean validation loss 0.059653; Mean validation F1 0.935635; Learning rate 0.000998;\n",
    "===========================================================\n",
    "Epoch 004/096 - Mean training loss 0.087330; Mean training F1 0.936141; Mean validation loss 0.105142; Mean validation F1 0.937589; Learning rate 0.000996;\n",
    "===========================================================\n",
    "Epoch 005/096 - Mean training loss 0.088238; Mean training F1 0.934768; Mean validation loss 0.104610; Mean validation F1 0.934129; Learning rate 0.000994;\n",
    "===========================================================\n",
    "Epoch 006/096 - Mean training loss 0.085244; Mean training F1 0.936727; Mean validation loss 0.078481; Mean validation F1 0.937930; Learning rate 0.000991;\n",
    "===========================================================\n",
    "Epoch 007/096 - Mean training loss 0.087287; Mean training F1 0.934123; Mean validation loss 0.070760; Mean validation F1 0.934048; Learning rate 0.000988;\n",
    "===========================================================\n",
    "Epoch 008/096 - Mean training loss 0.084695; Mean training F1 0.936387; Mean validation loss 0.070302; Mean validation F1 0.937684; Learning rate 0.000984;\n",
    "===========================================================\n",
    "Epoch 009/096 - Mean training loss 0.083362; Mean training F1 0.937336; Mean validation loss 0.065110; Mean validation F1 0.936568; Learning rate 0.000979;\n",
    "===========================================================\n",
    "Epoch 010/096 - Mean training loss 0.083193; Mean training F1 0.937234; Mean validation loss 0.076595; Mean validation F1 0.938654; Learning rate 0.000974;\n",
    "===========================================================\n",
    "Epoch 011/096 - Mean training loss 0.083210; Mean training F1 0.937404; Mean validation loss 0.097337; Mean validation F1 0.936874; Learning rate 0.000969;\n",
    "===========================================================\n",
    "Epoch 012/096 - Mean training loss 0.086622; Mean training F1 0.933796; Mean validation loss 0.102512; Mean validation F1 0.937390; Learning rate 0.000963;\n",
    "===========================================================\n",
    "Epoch 013/096 - Mean training loss 0.082365; Mean training F1 0.937810; Mean validation loss 0.076244; Mean validation F1 0.937206; Learning rate 0.000956;\n",
    "===========================================================\n",
    "Epoch 014/096 - Mean training loss 0.083307; Mean training F1 0.937004; Mean validation loss 0.080800; Mean validation F1 0.935868; Learning rate 0.000949;\n",
    "===========================================================\n",
    "Epoch 015/096 - Mean training loss 0.081936; Mean training F1 0.937800; Mean validation loss 0.082287; Mean validation F1 0.938280; Learning rate 0.000942;\n",
    "===========================================================\n",
    "Epoch 016/096 - Mean training loss 0.082364; Mean training F1 0.937184; Mean validation loss 0.060042; Mean validation F1 0.936854; Learning rate 0.000934;\n",
    "===========================================================\n",
    "Epoch 017/096 - Mean training loss 0.083998; Mean training F1 0.936769; Mean validation loss 0.049420; Mean validation F1 0.934258; Learning rate 0.000926;\n",
    "===========================================================\n",
    "Epoch 018/096 - Mean training loss 0.082625; Mean training F1 0.936977; Mean validation loss 0.094634; Mean validation F1 0.937665; Learning rate 0.000917;\n",
    "===========================================================\n",
    "Epoch 019/096 - Mean training loss 0.081529; Mean training F1 0.938164; Mean validation loss 0.079352; Mean validation F1 0.938219; Learning rate 0.000907;\n",
    "===========================================================\n",
    "Epoch 020/096 - Mean training loss 0.081118; Mean training F1 0.937949; Mean validation loss 0.077096; Mean validation F1 0.936672; Learning rate 0.000898;\n",
    "===========================================================\n",
    "Epoch 021/096 - Mean training loss 0.080956; Mean training F1 0.938509; Mean validation loss 0.091608; Mean validation F1 0.938595; Learning rate 0.000888;\n",
    "===========================================================\n",
    "Epoch 022/096 - Mean training loss 0.080958; Mean training F1 0.938278; Mean validation loss 0.078200; Mean validation F1 0.938401; Learning rate 0.000877;\n",
    "===========================================================\n",
    "Epoch 023/096 - Mean training loss 0.080861; Mean training F1 0.938080; Mean validation loss 0.058759; Mean validation F1 0.937564; Learning rate 0.000866;\n",
    "===========================================================\n",
    "Epoch 024/096 - Mean training loss 0.080366; Mean training F1 0.938549; Mean validation loss 0.070830; Mean validation F1 0.936989; Learning rate 0.000855;\n",
    "===========================================================\n",
    "Epoch 025/096 - Mean training loss 0.081092; Mean training F1 0.938047; Mean validation loss 0.090330; Mean validation F1 0.937716; Learning rate 0.000843;\n",
    "===========================================================\n",
    "Epoch 026/096 - Mean training loss 0.080689; Mean training F1 0.938324; Mean validation loss 0.056684; Mean validation F1 0.937953; Learning rate 0.000831;\n",
    "===========================================================\n",
    "Epoch 027/096 - Mean training loss 0.080385; Mean training F1 0.938437; Mean validation loss 0.085575; Mean validation F1 0.936242; Learning rate 0.000819;\n",
    "===========================================================\n",
    "Epoch 028/096 - Mean training loss 0.079647; Mean training F1 0.939185; Mean validation loss 0.068280; Mean validation F1 0.938242; Learning rate 0.000806;\n",
    "===========================================================\n",
    "Epoch 029/096 - Mean training loss 0.080187; Mean training F1 0.938550; Mean validation loss 0.078902; Mean validation F1 0.937575; Learning rate 0.000793;\n",
    "===========================================================\n",
    "Epoch 030/096 - Mean training loss 0.079673; Mean training F1 0.938991; Mean validation loss 0.067880; Mean validation F1 0.938405; Learning rate 0.000779;\n",
    "===========================================================\n",
    "Epoch 031/096 - Mean training loss 0.079305; Mean training F1 0.939177; Mean validation loss 0.096250; Mean validation F1 0.938319; Learning rate 0.000766;\n",
    "################################################################\n",
    "Training/validation for fold 2/5;\n",
    "===========================================================\n",
    "Epoch 001/096 - Mean training loss 0.543387; Mean training F1 0.673101; Mean validation loss 0.167398; Mean validation F1 0.894220; Learning rate 0.001000;\n",
    "===========================================================\n",
    "Epoch 002/096 - Mean training loss 0.117420; Mean training F1 0.925187; Mean validation loss 0.082025; Mean validation F1 0.933191; Learning rate 0.000999;\n",
    "===========================================================\n",
    "Epoch 003/096 - Mean training loss 0.092572; Mean training F1 0.933774; Mean validation loss 0.094819; Mean validation F1 0.933393; Learning rate 0.000998;\n",
    "===========================================================\n",
    "Epoch 004/096 - Mean training loss 0.089472; Mean training F1 0.934366; Mean validation loss 0.085554; Mean validation F1 0.926386; Learning rate 0.000996;\n",
    "===========================================================\n",
    "Epoch 005/096 - Mean training loss 0.089910; Mean training F1 0.933699; Mean validation loss 0.099924; Mean validation F1 0.936005; Learning rate 0.000994;\n",
    "===========================================================\n",
    "Epoch 006/096 - Mean training loss 0.084997; Mean training F1 0.936458; Mean validation loss 0.072433; Mean validation F1 0.935129; Learning rate 0.000991;\n",
    "===========================================================\n",
    "Epoch 007/096 - Mean training loss 0.087895; Mean training F1 0.934594; Mean validation loss 0.089995; Mean validation F1 0.936118; Learning rate 0.000988;\n",
    "===========================================================\n",
    "Epoch 008/096 - Mean training loss 0.083448; Mean training F1 0.937030; Mean validation loss 0.093319; Mean validation F1 0.933288; Learning rate 0.000984;\n",
    "===========================================================\n",
    "Epoch 009/096 - Mean training loss 0.083629; Mean training F1 0.936745; Mean validation loss 0.066193; Mean validation F1 0.935686; Learning rate 0.000979;\n",
    "===========================================================\n",
    "Epoch 010/096 - Mean training loss 0.082673; Mean training F1 0.937435; Mean validation loss 0.076565; Mean validation F1 0.936211; Learning rate 0.000974;\n",
    "===========================================================\n",
    "Epoch 011/096 - Mean training loss 0.083780; Mean training F1 0.936482; Mean validation loss 0.097195; Mean validation F1 0.936258; Learning rate 0.000969;\n",
    "===========================================================\n",
    "Epoch 012/096 - Mean training loss 0.082495; Mean training F1 0.937240; Mean validation loss 0.069438; Mean validation F1 0.932031; Learning rate 0.000963;\n",
    "===========================================================\n",
    "Epoch 013/096 - Mean training loss 0.084359; Mean training F1 0.935137; Mean validation loss 0.075321; Mean validation F1 0.934955; Learning rate 0.000956;\n",
    "===========================================================\n",
    "Epoch 014/096 - Mean training loss 0.083263; Mean training F1 0.936523; Mean validation loss 0.068573; Mean validation F1 0.933937; Learning rate 0.000949;\n",
    "===========================================================\n",
    "Epoch 015/096 - Mean training loss 0.082694; Mean training F1 0.937130; Mean validation loss 0.076013; Mean validation F1 0.936116; Learning rate 0.000942;\n",
    "===========================================================\n",
    "Epoch 016/096 - Mean training loss 0.081433; Mean training F1 0.937970; Mean validation loss 0.095539; Mean validation F1 0.936814; Learning rate 0.000934;\n",
    "===========================================================\n",
    "Epoch 017/096 - Mean training loss 0.081242; Mean training F1 0.938078; Mean validation loss 0.089987; Mean validation F1 0.937079; Learning rate 0.000926;\n",
    "===========================================================\n",
    "Epoch 018/096 - Mean training loss 0.080866; Mean training F1 0.938158; Mean validation loss 0.076559; Mean validation F1 0.936148; Learning rate 0.000917;\n",
    "===========================================================\n",
    "Epoch 019/096 - Mean training loss 0.080908; Mean training F1 0.938258; Mean validation loss 0.069268; Mean validation F1 0.937440; Learning rate 0.000907;\n",
    "===========================================================\n",
    "Epoch 020/096 - Mean training loss 0.080453; Mean training F1 0.938513; Mean validation loss 0.043213; Mean validation F1 0.937008; Learning rate 0.000898;\n",
    "===========================================================\n",
    "Epoch 021/096 - Mean training loss 0.081767; Mean training F1 0.937308; Mean validation loss 0.058216; Mean validation F1 0.928022; Learning rate 0.000888;\n",
    "===========================================================\n",
    "Epoch 022/096 - Mean training loss 0.082050; Mean training F1 0.937071; Mean validation loss 0.098820; Mean validation F1 0.937473; Learning rate 0.000877;\n",
    "===========================================================\n",
    "Epoch 023/096 - Mean training loss 0.079917; Mean training F1 0.938869; Mean validation loss 0.085051; Mean validation F1 0.937081; Learning rate 0.000866;\n",
    "===========================================================\n",
    "Epoch 024/096 - Mean training loss 0.080283; Mean training F1 0.938710; Mean validation loss 0.060687; Mean validation F1 0.937170; Learning rate 0.000855;\n",
    "===========================================================\n",
    "Epoch 025/096 - Mean training loss 0.080829; Mean training F1 0.937806; Mean validation loss 0.074502; Mean validation F1 0.936441; Learning rate 0.000843;\n",
    "===========================================================\n",
    "Epoch 026/096 - Mean training loss 0.080012; Mean training F1 0.938487; Mean validation loss 0.057382; Mean validation F1 0.936658; Learning rate 0.000831;\n",
    "===========================================================\n",
    "Epoch 027/096 - Mean training loss 0.079799; Mean training F1 0.938719; Mean validation loss 0.092689; Mean validation F1 0.937431; Learning rate 0.000819;\n",
    "===========================================================\n",
    "Epoch 028/096 - Mean training loss 0.080385; Mean training F1 0.937966; Mean validation loss 0.073003; Mean validation F1 0.934594; Learning rate 0.000806;\n",
    "===========================================================\n",
    "Epoch 029/096 - Mean training loss 0.080614; Mean training F1 0.937905; Mean validation loss 0.089309; Mean validation F1 0.937382; Learning rate 0.000793;\n",
    "===========================================================\n",
    "Epoch 030/096 - Mean training loss 0.079415; Mean training F1 0.939116; Mean validation loss 0.088406; Mean validation F1 0.936227; Learning rate 0.000779;\n",
    "===========================================================\n",
    "Epoch 031/096 - Mean training loss 0.079512; Mean training F1 0.938914; Mean validation loss 0.050030; Mean validation F1 0.936945; Learning rate 0.000766;\n",
    "===========================================================\n",
    "Epoch 032/096 - Mean training loss 0.079062; Mean training F1 0.939143; Mean validation loss 0.096380; Mean validation F1 0.937682; Learning rate 0.000752;\n",
    "===========================================================\n",
    "Epoch 033/096 - Mean training loss 0.078721; Mean training F1 0.939343; Mean validation loss 0.076458; Mean validation F1 0.937801; Learning rate 0.000738;\n",
    "===========================================================\n",
    "Epoch 034/096 - Mean training loss 0.078316; Mean training F1 0.939610; Mean validation loss 0.114250; Mean validation F1 0.937117; Learning rate 0.000723;\n",
    "===========================================================\n",
    "Epoch 035/096 - Mean training loss 0.078844; Mean training F1 0.939150; Mean validation loss 0.081617; Mean validation F1 0.937349; Learning rate 0.000708;\n",
    "===========================================================\n",
    "Epoch 036/096 - Mean training loss 0.078992; Mean training F1 0.938403; Mean validation loss 0.082058; Mean validation F1 0.937486; Learning rate 0.000693;\n",
    "===========================================================\n",
    "Epoch 037/096 - Mean training loss 0.078089; Mean training F1 0.939713; Mean validation loss 0.075647; Mean validation F1 0.937040; Learning rate 0.000678;\n",
    "===========================================================\n",
    "Epoch 038/096 - Mean training loss 0.078140; Mean training F1 0.939783; Mean validation loss 0.082033; Mean validation F1 0.937430; Learning rate 0.000663;\n",
    "===========================================================\n",
    "Epoch 039/096 - Mean training loss 0.077526; Mean training F1 0.940037; Mean validation loss 0.073602; Mean validation F1 0.936886; Learning rate 0.000647;\n",
    "===========================================================\n",
    "Epoch 040/096 - Mean training loss 0.077518; Mean training F1 0.940130; Mean validation loss 0.050796; Mean validation F1 0.934146; Learning rate 0.000632;\n",
    "===========================================================\n",
    "Epoch 041/096 - Mean training loss 0.078199; Mean training F1 0.939442; Mean validation loss 0.109949; Mean validation F1 0.937417; Learning rate 0.000616;\n",
    "===========================================================\n",
    "Epoch 042/096 - Mean training loss 0.077268; Mean training F1 0.940282; Mean validation loss 0.052707; Mean validation F1 0.933899; Learning rate 0.000600;\n",
    "===========================================================\n",
    "Epoch 043/096 - Mean training loss 0.076866; Mean training F1 0.940408; Mean validation loss 0.050909; Mean validation F1 0.937417; Learning rate 0.000584;\n",
    "===========================================================\n",
    "Epoch 044/096 - Mean training loss 0.076911; Mean training F1 0.940461; Mean validation loss 0.141203; Mean validation F1 0.937523; Learning rate 0.000568;\n",
    "===========================================================\n",
    "Epoch 045/096 - Mean training loss 0.077458; Mean training F1 0.940036; Mean validation loss 0.066925; Mean validation F1 0.937172; Learning rate 0.000552;\n",
    "===========================================================\n",
    "Epoch 046/096 - Mean training loss 0.076662; Mean training F1 0.940401; Mean validation loss 0.080962; Mean validation F1 0.937458; Learning rate 0.000536;\n",
    "===========================================================\n",
    "Epoch 047/096 - Mean training loss 0.076486; Mean training F1 0.940972; Mean validation loss 0.076547; Mean validation F1 0.936617; Learning rate 0.000519;\n",
    "===========================================================\n",
    "Epoch 048/096 - Mean training loss 0.076087; Mean training F1 0.940827; Mean validation loss 0.076698; Mean validation F1 0.937036; Learning rate 0.000503;\n",
    "===========================================================\n",
    "Epoch 049/096 - Mean training loss 0.076025; Mean training F1 0.941046; Mean validation loss 0.095532; Mean validation F1 0.936913; Learning rate 0.000487;\n",
    "===========================================================\n",
    "Epoch 050/096 - Mean training loss 0.075818; Mean training F1 0.941160; Mean validation loss 0.104190; Mean validation F1 0.937241; Learning rate 0.000470;\n",
    "===========================================================\n",
    "Epoch 051/096 - Mean training loss 0.075425; Mean training F1 0.941369; Mean validation loss 0.057931; Mean validation F1 0.936944; Learning rate 0.000454;\n",
    "===========================================================\n",
    "Epoch 052/096 - Mean training loss 0.075327; Mean training F1 0.941193; Mean validation loss 0.052583; Mean validation F1 0.936817; Learning rate 0.000438;\n",
    "===========================================================\n",
    "Epoch 053/096 - Mean training loss 0.074924; Mean training F1 0.941735; Mean validation loss 0.066851; Mean validation F1 0.936067; Learning rate 0.000422;\n",
    "===========================================================\n",
    "Epoch 054/096 - Mean training loss 0.074679; Mean training F1 0.941692; Mean validation loss 0.098611; Mean validation F1 0.936963; Learning rate 0.000406;\n",
    "################################################################\n",
    "Training/validation for fold 3/5;\n",
    "===========================================================\n",
    "Epoch 001/096 - Mean training loss 0.525345; Mean training F1 0.696107; Mean validation loss 0.117748; Mean validation F1 0.916548; Learning rate 0.001000;\n",
    "===========================================================\n",
    "Epoch 002/096 - Mean training loss 0.109223; Mean training F1 0.925654; Mean validation loss 0.101960; Mean validation F1 0.933345; Learning rate 0.000999;\n",
    "===========================================================\n",
    "Epoch 003/096 - Mean training loss 0.090795; Mean training F1 0.933939; Mean validation loss 0.088996; Mean validation F1 0.930335; Learning rate 0.000998;\n",
    "===========================================================\n",
    "Epoch 004/096 - Mean training loss 0.088331; Mean training F1 0.935189; Mean validation loss 0.084803; Mean validation F1 0.934106; Learning rate 0.000996;\n",
    "===========================================================\n",
    "Epoch 005/096 - Mean training loss 0.086585; Mean training F1 0.935963; Mean validation loss 0.087597; Mean validation F1 0.935325; Learning rate 0.000994;\n",
    "===========================================================\n",
    "Epoch 006/096 - Mean training loss 0.085985; Mean training F1 0.935828; Mean validation loss 0.094480; Mean validation F1 0.936499; Learning rate 0.000991;\n",
    "===========================================================\n",
    "Epoch 007/096 - Mean training loss 0.084298; Mean training F1 0.936474; Mean validation loss 0.065176; Mean validation F1 0.935590; Learning rate 0.000988;\n",
    "===========================================================\n",
    "Epoch 008/096 - Mean training loss 0.083984; Mean training F1 0.936745; Mean validation loss 0.117710; Mean validation F1 0.935943; Learning rate 0.000984;\n",
    "===========================================================\n",
    "Epoch 009/096 - Mean training loss 0.083334; Mean training F1 0.936952; Mean validation loss 0.105029; Mean validation F1 0.936521; Learning rate 0.000979;\n",
    "===========================================================\n",
    "Epoch 010/096 - Mean training loss 0.083540; Mean training F1 0.936718; Mean validation loss 0.081714; Mean validation F1 0.937482; Learning rate 0.000974;\n",
    "===========================================================\n",
    "Epoch 011/096 - Mean training loss 0.082215; Mean training F1 0.937548; Mean validation loss 0.070453; Mean validation F1 0.936214; Learning rate 0.000969;\n",
    "===========================================================\n",
    "Epoch 012/096 - Mean training loss 0.084957; Mean training F1 0.935056; Mean validation loss 0.079003; Mean validation F1 0.936684; Learning rate 0.000963;\n",
    "===========================================================\n",
    "Epoch 013/096 - Mean training loss 0.082557; Mean training F1 0.937435; Mean validation loss 0.082781; Mean validation F1 0.936552; Learning rate 0.000956;\n",
    "===========================================================\n",
    "Epoch 014/096 - Mean training loss 0.082290; Mean training F1 0.937582; Mean validation loss 0.074576; Mean validation F1 0.932517; Learning rate 0.000949;\n",
    "===========================================================\n",
    "Epoch 015/096 - Mean training loss 0.081840; Mean training F1 0.937312; Mean validation loss 0.081070; Mean validation F1 0.932501; Learning rate 0.000942;\n",
    "===========================================================\n",
    "Epoch 016/096 - Mean training loss 0.082047; Mean training F1 0.937645; Mean validation loss 0.050125; Mean validation F1 0.933426; Learning rate 0.000934;\n",
    "===========================================================\n",
    "Epoch 017/096 - Mean training loss 0.082165; Mean training F1 0.937143; Mean validation loss 0.074381; Mean validation F1 0.936577; Learning rate 0.000926;\n",
    "===========================================================\n",
    "Epoch 018/096 - Mean training loss 0.080800; Mean training F1 0.938003; Mean validation loss 0.096657; Mean validation F1 0.937242; Learning rate 0.000917;\n",
    "===========================================================\n",
    "Epoch 019/096 - Mean training loss 0.080413; Mean training F1 0.938379; Mean validation loss 0.078693; Mean validation F1 0.936661; Learning rate 0.000907;\n",
    "===========================================================\n",
    "Epoch 020/096 - Mean training loss 0.080757; Mean training F1 0.938214; Mean validation loss 0.130426; Mean validation F1 0.936876; Learning rate 0.000898;\n",
    "===========================================================\n",
    "Epoch 021/096 - Mean training loss 0.080510; Mean training F1 0.938251; Mean validation loss 0.048228; Mean validation F1 0.936548; Learning rate 0.000888;\n",
    "===========================================================\n",
    "Epoch 022/096 - Mean training loss 0.080720; Mean training F1 0.937860; Mean validation loss 0.059841; Mean validation F1 0.936450; Learning rate 0.000877;\n",
    "===========================================================\n",
    "Epoch 023/096 - Mean training loss 0.079927; Mean training F1 0.938779; Mean validation loss 0.070538; Mean validation F1 0.935927; Learning rate 0.000866;\n",
    "===========================================================\n",
    "Epoch 024/096 - Mean training loss 0.080017; Mean training F1 0.938298; Mean validation loss 0.108394; Mean validation F1 0.937134; Learning rate 0.000855;\n",
    "===========================================================\n",
    "Epoch 025/096 - Mean training loss 0.080501; Mean training F1 0.938248; Mean validation loss 0.108139; Mean validation F1 0.936988; Learning rate 0.000843;\n",
    "===========================================================\n",
    "Epoch 026/096 - Mean training loss 0.080011; Mean training F1 0.938400; Mean validation loss 0.100422; Mean validation F1 0.936800; Learning rate 0.000831;\n",
    "===========================================================\n",
    "Epoch 027/096 - Mean training loss 0.079735; Mean training F1 0.938650; Mean validation loss 0.075621; Mean validation F1 0.936807; Learning rate 0.000819;\n",
    "===========================================================\n",
    "Epoch 028/096 - Mean training loss 0.079049; Mean training F1 0.939099; Mean validation loss 0.077873; Mean validation F1 0.937198; Learning rate 0.000806;\n",
    "===========================================================\n",
    "Epoch 029/096 - Mean training loss 0.079031; Mean training F1 0.939214; Mean validation loss 0.094699; Mean validation F1 0.936774; Learning rate 0.000793;\n",
    "===========================================================\n",
    "Epoch 030/096 - Mean training loss 0.079490; Mean training F1 0.938769; Mean validation loss 0.095926; Mean validation F1 0.937553; Learning rate 0.000779;\n",
    "===========================================================\n",
    "Epoch 031/096 - Mean training loss 0.079180; Mean training F1 0.939134; Mean validation loss 0.066624; Mean validation F1 0.936512; Learning rate 0.000766;\n",
    "===========================================================\n",
    "Epoch 032/096 - Mean training loss 0.078735; Mean training F1 0.939137; Mean validation loss 0.085495; Mean validation F1 0.937315; Learning rate 0.000752;\n",
    "===========================================================\n",
    "Epoch 033/096 - Mean training loss 0.078527; Mean training F1 0.939287; Mean validation loss 0.080851; Mean validation F1 0.935806; Learning rate 0.000738;\n",
    "===========================================================\n",
    "Epoch 034/096 - Mean training loss 0.079020; Mean training F1 0.938942; Mean validation loss 0.088406; Mean validation F1 0.936677; Learning rate 0.000723;\n",
    "===========================================================\n",
    "Epoch 035/096 - Mean training loss 0.078676; Mean training F1 0.939130; Mean validation loss 0.057637; Mean validation F1 0.937188; Learning rate 0.000708;\n",
    "===========================================================\n",
    "Epoch 036/096 - Mean training loss 0.077996; Mean training F1 0.939661; Mean validation loss 0.066947; Mean validation F1 0.936978; Learning rate 0.000693;\n",
    "===========================================================\n",
    "Epoch 037/096 - Mean training loss 0.077745; Mean training F1 0.939654; Mean validation loss 0.121481; Mean validation F1 0.936035; Learning rate 0.000678;\n",
    "===========================================================\n",
    "Epoch 038/096 - Mean training loss 0.077628; Mean training F1 0.939736; Mean validation loss 0.054306; Mean validation F1 0.936354; Learning rate 0.000663;\n",
    "===========================================================\n",
    "Epoch 039/096 - Mean training loss 0.077398; Mean training F1 0.940051; Mean validation loss 0.074975; Mean validation F1 0.936959; Learning rate 0.000647;\n",
    "===========================================================\n",
    "Epoch 040/096 - Mean training loss 0.077563; Mean training F1 0.939817; Mean validation loss 0.068319; Mean validation F1 0.937390; Learning rate 0.000632;\n",
    "===========================================================\n",
    "Epoch 041/096 - Mean training loss 0.077434; Mean training F1 0.939984; Mean validation loss 0.039765; Mean validation F1 0.936963; Learning rate 0.000616;\n",
    "===========================================================\n",
    "Epoch 042/096 - Mean training loss 0.076891; Mean training F1 0.940323; Mean validation loss 0.093193; Mean validation F1 0.937572; Learning rate 0.000600;\n",
    "===========================================================\n",
    "Epoch 043/096 - Mean training loss 0.076624; Mean training F1 0.940545; Mean validation loss 0.069943; Mean validation F1 0.936779; Learning rate 0.000584;\n",
    "===========================================================\n",
    "Epoch 044/096 - Mean training loss 0.076515; Mean training F1 0.940502; Mean validation loss 0.059603; Mean validation F1 0.937151; Learning rate 0.000568;\n",
    "===========================================================\n",
    "Epoch 045/096 - Mean training loss 0.076098; Mean training F1 0.940660; Mean validation loss 0.059407; Mean validation F1 0.937315; Learning rate 0.000552;\n",
    "===========================================================\n",
    "Epoch 046/096 - Mean training loss 0.075731; Mean training F1 0.941137; Mean validation loss 0.068505; Mean validation F1 0.936908; Learning rate 0.000536;\n",
    "===========================================================\n",
    "Epoch 047/096 - Mean training loss 0.075759; Mean training F1 0.941053; Mean validation loss 0.066467; Mean validation F1 0.936311; Learning rate 0.000519;\n",
    "===========================================================\n",
    "Epoch 048/096 - Mean training loss 0.075719; Mean training F1 0.940750; Mean validation loss 0.064738; Mean validation F1 0.936937; Learning rate 0.000503;\n",
    "===========================================================\n",
    "Epoch 049/096 - Mean training loss 0.075179; Mean training F1 0.941215; Mean validation loss 0.060010; Mean validation F1 0.937133; Learning rate 0.000487;\n",
    "===========================================================\n",
    "Epoch 050/096 - Mean training loss 0.075242; Mean training F1 0.941106; Mean validation loss 0.082198; Mean validation F1 0.936857; Learning rate 0.000470;\n",
    "===========================================================\n",
    "Epoch 051/096 - Mean training loss 0.074569; Mean training F1 0.941696; Mean validation loss 0.083155; Mean validation F1 0.936922; Learning rate 0.000454;\n",
    "===========================================================\n",
    "Epoch 052/096 - Mean training loss 0.074373; Mean training F1 0.941748; Mean validation loss 0.071412; Mean validation F1 0.935937; Learning rate 0.000438;\n",
    "===========================================================\n",
    "Epoch 053/096 - Mean training loss 0.074111; Mean training F1 0.941800; Mean validation loss 0.057536; Mean validation F1 0.935017; Learning rate 0.000422;\n",
    "===========================================================\n",
    "Epoch 054/096 - Mean training loss 0.073728; Mean training F1 0.942013; Mean validation loss 0.092219; Mean validation F1 0.936441; Learning rate 0.000406;\n",
    "===========================================================\n",
    "Epoch 055/096 - Mean training loss 0.073464; Mean training F1 0.942214; Mean validation loss 0.076309; Mean validation F1 0.936982; Learning rate 0.000390;\n",
    "===========================================================\n",
    "Epoch 056/096 - Mean training loss 0.072901; Mean training F1 0.942616; Mean validation loss 0.090715; Mean validation F1 0.936309; Learning rate 0.000374;\n",
    "===========================================================\n",
    "Epoch 057/096 - Mean training loss 0.072823; Mean training F1 0.942794; Mean validation loss 0.077575; Mean validation F1 0.936248; Learning rate 0.000359;\n",
    "===========================================================\n",
    "Epoch 058/096 - Mean training loss 0.072332; Mean training F1 0.942904; Mean validation loss 0.044868; Mean validation F1 0.936241; Learning rate 0.000343;\n",
    "===========================================================\n",
    "Epoch 059/096 - Mean training loss 0.071725; Mean training F1 0.943414; Mean validation loss 0.096194; Mean validation F1 0.936571; Learning rate 0.000328;\n",
    "===========================================================\n",
    "Epoch 060/096 - Mean training loss 0.071276; Mean training F1 0.943672; Mean validation loss 0.049210; Mean validation F1 0.935929; Learning rate 0.000313;\n",
    "===========================================================\n",
    "Epoch 061/096 - Mean training loss 0.070895; Mean training F1 0.943936; Mean validation loss 0.073579; Mean validation F1 0.936027; Learning rate 0.000298;\n",
    "===========================================================\n",
    "Epoch 062/096 - Mean training loss 0.070403; Mean training F1 0.944147; Mean validation loss 0.052764; Mean validation F1 0.936150; Learning rate 0.000283;\n",
    "===========================================================\n",
    "Epoch 063/096 - Mean training loss 0.069900; Mean training F1 0.944463; Mean validation loss 0.061916; Mean validation F1 0.935749; Learning rate 0.000268;\n",
    "################################################################\n",
    "Training/validation for fold 4/5;\n",
    "===========================================================\n",
    "Epoch 001/096 - Mean training loss 0.516431; Mean training F1 0.706430; Mean validation loss 0.157656; Mean validation F1 0.899447; Learning rate 0.001000;\n",
    "===========================================================\n",
    "Epoch 002/096 - Mean training loss 0.105281; Mean training F1 0.929815; Mean validation loss 0.129303; Mean validation F1 0.931164; Learning rate 0.000999;\n",
    "===========================================================\n",
    "Epoch 003/096 - Mean training loss 0.093229; Mean training F1 0.932906; Mean validation loss 0.100408; Mean validation F1 0.936616; Learning rate 0.000998;\n",
    "===========================================================\n",
    "Epoch 004/096 - Mean training loss 0.089680; Mean training F1 0.935044; Mean validation loss 0.084001; Mean validation F1 0.933690; Learning rate 0.000996;\n",
    "===========================================================\n",
    "Epoch 005/096 - Mean training loss 0.087382; Mean training F1 0.936255; Mean validation loss 0.086108; Mean validation F1 0.937617; Learning rate 0.000994;\n",
    "===========================================================\n",
    "Epoch 006/096 - Mean training loss 0.086161; Mean training F1 0.935860; Mean validation loss 0.059875; Mean validation F1 0.936548; Learning rate 0.000991;\n",
    "===========================================================\n",
    "Epoch 007/096 - Mean training loss 0.085591; Mean training F1 0.936549; Mean validation loss 0.075837; Mean validation F1 0.936576; Learning rate 0.000988;\n",
    "===========================================================\n",
    "Epoch 008/096 - Mean training loss 0.085446; Mean training F1 0.936326; Mean validation loss 0.068266; Mean validation F1 0.931305; Learning rate 0.000984;\n",
    "===========================================================\n",
    "Epoch 009/096 - Mean training loss 0.085622; Mean training F1 0.935734; Mean validation loss 0.069348; Mean validation F1 0.937503; Learning rate 0.000979;\n",
    "===========================================================\n",
    "Epoch 010/096 - Mean training loss 0.085327; Mean training F1 0.935915; Mean validation loss 0.110802; Mean validation F1 0.935532; Learning rate 0.000974;\n",
    "===========================================================\n",
    "Epoch 011/096 - Mean training loss 0.084573; Mean training F1 0.936586; Mean validation loss 0.081215; Mean validation F1 0.938053; Learning rate 0.000969;\n",
    "===========================================================\n",
    "Epoch 012/096 - Mean training loss 0.084358; Mean training F1 0.936511; Mean validation loss 0.056246; Mean validation F1 0.915198; Learning rate 0.000963;\n",
    "===========================================================\n",
    "Epoch 013/096 - Mean training loss 0.084665; Mean training F1 0.935939; Mean validation loss 0.089797; Mean validation F1 0.936749; Learning rate 0.000956;\n",
    "===========================================================\n",
    "Epoch 014/096 - Mean training loss 0.083641; Mean training F1 0.937343; Mean validation loss 0.079582; Mean validation F1 0.937299; Learning rate 0.000949;\n",
    "===========================================================\n",
    "Epoch 015/096 - Mean training loss 0.082773; Mean training F1 0.937624; Mean validation loss 0.085849; Mean validation F1 0.938499; Learning rate 0.000942;\n",
    "===========================================================\n",
    "Epoch 016/096 - Mean training loss 0.082800; Mean training F1 0.937534; Mean validation loss 0.064528; Mean validation F1 0.937870; Learning rate 0.000934;\n",
    "===========================================================\n",
    "Epoch 017/096 - Mean training loss 0.083188; Mean training F1 0.937112; Mean validation loss 0.118460; Mean validation F1 0.937149; Learning rate 0.000926;\n",
    "===========================================================\n",
    "Epoch 018/096 - Mean training loss 0.082949; Mean training F1 0.936963; Mean validation loss 0.085635; Mean validation F1 0.936593; Learning rate 0.000917;\n",
    "===========================================================\n",
    "Epoch 019/096 - Mean training loss 0.082763; Mean training F1 0.937298; Mean validation loss 0.075303; Mean validation F1 0.937439; Learning rate 0.000907;\n",
    "===========================================================\n",
    "Epoch 020/096 - Mean training loss 0.081896; Mean training F1 0.937712; Mean validation loss 0.059499; Mean validation F1 0.938283; Learning rate 0.000898;\n",
    "===========================================================\n",
    "Epoch 021/096 - Mean training loss 0.081711; Mean training F1 0.937866; Mean validation loss 0.089996; Mean validation F1 0.937923; Learning rate 0.000888;\n",
    "===========================================================\n",
    "Epoch 022/096 - Mean training loss 0.081727; Mean training F1 0.938058; Mean validation loss 0.065575; Mean validation F1 0.934842; Learning rate 0.000877;\n",
    "===========================================================\n",
    "Epoch 023/096 - Mean training loss 0.080911; Mean training F1 0.938734; Mean validation loss 0.082756; Mean validation F1 0.939043; Learning rate 0.000866;\n",
    "===========================================================\n",
    "Epoch 024/096 - Mean training loss 0.081443; Mean training F1 0.937958; Mean validation loss 0.086582; Mean validation F1 0.937612; Learning rate 0.000855;\n",
    "===========================================================\n",
    "Epoch 025/096 - Mean training loss 0.081510; Mean training F1 0.937972; Mean validation loss 0.076012; Mean validation F1 0.937919; Learning rate 0.000843;\n",
    "===========================================================\n",
    "Epoch 026/096 - Mean training loss 0.080498; Mean training F1 0.938566; Mean validation loss 0.058771; Mean validation F1 0.929397; Learning rate 0.000831;\n",
    "===========================================================\n",
    "Epoch 027/096 - Mean training loss 0.081018; Mean training F1 0.938351; Mean validation loss 0.078047; Mean validation F1 0.939049; Learning rate 0.000819;\n",
    "===========================================================\n",
    "Epoch 028/096 - Mean training loss 0.080785; Mean training F1 0.938223; Mean validation loss 0.090134; Mean validation F1 0.938683; Learning rate 0.000806;\n",
    "===========================================================\n",
    "Epoch 029/096 - Mean training loss 0.080227; Mean training F1 0.938714; Mean validation loss 0.078085; Mean validation F1 0.939015; Learning rate 0.000793;\n",
    "===========================================================\n",
    "Epoch 030/096 - Mean training loss 0.080321; Mean training F1 0.938455; Mean validation loss 0.100089; Mean validation F1 0.938742; Learning rate 0.000779;\n",
    "===========================================================\n",
    "Epoch 031/096 - Mean training loss 0.079484; Mean training F1 0.939320; Mean validation loss 0.078501; Mean validation F1 0.938688; Learning rate 0.000766;\n",
    "===========================================================\n",
    "Epoch 032/096 - Mean training loss 0.079678; Mean training F1 0.939166; Mean validation loss 0.096394; Mean validation F1 0.938316; Learning rate 0.000752;\n",
    "===========================================================\n",
    "Epoch 033/096 - Mean training loss 0.080023; Mean training F1 0.938855; Mean validation loss 0.040020; Mean validation F1 0.937276; Learning rate 0.000738;\n",
    "===========================================================\n",
    "Epoch 034/096 - Mean training loss 0.079390; Mean training F1 0.939098; Mean validation loss 0.087463; Mean validation F1 0.939171; Learning rate 0.000723;\n",
    "===========================================================\n",
    "Epoch 035/096 - Mean training loss 0.079555; Mean training F1 0.939142; Mean validation loss 0.084554; Mean validation F1 0.938502; Learning rate 0.000708;\n",
    "===========================================================\n",
    "Epoch 036/096 - Mean training loss 0.079124; Mean training F1 0.939219; Mean validation loss 0.091804; Mean validation F1 0.938337; Learning rate 0.000693;\n",
    "===========================================================\n",
    "Epoch 037/096 - Mean training loss 0.078776; Mean training F1 0.939425; Mean validation loss 0.071831; Mean validation F1 0.938379; Learning rate 0.000678;\n",
    "===========================================================\n",
    "Epoch 038/096 - Mean training loss 0.078621; Mean training F1 0.939710; Mean validation loss 0.036755; Mean validation F1 0.937237; Learning rate 0.000663;\n",
    "===========================================================\n",
    "Epoch 039/096 - Mean training loss 0.078208; Mean training F1 0.939862; Mean validation loss 0.093551; Mean validation F1 0.938596; Learning rate 0.000647;\n",
    "===========================================================\n",
    "Epoch 040/096 - Mean training loss 0.078001; Mean training F1 0.940160; Mean validation loss 0.055330; Mean validation F1 0.938567; Learning rate 0.000632;\n",
    "===========================================================\n",
    "Epoch 041/096 - Mean training loss 0.077564; Mean training F1 0.940377; Mean validation loss 0.075254; Mean validation F1 0.939168; Learning rate 0.000616;\n",
    "===========================================================\n",
    "Epoch 042/096 - Mean training loss 0.077321; Mean training F1 0.940535; Mean validation loss 0.053862; Mean validation F1 0.936440; Learning rate 0.000600;\n",
    "===========================================================\n",
    "Epoch 043/096 - Mean training loss 0.076854; Mean training F1 0.940752; Mean validation loss 0.065178; Mean validation F1 0.935408; Learning rate 0.000584;\n",
    "===========================================================\n",
    "Epoch 044/096 - Mean training loss 0.076839; Mean training F1 0.940617; Mean validation loss 0.090325; Mean validation F1 0.938017; Learning rate 0.000568;\n",
    "===========================================================\n",
    "Epoch 045/096 - Mean training loss 0.076190; Mean training F1 0.941252; Mean validation loss 0.056598; Mean validation F1 0.938155; Learning rate 0.000552;\n",
    "===========================================================\n",
    "Epoch 046/096 - Mean training loss 0.076281; Mean training F1 0.941319; Mean validation loss 0.072863; Mean validation F1 0.938352; Learning rate 0.000536;\n",
    "===========================================================\n",
    "Epoch 047/096 - Mean training loss 0.075586; Mean training F1 0.941646; Mean validation loss 0.074843; Mean validation F1 0.938546; Learning rate 0.000519;\n",
    "===========================================================\n",
    "Epoch 048/096 - Mean training loss 0.075389; Mean training F1 0.941735; Mean validation loss 0.093167; Mean validation F1 0.937563; Learning rate 0.000503;\n",
    "===========================================================\n",
    "Epoch 049/096 - Mean training loss 0.074920; Mean training F1 0.942009; Mean validation loss 0.092682; Mean validation F1 0.937238; Learning rate 0.000487;\n",
    "===========================================================\n",
    "Epoch 050/096 - Mean training loss 0.074118; Mean training F1 0.942569; Mean validation loss 0.080040; Mean validation F1 0.938167; Learning rate 0.000470;\n",
    "===========================================================\n",
    "Epoch 051/096 - Mean training loss 0.073887; Mean training F1 0.942647; Mean validation loss 0.074987; Mean validation F1 0.938361; Learning rate 0.000454;\n",
    "===========================================================\n",
    "Epoch 052/096 - Mean training loss 0.073356; Mean training F1 0.943126; Mean validation loss 0.086918; Mean validation F1 0.938259; Learning rate 0.000438;\n",
    "===========================================================\n",
    "Epoch 053/096 - Mean training loss 0.072443; Mean training F1 0.943660; Mean validation loss 0.083780; Mean validation F1 0.937516; Learning rate 0.000422;\n",
    "===========================================================\n",
    "Epoch 054/096 - Mean training loss 0.071794; Mean training F1 0.944022; Mean validation loss 0.069762; Mean validation F1 0.937826; Learning rate 0.000406;\n",
    "===========================================================\n",
    "Epoch 055/096 - Mean training loss 0.071117; Mean training F1 0.944694; Mean validation loss 0.091770; Mean validation F1 0.937169; Learning rate 0.000390;\n",
    "################################################################\n",
    "Training/validation for fold 5/5;\n",
    "===========================================================\n",
    "Epoch 001/096 - Mean training loss 0.512450; Mean training F1 0.704669; Mean validation loss 0.127408; Mean validation F1 0.922793; Learning rate 0.001000;\n",
    "===========================================================\n",
    "Epoch 002/096 - Mean training loss 0.108762; Mean training F1 0.929649; Mean validation loss 0.120044; Mean validation F1 0.930619; Learning rate 0.000999;\n",
    "===========================================================\n",
    "Epoch 003/096 - Mean training loss 0.094265; Mean training F1 0.932344; Mean validation loss 0.089755; Mean validation F1 0.936259; Learning rate 0.000998;\n",
    "===========================================================\n",
    "Epoch 004/096 - Mean training loss 0.092009; Mean training F1 0.933653; Mean validation loss 0.083549; Mean validation F1 0.936869; Learning rate 0.000996;\n",
    "===========================================================\n",
    "Epoch 005/096 - Mean training loss 0.088206; Mean training F1 0.935274; Mean validation loss 0.066628; Mean validation F1 0.932495; Learning rate 0.000994;\n",
    "===========================================================\n",
    "Epoch 006/096 - Mean training loss 0.091607; Mean training F1 0.931183; Mean validation loss 0.097725; Mean validation F1 0.930363; Learning rate 0.000991;\n",
    "===========================================================\n",
    "Epoch 007/096 - Mean training loss 0.086392; Mean training F1 0.935833; Mean validation loss 0.134914; Mean validation F1 0.937558; Learning rate 0.000988;\n",
    "===========================================================\n",
    "Epoch 008/096 - Mean training loss 0.084476; Mean training F1 0.937433; Mean validation loss 0.114162; Mean validation F1 0.937324; Learning rate 0.000984;\n",
    "===========================================================\n",
    "Epoch 009/096 - Mean training loss 0.084482; Mean training F1 0.937248; Mean validation loss 0.106940; Mean validation F1 0.937726; Learning rate 0.000979;\n",
    "===========================================================\n",
    "Epoch 010/096 - Mean training loss 0.084133; Mean training F1 0.937295; Mean validation loss 0.092704; Mean validation F1 0.936342; Learning rate 0.000974;\n",
    "===========================================================\n",
    "Epoch 011/096 - Mean training loss 0.084394; Mean training F1 0.937057; Mean validation loss 0.069769; Mean validation F1 0.932651; Learning rate 0.000969;\n",
    "===========================================================\n",
    "Epoch 012/096 - Mean training loss 0.083656; Mean training F1 0.937026; Mean validation loss 0.106467; Mean validation F1 0.937690; Learning rate 0.000963;\n",
    "===========================================================\n",
    "Epoch 013/096 - Mean training loss 0.083638; Mean training F1 0.937092; Mean validation loss 0.143796; Mean validation F1 0.936662; Learning rate 0.000956;\n",
    "===========================================================\n",
    "Epoch 014/096 - Mean training loss 0.083199; Mean training F1 0.937362; Mean validation loss 0.057015; Mean validation F1 0.937381; Learning rate 0.000949;\n",
    "===========================================================\n",
    "Epoch 015/096 - Mean training loss 0.083662; Mean training F1 0.936779; Mean validation loss 0.069282; Mean validation F1 0.937640; Learning rate 0.000942;\n",
    "===========================================================\n",
    "Epoch 016/096 - Mean training loss 0.082277; Mean training F1 0.937924; Mean validation loss 0.058127; Mean validation F1 0.936228; Learning rate 0.000934;\n",
    "===========================================================\n",
    "Epoch 017/096 - Mean training loss 0.082990; Mean training F1 0.937577; Mean validation loss 0.075559; Mean validation F1 0.937260; Learning rate 0.000926;\n",
    "===========================================================\n",
    "Epoch 018/096 - Mean training loss 0.082715; Mean training F1 0.937688; Mean validation loss 0.074387; Mean validation F1 0.935302; Learning rate 0.000917;\n",
    "===========================================================\n",
    "Epoch 019/096 - Mean training loss 0.082957; Mean training F1 0.937586; Mean validation loss 0.083636; Mean validation F1 0.936029; Learning rate 0.000907;\n",
    "===========================================================\n",
    "Epoch 020/096 - Mean training loss 0.081854; Mean training F1 0.938223; Mean validation loss 0.106411; Mean validation F1 0.937761; Learning rate 0.000898;\n",
    "===========================================================\n",
    "Epoch 021/096 - Mean training loss 0.082564; Mean training F1 0.937363; Mean validation loss 0.111286; Mean validation F1 0.937992; Learning rate 0.000888;\n",
    "===========================================================\n",
    "Epoch 022/096 - Mean training loss 0.081103; Mean training F1 0.938763; Mean validation loss 0.061802; Mean validation F1 0.938218; Learning rate 0.000877;\n",
    "===========================================================\n",
    "Epoch 023/096 - Mean training loss 0.081254; Mean training F1 0.938530; Mean validation loss 0.085220; Mean validation F1 0.938183; Learning rate 0.000866;\n",
    "===========================================================\n",
    "Epoch 024/096 - Mean training loss 0.081132; Mean training F1 0.938560; Mean validation loss 0.066151; Mean validation F1 0.935092; Learning rate 0.000855;\n",
    "===========================================================\n",
    "Epoch 025/096 - Mean training loss 0.081724; Mean training F1 0.937932; Mean validation loss 0.083868; Mean validation F1 0.937739; Learning rate 0.000843;\n",
    "===========================================================\n",
    "Epoch 026/096 - Mean training loss 0.082617; Mean training F1 0.936914; Mean validation loss 0.080032; Mean validation F1 0.937975; Learning rate 0.000831;\n",
    "===========================================================\n",
    "Epoch 027/096 - Mean training loss 0.080630; Mean training F1 0.938894; Mean validation loss 0.069103; Mean validation F1 0.937740; Learning rate 0.000819;\n",
    "===========================================================\n",
    "Epoch 028/096 - Mean training loss 0.080277; Mean training F1 0.938893; Mean validation loss 0.066327; Mean validation F1 0.937465; Learning rate 0.000806;\n",
    "===========================================================\n",
    "Epoch 029/096 - Mean training loss 0.080223; Mean training F1 0.938841; Mean validation loss 0.064501; Mean validation F1 0.933436; Learning rate 0.000793;\n",
    "===========================================================\n",
    "Epoch 030/096 - Mean training loss 0.080695; Mean training F1 0.938445; Mean validation loss 0.053320; Mean validation F1 0.938333; Learning rate 0.000779;\n",
    "===========================================================\n",
    "Epoch 031/096 - Mean training loss 0.079739; Mean training F1 0.939154; Mean validation loss 0.062363; Mean validation F1 0.937498; Learning rate 0.000766;\n",
    "===========================================================\n",
    "Epoch 032/096 - Mean training loss 0.080297; Mean training F1 0.938961; Mean validation loss 0.063914; Mean validation F1 0.938228; Learning rate 0.000752;\n",
    "===========================================================\n",
    "Epoch 033/096 - Mean training loss 0.079743; Mean training F1 0.939218; Mean validation loss 0.071326; Mean validation F1 0.937708; Learning rate 0.000738;\n",
    "===========================================================\n",
    "Epoch 034/096 - Mean training loss 0.079566; Mean training F1 0.939036; Mean validation loss 0.061937; Mean validation F1 0.937257; Learning rate 0.000723;\n",
    "===========================================================\n",
    "Epoch 035/096 - Mean training loss 0.079869; Mean training F1 0.938942; Mean validation loss 0.037755; Mean validation F1 0.938114; Learning rate 0.000708;\n",
    "===========================================================\n",
    "Epoch 036/096 - Mean training loss 0.079102; Mean training F1 0.939629; Mean validation loss 0.059569; Mean validation F1 0.937852; Learning rate 0.000693;\n",
    "===========================================================\n",
    "Epoch 037/096 - Mean training loss 0.079201; Mean training F1 0.939362; Mean validation loss 0.032088; Mean validation F1 0.938076; Learning rate 0.000678;\n",
    "===========================================================\n",
    "Epoch 038/096 - Mean training loss 0.078796; Mean training F1 0.939588; Mean validation loss 0.075962; Mean validation F1 0.936532; Learning rate 0.000663;\n",
    "===========================================================\n",
    "Epoch 039/096 - Mean training loss 0.078767; Mean training F1 0.939538; Mean validation loss 0.075177; Mean validation F1 0.938344; Learning rate 0.000647;\n",
    "===========================================================\n",
    "Epoch 040/096 - Mean training loss 0.078205; Mean training F1 0.940172; Mean validation loss 0.077386; Mean validation F1 0.938365; Learning rate 0.000632;\n",
    "===========================================================\n",
    "Epoch 041/096 - Mean training loss 0.077789; Mean training F1 0.940206; Mean validation loss 0.052870; Mean validation F1 0.937634; Learning rate 0.000616;\n",
    "===========================================================\n",
    "Epoch 042/096 - Mean training loss 0.077823; Mean training F1 0.940253; Mean validation loss 0.107542; Mean validation F1 0.937526; Learning rate 0.000600;\n",
    "===========================================================\n",
    "Epoch 043/096 - Mean training loss 0.077953; Mean training F1 0.939914; Mean validation loss 0.045878; Mean validation F1 0.937189; Learning rate 0.000584;\n",
    "===========================================================\n",
    "Epoch 044/096 - Mean training loss 0.077047; Mean training F1 0.940687; Mean validation loss 0.090659; Mean validation F1 0.937039; Learning rate 0.000568;\n",
    "===========================================================\n",
    "Epoch 045/096 - Mean training loss 0.077210; Mean training F1 0.940452; Mean validation loss 0.099816; Mean validation F1 0.938094; Learning rate 0.000552;\n",
    "===========================================================\n",
    "Epoch 046/096 - Mean training loss 0.076701; Mean training F1 0.940875; Mean validation loss 0.101843; Mean validation F1 0.938186; Learning rate 0.000536;\n",
    "===========================================================\n",
    "Epoch 047/096 - Mean training loss 0.076777; Mean training F1 0.940557; Mean validation loss 0.068809; Mean validation F1 0.937253; Learning rate 0.000519;\n",
    "===========================================================\n",
    "Epoch 048/096 - Mean training loss 0.075993; Mean training F1 0.941140; Mean validation loss 0.105771; Mean validation F1 0.938031; Learning rate 0.000503;\n",
    "===========================================================\n",
    "Epoch 049/096 - Mean training loss 0.075626; Mean training F1 0.941334; Mean validation loss 0.081205; Mean validation F1 0.937123; Learning rate 0.000487;\n",
    "===========================================================\n",
    "Epoch 050/096 - Mean training loss 0.075416; Mean training F1 0.941434; Mean validation loss 0.099273; Mean validation F1 0.938036; Learning rate 0.000470;\n",
    "===========================================================\n",
    "Epoch 051/096 - Mean training loss 0.074745; Mean training F1 0.941938; Mean validation loss 0.064485; Mean validation F1 0.937843; Learning rate 0.000454;\n",
    "===========================================================\n",
    "Epoch 052/096 - Mean training loss 0.074302; Mean training F1 0.942073; Mean validation loss 0.057841; Mean validation F1 0.937899; Learning rate 0.000438;\n",
    "===========================================================\n",
    "Epoch 053/096 - Mean training loss 0.073929; Mean training F1 0.942324; Mean validation loss 0.058807; Mean validation F1 0.937681; Learning rate 0.000422;\n",
    "===========================================================\n",
    "Epoch 054/096 - Mean training loss 0.073491; Mean training F1 0.942445; Mean validation loss 0.044261; Mean validation F1 0.937720; Learning rate 0.000406;\n",
    "===========================================================\n",
    "Epoch 055/096 - Mean training loss 0.073154; Mean training F1 0.942609; Mean validation loss 0.030000; Mean validation F1 0.936909; Learning rate 0.000390;\n",
    "===========================================================\n",
    "Epoch 056/096 - Mean training loss 0.072428; Mean training F1 0.943122; Mean validation loss 0.067273; Mean validation F1 0.937652; Learning rate 0.000374;\n",
    "===========================================================\n",
    "Epoch 057/096 - Mean training loss 0.071957; Mean training F1 0.943224; Mean validation loss 0.074976; Mean validation F1 0.937411; Learning rate 0.000359;\n",
    "===========================================================\n",
    "Epoch 058/096 - Mean training loss 0.071460; Mean training F1 0.943649; Mean validation loss 0.091941; Mean validation F1 0.937425; Learning rate 0.000343;\n",
    "===========================================================\n",
    "Epoch 059/096 - Mean training loss 0.070912; Mean training F1 0.944070; Mean validation loss 0.060768; Mean validation F1 0.936921; Learning rate 0.000328;\n",
    "===========================================================\n",
    "Epoch 060/096 - Mean training loss 0.070462; Mean training F1 0.944284; Mean validation loss 0.057042; Mean validation F1 0.936978; Learning rate 0.000313;\n",
    "===========================================================\n",
    "Epoch 061/096 - Mean training loss 0.069798; Mean training F1 0.944641; Mean validation loss 0.076761; Mean validation F1 0.936859; Learning rate 0.000298;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
