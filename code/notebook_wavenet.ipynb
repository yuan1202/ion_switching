{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bloscpack as bp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from NNs import Wave_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb0279e67b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "EPOCHS = 96\n",
    "BATCHSIZE = 32\n",
    "SEED = 19550423\n",
    "LR = 0.001\n",
    "SPLITS = 5\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_fs = sorted([f for f in os.listdir('../input/') if (('trn_srs_dat' in f) and ('s1000' in f) and ('w4000' in f))])\n",
    "lbl_fs = sorted([f for f in os.listdir('../input/') if ('trn_srs_lbl' in f) and ('s1000' in f) and ('w4000' in f)])\n",
    "\n",
    "# tst_fs = sorted([f for f in os.listdir('../input/') if (('tst_srs_dat' in f) and ('s1000' in f) and ('w1000' in f))])\n",
    "# tst_fs = [tst_fs[i] for i in [0, 11, 12, 13, 14, 15, 16, 17, 18, 19]] + tst_fs[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_trn = np.concatenate(\n",
    "    [bp.unpack_ndarray_from_file(os.path.join('../input/', f)) for f in trn_fs],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "series_lbl = [bp.unpack_ndarray_from_file(os.path.join('../input/', f)) for f in lbl_fs]\n",
    "\n",
    "series_grp = np.concatenate(\n",
    "    [np.ones(shape=(arr.shape[0],)) * i for i, arr in zip([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], series_lbl)],\n",
    "    axis=0\n",
    ").astype(int)\n",
    "\n",
    "series_lbl = np.concatenate(\n",
    "    series_lbl,\n",
    "    axis=0\n",
    ")[:, :, None]\n",
    "\n",
    "# series_tst = np.concatenate(\n",
    "#     [bp.unpack_ndarray_from_file(os.path.join('../input/', f)) for f in tst_fs],\n",
    "#     axis=0\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4970, 4000, 49)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(series_trn.shape[-1]):\n",
    "    high = series_trn[:, :, i].max()\n",
    "    low = series_trn[:, :, i].min()\n",
    "#     high= max(series_trn[:, :, i].max(), series_tst[:, :, i].max())\n",
    "#     low = min(series_trn[:, :, i].min(), series_tst[:, :, i].min())\n",
    "    series_trn[:, :, i] = 2 * (series_trn[:, :, i] - low) / (high - low) - 1\n",
    "#     series_tst[:, :, i] = 2 * (series_tst[:, :, i] - low) / (high - low) - 1\n",
    "#     print('---------')\n",
    "#     print('{:d} - max {:.3f}; min {:.3f};'.format(i, series_trn[:, :, i].max(), series_trn[:, :, i].min()))\n",
    "#     print('{:d} - max {:.3f}; min {:.3f};'.format(i, series_tst[:, :, i].max(), series_tst[:, :, i].min()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(series_dat.shape[-1]):\n",
    "    print(i, series_dat[:, :, i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def skewness_class(skw):\n",
    "#     if skw < -.7:\n",
    "#         return 0\n",
    "#     elif -.7 <= skw < -.42:\n",
    "#         return 1\n",
    "#     elif -.42 <= skw < -.27:\n",
    "#         return 2\n",
    "#     elif -.27 <= skw:\n",
    "#         return 3\n",
    "    \n",
    "# lbl_skewness = [skewness_class(skew(lst)) for lst in series_lbl.squeeze(-1).tolist()]\n",
    "\n",
    "lbl_skewness = [np.unique(lst).shape[0] for lst in series_lbl.squeeze(-1).tolist()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lbl_skewness = [skew(lst) for lst in series_lbl.squeeze(-1).tolist()]\n",
    "cnts, bins = np.histogram(lbl_skewness, bins=(-100, -.7, -.42, -.27, 100))\n",
    "\n",
    "for c, b0, b1 in zip(cnts, bins[:-1], bins[1:]):\n",
    "    print(c, b0, b1)\n",
    "    \n",
    "'''\n",
    "868 -100.0 -0.8\n",
    "898 -0.8 -0.48\n",
    "845 -0.48 -0.35\n",
    "885 -0.35 -0.22\n",
    "479 -0.22 0.0\n",
    "640 0.0 1e-05\n",
    "375 1e-05 100.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf_grp = [str(a) + '_' + str(b) for a, b in zip(series_grp, lbl_skewness)]\n",
    "us = np.unique(skf_grp)\n",
    "umap = {u: i for u, i in zip(us, range(len(us)))}\n",
    "skf_grp = [umap[u] for u in skf_grp]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "us, cs = np.unique(skf_grp, return_counts=True)\n",
    "for u, c in zip(us, cs):\n",
    "    print(u, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Waveset(Dataset):\n",
    "    def __init__(self, data, labels=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        \n",
    "        if self.labels is None:\n",
    "            return data.astype(np.float32)\n",
    "        else:\n",
    "            labels = self.labels[idx]\n",
    "            return (data.astype(np.float32), labels.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_train_validate(model, optimizer, criterion, scheduler, training_loader, validation_loader, fold_number):\n",
    "\n",
    "    trn_losses = [np.nan]\n",
    "    vld_losses = [np.nan]\n",
    "    vld_f1s = [np.nan]\n",
    "\n",
    "    for epc in range(EPOCHS):\n",
    "        print('===========================================================')\n",
    "\n",
    "        epoch_trn_losses = []\n",
    "        epoch_trn_lbls = []\n",
    "        epoch_trn_prds = []\n",
    "        epoch_vld_losses = []\n",
    "        epoch_vld_lbls = []\n",
    "        epoch_vld_prds = []\n",
    "\n",
    "        # ------ training ------\n",
    "        model.train()\n",
    "        for i, (trn_batch_dat, trn_batch_lbl) in enumerate(training_loader):\n",
    "            trn_batch_dat, trn_batch_lbl = trn_batch_dat.to(DEVICE), trn_batch_lbl.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            trn_batch_prd = model(trn_batch_dat)\n",
    "            trn_batch_prd = trn_batch_prd.view(-1, trn_batch_prd.size(-1))\n",
    "            trn_batch_lbl = trn_batch_lbl.view(-1)\n",
    "            loss = criterion(trn_batch_prd, trn_batch_lbl)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_trn_losses.append(loss.item())\n",
    "            epoch_trn_lbls.append(trn_batch_lbl.detach().cpu().numpy())\n",
    "            epoch_trn_prds.append(trn_batch_prd.detach().cpu().numpy())\n",
    "\n",
    "            print(\n",
    "                'Epoch {:03d}/{:03d} - Training batch {:04d}/{:04d}: Training loss {:.6f};'.format(\n",
    "                    epc + 1, EPOCHS, i + 1, len(training_loader), epoch_trn_losses[-1],\n",
    "                ), \n",
    "                end='\\r'\n",
    "            )\n",
    "\n",
    "        # ------ validation ------\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (vld_batch_dat, vld_batch_lbl) in enumerate(validation_loader):\n",
    "                vld_batch_dat, vld_batch_lbl = vld_batch_dat.to(DEVICE), vld_batch_lbl.to(DEVICE)\n",
    "\n",
    "                vld_batch_prd = model(vld_batch_dat)\n",
    "                vld_batch_prd = vld_batch_prd.view(-1, vld_batch_prd.size(-1))\n",
    "                vld_batch_lbl = vld_batch_lbl.view(-1)\n",
    "                loss = criterion(trn_batch_prd, trn_batch_lbl)\n",
    "\n",
    "                epoch_vld_losses.append(loss.item())\n",
    "                epoch_vld_lbls.append(vld_batch_lbl.detach().cpu().numpy())\n",
    "                epoch_vld_prds.append(vld_batch_prd.detach().cpu().numpy())\n",
    "\n",
    "                print(\n",
    "                    'Epoch {:03d}/{:03d} - Validation batch {:04d}/{:04d}: Validation loss {:.6f};'.format(\n",
    "                        epc + 1, EPOCHS, i + 1, len(validation_loader), epoch_vld_losses[-1],\n",
    "                    ), \n",
    "                    end='\\r'\n",
    "                )\n",
    "\n",
    "        # ------ epoch end ------\n",
    "        f1_trn = f1_score(\n",
    "            np.concatenate(epoch_trn_lbls, axis=0), \n",
    "            np.concatenate(epoch_trn_prds, axis=0).argmax(1),\n",
    "            labels=list(range(11)), \n",
    "            average='macro'\n",
    "        )\n",
    "        f1_vld = f1_score(\n",
    "            np.concatenate(epoch_vld_lbls, axis=0), \n",
    "            np.concatenate(epoch_vld_prds, axis=0).argmax(1),\n",
    "            labels=list(range(11)), \n",
    "            average='macro'\n",
    "        )\n",
    "\n",
    "\n",
    "        print(\n",
    "            'Epoch {:03d}/{:03d} - Mean training loss {:.6f}; Mean training F1 {:.6f}; Mean validation loss {:.6f}; Mean validation F1 {:.6f}; Learning rate {:.6f};'.format(\n",
    "                epc + 1, EPOCHS, np.mean(epoch_trn_losses), f1_trn, np.mean(epoch_vld_losses), f1_vld, scheduler.get_lr()[0],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if f1_vld > np.nanmax(vld_f1s):\n",
    "            torch.save(\n",
    "                {\n",
    "                    'epoch': epc + 1,\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'f1': f1_vld,\n",
    "                    'loss': np.mean(epoch_vld_losses),\n",
    "                }, \n",
    "                './saved_models/wavenet_model_fold{:03d}_checkpoint.pth'.format(fold_number)\n",
    "            )\n",
    "\n",
    "        vld_f1s.append(f1_vld)\n",
    "\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################\n",
      "Training/validation for fold 1/5;\n",
      "===========================================================\n",
      "Epoch 001/096 - Training batch 0065/0125: Training loss nan;7980;\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-56b749ee512b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mfold_train_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimzr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcritrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschdlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader_vld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-03aace2629cb>\u001b[0m in \u001b[0;36mfold_train_validate\u001b[0;34m(model, optimizer, criterion, scheduler, training_loader, validation_loader, fold_number)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrn_batch_dat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_batch_lbl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mtrn_batch_dat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_batch_lbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrn_batch_dat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_batch_lbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fld, (ndcs_trn, ndcs_vld) in enumerate(skf.split(series_trn, skf_grp)):\n",
    "    print('################################################################')\n",
    "    print('Training/validation for fold {:d}/{:d};'.format(fld+1, N_FOLDS))\n",
    "    \n",
    "    # setup fold data\n",
    "    dat_trn, lbl_trn = series_trn[ndcs_trn], series_lbl[ndcs_trn]\n",
    "    dat_vld, lbl_vld = series_trn[ndcs_vld], series_lbl[ndcs_vld]\n",
    "    \n",
    "    waveset_trn = Waveset(dat_trn, lbl_trn)\n",
    "    waveset_vld = Waveset(dat_vld, lbl_vld)\n",
    "\n",
    "    loader_trn = DataLoader(waveset_trn, BATCHSIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    loader_vld = DataLoader(waveset_vld, BATCHSIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    # setup fold model\n",
    "    mdl = Wave_Classifier(series_trn.shape[-1]).to(DEVICE)\n",
    "    critrn = nn.CrossEntropyLoss()\n",
    "    optimzr = torch.optim.AdamW(mdl.parameters(), lr=LR)\n",
    "    schdlr = torch.optim.lr_scheduler.CosineAnnealingLR(optimzr, T_max=EPOCHS, eta_min=LR/100)\n",
    "    \n",
    "    # run\n",
    "    fold_train_validate(model=mdl, optimizer=optimzr, criterion=critrn, scheduler=schdlr, training_loader=loader_trn, validation_loader=loader_vld, fold_number=fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv', dtype={'time': str, 'open_channels': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- fold 0 --------\n",
      "model validation loss: 0.107; validation f1: 0.941;\n",
      "-------- fold 1 --------\n",
      "model validation loss: 0.049; validation f1: 0.934;\n",
      "-------- fold 2 --------\n",
      "model validation loss: 0.091; validation f1: 0.938;\n",
      "-------- fold 3 --------\n",
      "model validation loss: 0.073; validation f1: 0.940;\n",
      "-------- fold 4 --------\n",
      "model validation loss: 0.075; validation f1: 0.939;\n"
     ]
    }
   ],
   "source": [
    "submission_pred = np.zeros(shape=(submission.shape[0], 11))\n",
    "\n",
    "waveset_tst = Waveset(series_tst)\n",
    "loader_tst = DataLoader(waveset_tst, BATCHSIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "for fld in range(5):\n",
    "    print('-------- fold {:d} --------'.format(fld))\n",
    "    fld_weight = torch.load('./saved_models/wavenet_model_fold{:03d}_checkpoint.pth'.format(fld))\n",
    "    print('model validation loss: {:.3f}; validation f1: {:.3f};'.format(fld_weight['loss'], fld_weight['f1']))\n",
    "    mdl = Classifier(series_tst.shape[-1]).to(DEVICE)\n",
    "    mdl.load_state_dict(fld_weight['model'])\n",
    "    with torch.no_grad():\n",
    "        tst_fold_prd = []\n",
    "        for tst_batch_dat in loader_tst:\n",
    "            tst_batch_prd = mdl(tst_batch_dat.to(DEVICE))\n",
    "            tst_batch_prd = tst_batch_prd.view(-1, tst_batch_prd.size(-1)).detach().cpu().numpy()\n",
    "            tst_fold_prd.append(tst_batch_prd)\n",
    "            \n",
    "        submission_pred += np.concatenate(tst_fold_prd, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['open_channels'] = submission_pred.argmax(1)\n",
    "submission.to_csv(\"../submissions/sub0_wavenet_myfeats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ndcs_trn, ndcs_vld in skf.split(series_trn, skf_grp):\n",
    "    dat_trn, lbl_trn = series_trn[ndcs_trn], series_lbl[ndcs_trn]\n",
    "    dat_vld, lbl_vld = series_trn[ndcs_vld], series_lbl[ndcs_vld]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = Wave_Classifier(49).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(mdl.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LR/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveset_trn = Waveset(dat_trn, lbl_trn)\n",
    "waveset_vld = Waveset(dat_vld, lbl_vld)\n",
    "\n",
    "loader_trn = DataLoader(waveset_trn, BATCHSIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "loader_vld = DataLoader(waveset_vld, BATCHSIZE, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 1,  ..., 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_batch_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "Epoch 001/096 - Training batch 0055/0125: Training loss nan;7175;\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b5696be340f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_batch_prd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_batch_lbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mepoch_trn_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.7/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trn_losses = [np.nan]\n",
    "vld_losses = [np.nan]\n",
    "vld_f1s = [np.nan]\n",
    "\n",
    "for epc in range(EPOCHS):\n",
    "    print('===========================================================')\n",
    "    \n",
    "    epoch_trn_losses = []\n",
    "    epoch_trn_lbls = []\n",
    "    epoch_trn_prds = []\n",
    "    epoch_vld_losses = []\n",
    "    epoch_vld_lbls = []\n",
    "    epoch_vld_prds = []\n",
    "    \n",
    "    # ------ training ------\n",
    "    mdl.train()\n",
    "    for i, (trn_batch_dat, trn_batch_lbl) in enumerate(loader_trn):\n",
    "        trn_batch_dat, trn_batch_lbl = trn_batch_dat.to(DEVICE), trn_batch_lbl.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        trn_batch_prd = mdl(trn_batch_dat)\n",
    "        trn_batch_prd = trn_batch_prd.view(-1, trn_batch_prd.size(-1))\n",
    "        trn_batch_lbl = trn_batch_lbl.view(-1)\n",
    "        loss = criterion(trn_batch_prd, trn_batch_lbl)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_trn_losses.append(loss.item())\n",
    "        epoch_trn_lbls.append(trn_batch_lbl.detach().cpu().numpy())\n",
    "        epoch_trn_prds.append(trn_batch_prd.detach().cpu().numpy())\n",
    "        \n",
    "        print(\n",
    "            'Epoch {:03d}/{:03d} - Training batch {:04d}/{:04d}: Training loss {:.6f};'.format(\n",
    "                epc + 1, EPOCHS, i + 1, len(loader_trn), epoch_trn_losses[-1],\n",
    "            ), \n",
    "            end='\\r'\n",
    "        )\n",
    "    \n",
    "    # ------ validation ------\n",
    "    mdl.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (vld_batch_dat, vld_batch_lbl) in enumerate(loader_vld):\n",
    "            vld_batch_dat, vld_batch_lbl = vld_batch_dat.to(DEVICE), vld_batch_lbl.to(DEVICE)\n",
    "            \n",
    "            vld_batch_prd = mdl(vld_batch_dat)\n",
    "            vld_batch_prd = vld_batch_prd.view(-1, vld_batch_prd.size(-1))\n",
    "            vld_batch_lbl = vld_batch_lbl.view(-1)\n",
    "            loss = criterion(trn_batch_prd, trn_batch_lbl)\n",
    "            \n",
    "            epoch_vld_losses.append(loss.item())\n",
    "            epoch_vld_lbls.append(vld_batch_lbl.detach().cpu().numpy())\n",
    "            epoch_vld_prds.append(vld_batch_prd.detach().cpu().numpy())\n",
    "            \n",
    "            print(\n",
    "                'Epoch {:03d}/{:03d} - Validation batch {:04d}/{:04d}: Validation loss {:.6f};'.format(\n",
    "                    epc + 1, EPOCHS, i + 1, len(loader_vld), epoch_vld_losses[-1],\n",
    "                ), \n",
    "                end='\\r'\n",
    "            )\n",
    "    \n",
    "    # ------ epoch end ------\n",
    "    f1_trn = f1_score(\n",
    "        np.concatenate(epoch_trn_lbls, axis=0), \n",
    "        np.concatenate(epoch_trn_prds, axis=0).argmax(1),\n",
    "        labels=list(range(11)), \n",
    "        average='macro'\n",
    "    )\n",
    "    f1_vld = f1_score(\n",
    "        np.concatenate(epoch_vld_lbls, axis=0), \n",
    "        np.concatenate(epoch_vld_prds, axis=0).argmax(1),\n",
    "        labels=list(range(11)), \n",
    "        average='macro'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(\n",
    "        'Epoch {:03d}/{:03d} - Mean training loss {:.6f}; Mean training F1 {:.6f}; Mean validation loss {:.6f}; Mean validation F1 {:.6f}; Learning rate {:.6f};'.format(\n",
    "            epc + 1, EPOCHS, np.mean(epoch_trn_losses), f1_trn, np.mean(epoch_vld_losses), f1_vld, scheduler.get_lr()[0],\n",
    "        )\n",
    "    )\n",
    "    \n",
    "#     if f1_vld > np.max(vld_f1s):\n",
    "#         torch.save(\n",
    "#             {\n",
    "#                 'epoch': epc + 1,\n",
    "#                 'model': mdl.state_dict(),\n",
    "#                 'optimizer': optimizer.state_dict(),\n",
    "#                 'f1': f1_vld,\n",
    "#                 'loss': np.mean(epoch_vld_losses),\n",
    "#             }, \n",
    "#             './checkpoints/wavenet_model_fold{:03d}_checkpoint.pth'.format(fld)\n",
    "#         )\n",
    "    \n",
    "#     vld_f1s.append(f1_vld)\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for index, (train_index, val_index, _) in enumerate(new_splits[0:], start=0):\n",
    "    print(\"Fold : {}\".format(index))\n",
    "    train_dataset = IronDataset(train[train_index], train_tr[train_index], seq_len=GROUP_BATCH_SIZE, flip=flip, noise_level=noise)\n",
    "    train_dataloader = DataLoader(train_dataset, NNBATCHSIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    valid_dataset = IronDataset(train[val_index], train_tr[val_index], seq_len=GROUP_BATCH_SIZE, flip=False)\n",
    "    valid_dataloader = DataLoader(valid_dataset, NNBATCHSIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    it = 0\n",
    "    model = Classifier()\n",
    "    model = model.cuda()\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=40, is_maximize=True,\n",
    "                                   checkpoint_path=os.path.join(outdir, \"gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index,\n",
    "                                                                                                             it)))\n",
    "\n",
    "    weight = None#cal_weights()\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "    schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.2)\n",
    "    avg_train_losses, avg_valid_losses = [], []\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('**********************************')\n",
    "        print(\"Folder : {} Epoch : {}\".format(index, epoch))\n",
    "        print(\"Curr learning_rate: {:0.9f}\".format(optimizer.param_groups[0]['lr']))\n",
    "        train_losses, valid_losses = [], []\n",
    "        tr_loss_cls_item, val_loss_cls_item = [], []\n",
    "\n",
    "        model.train()  # prep model for training\n",
    "        train_preds, train_true = torch.Tensor([]).cuda(), torch.LongTensor([]).cuda()#.to(device)\n",
    "\n",
    "        for x, y in tqdm(train_dataloader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(x)\n",
    "\n",
    "            predictions_ = predictions.view(-1, predictions.shape[-1])\n",
    "            y_ = y.view(-1)\n",
    "\n",
    "            loss = criterion(predictions_, y_)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            #schedular.step()\n",
    "            # record training lossa\n",
    "            train_losses.append(loss.item())\n",
    "            train_true = torch.cat([train_true, y_], 0)\n",
    "            train_preds = torch.cat([train_preds, predictions_], 0)\n",
    "\n",
    "        model.eval()  # prep model for evaluation\n",
    "        val_preds, val_true = torch.Tensor([]).cuda(), torch.LongTensor([]).cuda()\n",
    "        print('EVALUATION')\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(valid_dataloader):\n",
    "                x = x.cuda()#.to(device)\n",
    "                y = y.cuda()#..to(device)\n",
    "\n",
    "                predictions = model(x)\n",
    "                predictions_ = predictions.view(-1, predictions.shape[-1])\n",
    "                y_ = y.view(-1)\n",
    "\n",
    "                loss = criterion(predictions_, y_)\n",
    "\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "\n",
    "                val_true = torch.cat([val_true, y_], 0)\n",
    "                val_preds = torch.cat([val_preds, predictions_], 0)\n",
    "\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        print(\"train_loss: {:0.6f}, valid_loss: {:0.6f}\".format(train_loss, valid_loss))\n",
    "\n",
    "        train_score = f1_score(train_true.cpu().detach().numpy(), train_preds.cpu().detach().numpy().argmax(1),\n",
    "                               labels=list(range(11)), average='macro')\n",
    "\n",
    "        val_score = f1_score(val_true.cpu().detach().numpy(), val_preds.cpu().detach().numpy().argmax(1),\n",
    "                             labels=list(range(11)), average='macro')\n",
    "\n",
    "        schedular.step(val_score)\n",
    "        print(\"train_f1: {:0.6f}, valid_f1: {:0.6f}\".format(train_score, val_score))\n",
    "        res = early_stopping(val_score, model)\n",
    "        #print('fres:', res)\n",
    "        if  res == 2:\n",
    "            print(\"Early Stopping\")\n",
    "            print('folder %d global best val max f1 model score %f' % (index, early_stopping.best_score))\n",
    "            break\n",
    "        elif res == 1:\n",
    "            print('save folder %d global val max f1 model score %f' % (index, val_score))\n",
    "    print('Folder {} finally best global max f1 score is {}'.format(index, early_stopping.best_score))\n",
    "    oof_score.append(round(early_stopping.best_score, 6))\n",
    "    \n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(test_dataloader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            predictions = model(x)\n",
    "            predictions_ = predictions.view(-1, predictions.shape[-1]) # shape [128, 4000, 11]\n",
    "            #print(predictions.shape, F.softmax(predictions_, dim=1).cpu().numpy().shape)\n",
    "            pred_list.append(F.softmax(predictions_, dim=1).cpu().numpy()) # shape (512000, 11)\n",
    "            #a = input()\n",
    "        test_preds = np.vstack(pred_list) # shape [2000000, 11]\n",
    "        test_preds_all += test_preds\n",
    "print('all folder score is:%s'%str(oof_score))\n",
    "print('OOF mean score is: %f'% (sum(oof_score)/len(oof_score)))\n",
    "print('Generate submission.............')\n",
    "submission_csv_path = '/kaggle/input/liverpool-ion-switching/sample_submission.csv'\n",
    "ss = pd.read_csv(submission_csv_path, dtype={'time': str})\n",
    "test_preds_all = test_preds_all / np.sum(test_preds_all, axis=1)[:, None]\n",
    "test_pred_frame = pd.DataFrame({'time': ss['time'].astype(str),\n",
    "                                'open_channels': np.argmax(test_preds_all, axis=1)})\n",
    "test_pred_frame.to_csv(\"./gru_preds.csv\", index=False)\n",
    "print('over')\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "################################################################\n",
    "Training/validation for fold 1/5;\n",
    "===========================================================\n",
    "Epoch 001/096 - Mean training loss 0.781633; Mean training F1 0.553002; Mean validation loss 0.412809; Mean validation F1 0.766302; Learning rate 0.001000;\n",
    "===========================================================\n",
    "Epoch 002/096 - Mean training loss 0.261983; Mean training F1 0.859350; Mean validation loss 0.227335; Mean validation F1 0.901282; Learning rate 0.000999;\n",
    "===========================================================\n",
    "Epoch 003/096 - Mean training loss 0.178007; Mean training F1 0.895909; Mean validation loss 0.200260; Mean validation F1 0.914850; Learning rate 0.000998;\n",
    "===========================================================\n",
    "Epoch 004/096 - Mean training loss 0.134092; Mean training F1 0.915491; Mean validation loss 0.080251; Mean validation F1 0.931673; Learning rate 0.000996;\n",
    "===========================================================\n",
    "Epoch 005/096 - Mean training loss 0.101646; Mean training F1 0.929089; Mean validation loss 0.099819; Mean validation F1 0.936699; Learning rate 0.000994;\n",
    "===========================================================\n",
    "Epoch 006/096 - Mean training loss 0.095201; Mean training F1 0.932064; Mean validation loss 0.088729; Mean validation F1 0.935807; Learning rate 0.000991;\n",
    "===========================================================\n",
    "Epoch 007/096 - Mean training loss 0.092859; Mean training F1 0.933430; Mean validation loss 0.073633; Mean validation F1 0.934309; Learning rate 0.000988;\n",
    "===========================================================\n",
    "Epoch 008/096 - Mean training loss 0.093573; Mean training F1 0.931454; Mean validation loss 0.140850; Mean validation F1 0.938138; Learning rate 0.000984;\n",
    "===========================================================\n",
    "Epoch 009/096 - Mean training loss 0.089503; Mean training F1 0.934797; Mean validation loss 0.060738; Mean validation F1 0.937944; Learning rate 0.000979;\n",
    "===========================================================\n",
    "Epoch 010/096 - Mean training loss 0.141612; Mean training F1 0.906454; Mean validation loss 0.127296; Mean validation F1 0.901391; Learning rate 0.000974;\n",
    "===========================================================\n",
    "Epoch 011/096 - Mean training loss 0.094788; Mean training F1 0.931791; Mean validation loss 0.085189; Mean validation F1 0.936724; Learning rate 0.000969;\n",
    "===========================================================\n",
    "Epoch 012/096 - Mean training loss 0.089199; Mean training F1 0.935149; Mean validation loss 0.098268; Mean validation F1 0.937406; Learning rate 0.000963;\n",
    "===========================================================\n",
    "Epoch 013/096 - Mean training loss 0.088318; Mean training F1 0.935515; Mean validation loss 0.035736; Mean validation F1 0.938002; Learning rate 0.000956;\n",
    "===========================================================\n",
    "Epoch 014/096 - Mean training loss 0.088026; Mean training F1 0.935420; Mean validation loss 0.069937; Mean validation F1 0.938388; Learning rate 0.000949;\n",
    "===========================================================\n",
    "Epoch 015/096 - Mean training loss 0.087992; Mean training F1 0.935621; Mean validation loss 0.076171; Mean validation F1 0.938521; Learning rate 0.000942;\n",
    "===========================================================\n",
    "Epoch 016/096 - Mean training loss 0.087366; Mean training F1 0.935681; Mean validation loss 0.082512; Mean validation F1 0.938560; Learning rate 0.000934;\n",
    "===========================================================\n",
    "Epoch 017/096 - Mean training loss 0.087980; Mean training F1 0.935583; Mean validation loss 0.055329; Mean validation F1 0.937896; Learning rate 0.000926;\n",
    "===========================================================\n",
    "Epoch 018/096 - Mean training loss 0.087576; Mean training F1 0.935585; Mean validation loss 0.081457; Mean validation F1 0.936810; Learning rate 0.000917;\n",
    "===========================================================\n",
    "Epoch 019/096 - Mean training loss 0.087624; Mean training F1 0.935660; Mean validation loss 0.063963; Mean validation F1 0.937392; Learning rate 0.000908;\n",
    "===========================================================\n",
    "Epoch 020/096 - Mean training loss 0.087016; Mean training F1 0.936094; Mean validation loss 0.101678; Mean validation F1 0.939724; Learning rate 0.000898;\n",
    "===========================================================\n",
    "Epoch 021/096 - Mean training loss 0.086274; Mean training F1 0.936352; Mean validation loss 0.081030; Mean validation F1 0.938444; Learning rate 0.000888;\n",
    "===========================================================\n",
    "Epoch 022/096 - Mean training loss 0.086407; Mean training F1 0.936237; Mean validation loss 0.071360; Mean validation F1 0.938917; Learning rate 0.000878;\n",
    "===========================================================\n",
    "Epoch 023/096 - Mean training loss 0.086090; Mean training F1 0.936535; Mean validation loss 0.099004; Mean validation F1 0.939214; Learning rate 0.000867;\n",
    "===========================================================\n",
    "Epoch 024/096 - Mean training loss 0.086886; Mean training F1 0.935604; Mean validation loss 0.085522; Mean validation F1 0.938870; Learning rate 0.000856;\n",
    "===========================================================\n",
    "Epoch 025/096 - Mean training loss 0.086054; Mean training F1 0.936387; Mean validation loss 0.070791; Mean validation F1 0.938601; Learning rate 0.000844;\n",
    "===========================================================\n",
    "Epoch 026/096 - Mean training loss 0.085658; Mean training F1 0.936639; Mean validation loss 0.097455; Mean validation F1 0.938912; Learning rate 0.000832;\n",
    "===========================================================\n",
    "Epoch 027/096 - Mean training loss 0.085226; Mean training F1 0.937010; Mean validation loss 0.086390; Mean validation F1 0.939018; Learning rate 0.000820;\n",
    "===========================================================\n",
    "Epoch 028/096 - Mean training loss 0.087281; Mean training F1 0.935091; Mean validation loss 0.097088; Mean validation F1 0.939804; Learning rate 0.000807;\n",
    "===========================================================\n",
    "Epoch 029/096 - Mean training loss 0.085538; Mean training F1 0.936775; Mean validation loss 0.051526; Mean validation F1 0.939362; Learning rate 0.000794;\n",
    "===========================================================\n",
    "Epoch 030/096 - Mean training loss 0.086680; Mean training F1 0.935778; Mean validation loss 0.076377; Mean validation F1 0.938687; Learning rate 0.000781;\n",
    "===========================================================\n",
    "Epoch 031/096 - Mean training loss 0.086245; Mean training F1 0.936273; Mean validation loss 0.057200; Mean validation F1 0.938451; Learning rate 0.000767;\n",
    "===========================================================\n",
    "Epoch 032/096 - Mean training loss 0.084974; Mean training F1 0.937101; Mean validation loss 0.069085; Mean validation F1 0.938706; Learning rate 0.000753;\n",
    "===========================================================\n",
    "Epoch 033/096 - Mean training loss 0.084679; Mean training F1 0.937117; Mean validation loss 0.066366; Mean validation F1 0.939309; Learning rate 0.000739;\n",
    "===========================================================\n",
    "Epoch 034/096 - Mean training loss 0.084766; Mean training F1 0.937184; Mean validation loss 0.093857; Mean validation F1 0.939059; Learning rate 0.000724;\n",
    "===========================================================\n",
    "Epoch 035/096 - Mean training loss 0.085089; Mean training F1 0.936569; Mean validation loss 0.083715; Mean validation F1 0.938494; Learning rate 0.000710;\n",
    "===========================================================\n",
    "Epoch 036/096 - Mean training loss 0.084418; Mean training F1 0.937031; Mean validation loss 0.059908; Mean validation F1 0.938586; Learning rate 0.000695;\n",
    "===========================================================\n",
    "Epoch 037/096 - Mean training loss 0.084466; Mean training F1 0.936918; Mean validation loss 0.133200; Mean validation F1 0.939829; Learning rate 0.000680;\n",
    "===========================================================\n",
    "Epoch 038/096 - Mean training loss 0.083896; Mean training F1 0.937493; Mean validation loss 0.100102; Mean validation F1 0.939651; Learning rate 0.000665;\n",
    "===========================================================\n",
    "Epoch 039/096 - Mean training loss 0.084589; Mean training F1 0.937093; Mean validation loss 0.072560; Mean validation F1 0.938249; Learning rate 0.000649;\n",
    "===========================================================\n",
    "Epoch 040/096 - Mean training loss 0.084148; Mean training F1 0.937240; Mean validation loss 0.086210; Mean validation F1 0.938434; Learning rate 0.000634;\n",
    "===========================================================\n",
    "Epoch 041/096 - Mean training loss 0.084463; Mean training F1 0.936949; Mean validation loss 0.081103; Mean validation F1 0.939274; Learning rate 0.000618;\n",
    "===========================================================\n",
    "Epoch 042/096 - Mean training loss 0.084264; Mean training F1 0.937024; Mean validation loss 0.062746; Mean validation F1 0.939249; Learning rate 0.000602;\n",
    "===========================================================\n",
    "Epoch 043/096 - Mean training loss 0.083325; Mean training F1 0.937824; Mean validation loss 0.094868; Mean validation F1 0.939783; Learning rate 0.000586;\n",
    "===========================================================\n",
    "Epoch 044/096 - Mean training loss 0.083286; Mean training F1 0.937732; Mean validation loss 0.090414; Mean validation F1 0.940102; Learning rate 0.000570;\n",
    "===========================================================\n",
    "Epoch 045/096 - Mean training loss 0.102585; Mean training F1 0.923650; Mean validation loss 0.130771; Mean validation F1 0.933253; Learning rate 0.000554;\n",
    "===========================================================\n",
    "Epoch 046/096 - Mean training loss 0.088873; Mean training F1 0.934698; Mean validation loss 0.088191; Mean validation F1 0.938272; Learning rate 0.000538;\n",
    "===========================================================\n",
    "Epoch 047/096 - Mean training loss 0.084537; Mean training F1 0.936968; Mean validation loss 0.054124; Mean validation F1 0.939712; Learning rate 0.000522;\n",
    "===========================================================\n",
    "Epoch 048/096 - Mean training loss 0.083645; Mean training F1 0.937522; Mean validation loss 0.096849; Mean validation F1 0.940002; Learning rate 0.000506;\n",
    "===========================================================\n",
    "Epoch 049/096 - Mean training loss 0.083462; Mean training F1 0.937672; Mean validation loss 0.113205; Mean validation F1 0.940016; Learning rate 0.000489;\n",
    "===========================================================\n",
    "Epoch 050/096 - Mean training loss 0.083508; Mean training F1 0.937435; Mean validation loss 0.061859; Mean validation F1 0.939197; Learning rate 0.000473;\n",
    "===========================================================\n",
    "Epoch 051/096 - Mean training loss 0.083153; Mean training F1 0.937708; Mean validation loss 0.063504; Mean validation F1 0.939989; Learning rate 0.000457;\n",
    "===========================================================\n",
    "Epoch 052/096 - Mean training loss 0.082961; Mean training F1 0.937798; Mean validation loss 0.097192; Mean validation F1 0.940010; Learning rate 0.000441;\n",
    "===========================================================\n",
    "Epoch 053/096 - Mean training loss 0.082968; Mean training F1 0.937854; Mean validation loss 0.084846; Mean validation F1 0.939689; Learning rate 0.000425;\n",
    "===========================================================\n",
    "Epoch 054/096 - Mean training loss 0.083456; Mean training F1 0.937590; Mean validation loss 0.053251; Mean validation F1 0.939848; Learning rate 0.000409;\n",
    "===========================================================\n",
    "Epoch 055/096 - Mean training loss 0.082635; Mean training F1 0.937955; Mean validation loss 0.073831; Mean validation F1 0.939850; Learning rate 0.000393;\n",
    "===========================================================\n",
    "Epoch 056/096 - Mean training loss 0.082742; Mean training F1 0.937893; Mean validation loss 0.078202; Mean validation F1 0.939559; Learning rate 0.000377;\n",
    "===========================================================\n",
    "Epoch 057/096 - Mean training loss 0.082728; Mean training F1 0.938015; Mean validation loss 0.069343; Mean validation F1 0.940232; Learning rate 0.000362;\n",
    "===========================================================\n",
    "Epoch 058/096 - Mean training loss 0.082498; Mean training F1 0.938190; Mean validation loss 0.098570; Mean validation F1 0.939938; Learning rate 0.000346;\n",
    "===========================================================\n",
    "Epoch 059/096 - Mean training loss 0.082428; Mean training F1 0.938149; Mean validation loss 0.067996; Mean validation F1 0.940307; Learning rate 0.000331;\n",
    "===========================================================\n",
    "Epoch 060/096 - Mean training loss 0.082369; Mean training F1 0.938052; Mean validation loss 0.080549; Mean validation F1 0.940003; Learning rate 0.000316;\n",
    "===========================================================\n",
    "Epoch 061/096 - Mean training loss 0.082161; Mean training F1 0.938190; Mean validation loss 0.067339; Mean validation F1 0.940188; Learning rate 0.000301;\n",
    "===========================================================\n",
    "Epoch 062/096 - Mean training loss 0.082167; Mean training F1 0.938337; Mean validation loss 0.110540; Mean validation F1 0.940189; Learning rate 0.000287;\n",
    "===========================================================\n",
    "Epoch 063/096 - Mean training loss 0.081899; Mean training F1 0.938396; Mean validation loss 0.060367; Mean validation F1 0.940011; Learning rate 0.000272;\n",
    "===========================================================\n",
    "Epoch 064/096 - Mean training loss 0.082032; Mean training F1 0.938250; Mean validation loss 0.064517; Mean validation F1 0.940359; Learning rate 0.000258;\n",
    "===========================================================\n",
    "Epoch 065/096 - Mean training loss 0.081877; Mean training F1 0.938443; Mean validation loss 0.101256; Mean validation F1 0.939939; Learning rate 0.000244;\n",
    "===========================================================\n",
    "Epoch 066/096 - Mean training loss 0.081839; Mean training F1 0.938489; Mean validation loss 0.074373; Mean validation F1 0.939770; Learning rate 0.000230;\n",
    "===========================================================\n",
    "Epoch 067/096 - Mean training loss 0.081671; Mean training F1 0.938452; Mean validation loss 0.087162; Mean validation F1 0.940069; Learning rate 0.000217;\n",
    "===========================================================\n",
    "Epoch 068/096 - Mean training loss 0.081780; Mean training F1 0.938440; Mean validation loss 0.082698; Mean validation F1 0.940303; Learning rate 0.000204;\n",
    "===========================================================\n",
    "Epoch 069/096 - Mean training loss 0.081650; Mean training F1 0.938428; Mean validation loss 0.085629; Mean validation F1 0.940190; Learning rate 0.000191;\n",
    "===========================================================\n",
    "Epoch 070/096 - Mean training loss 0.081616; Mean training F1 0.938469; Mean validation loss 0.061089; Mean validation F1 0.940007; Learning rate 0.000179;\n",
    "===========================================================\n",
    "Epoch 071/096 - Mean training loss 0.081492; Mean training F1 0.938593; Mean validation loss 0.098063; Mean validation F1 0.940329; Learning rate 0.000167;\n",
    "===========================================================\n",
    "Epoch 072/096 - Mean training loss 0.081463; Mean training F1 0.938613; Mean validation loss 0.066613; Mean validation F1 0.939967; Learning rate 0.000155;\n",
    "===========================================================\n",
    "Epoch 073/096 - Mean training loss 0.081461; Mean training F1 0.938638; Mean validation loss 0.107404; Mean validation F1 0.940527; Learning rate 0.000144;\n",
    "===========================================================\n",
    "Epoch 074/096 - Mean training loss 0.081361; Mean training F1 0.938595; Mean validation loss 0.071343; Mean validation F1 0.940515; Learning rate 0.000133;\n",
    "===========================================================\n",
    "Epoch 075/096 - Mean training loss 0.081223; Mean training F1 0.938782; Mean validation loss 0.052674; Mean validation F1 0.939334; Learning rate 0.000123;\n",
    "===========================================================\n",
    "Epoch 076/096 - Mean training loss 0.081210; Mean training F1 0.938610; Mean validation loss 0.073499; Mean validation F1 0.940297; Learning rate 0.000113;\n",
    "===========================================================\n",
    "Epoch 077/096 - Mean training loss 0.081175; Mean training F1 0.938802; Mean validation loss 0.083867; Mean validation F1 0.940094; Learning rate 0.000103;\n",
    "===========================================================\n",
    "Epoch 078/096 - Mean training loss 0.081151; Mean training F1 0.938738; Mean validation loss 0.105408; Mean validation F1 0.940388; Learning rate 0.000094;\n",
    "===========================================================\n",
    "Epoch 079/096 - Mean training loss 0.081123; Mean training F1 0.938837; Mean validation loss 0.076650; Mean validation F1 0.940457; Learning rate 0.000085;\n",
    "===========================================================\n",
    "Epoch 080/096 - Mean training loss 0.081095; Mean training F1 0.938818; Mean validation loss 0.079675; Mean validation F1 0.940519; Learning rate 0.000077;\n",
    "===========================================================\n",
    "Epoch 081/096 - Mean training loss 0.081055; Mean training F1 0.938761; Mean validation loss 0.091190; Mean validation F1 0.940493; Learning rate 0.000069;\n",
    "===========================================================\n",
    "Epoch 082/096 - Mean training loss 0.080974; Mean training F1 0.938899; Mean validation loss 0.051645; Mean validation F1 0.940259; Learning rate 0.000062;\n",
    "===========================================================\n",
    "Epoch 083/096 - Mean training loss 0.080944; Mean training F1 0.938941; Mean validation loss 0.060831; Mean validation F1 0.940220; Learning rate 0.000055;\n",
    "===========================================================\n",
    "Epoch 084/096 - Mean training loss 0.080961; Mean training F1 0.938867; Mean validation loss 0.083580; Mean validation F1 0.940361; Learning rate 0.000048;\n",
    "===========================================================\n",
    "Epoch 085/096 - Mean training loss 0.080858; Mean training F1 0.938888; Mean validation loss 0.065490; Mean validation F1 0.940229; Learning rate 0.000042;\n",
    "===========================================================\n",
    "Epoch 086/096 - Mean training loss 0.080860; Mean training F1 0.938843; Mean validation loss 0.090887; Mean validation F1 0.940424; Learning rate 0.000037;\n",
    "===========================================================\n",
    "Epoch 087/096 - Mean training loss 0.080802; Mean training F1 0.938914; Mean validation loss 0.042099; Mean validation F1 0.940446; Learning rate 0.000032;\n",
    "===========================================================\n",
    "Epoch 088/096 - Mean training loss 0.080770; Mean training F1 0.939016; Mean validation loss 0.041644; Mean validation F1 0.940312; Learning rate 0.000027;\n",
    "===========================================================\n",
    "Epoch 089/096 - Mean training loss 0.080750; Mean training F1 0.938956; Mean validation loss 0.098602; Mean validation F1 0.940418; Learning rate 0.000023;\n",
    "===========================================================\n",
    "Epoch 090/096 - Mean training loss 0.080742; Mean training F1 0.938993; Mean validation loss 0.107095; Mean validation F1 0.940463; Learning rate 0.000020;\n",
    "===========================================================\n",
    "Epoch 091/096 - Mean training loss 0.080707; Mean training F1 0.938967; Mean validation loss 0.075413; Mean validation F1 0.940384; Learning rate 0.000017;\n",
    "===========================================================\n",
    "Epoch 092/096 - Mean training loss 0.080699; Mean training F1 0.939055; Mean validation loss 0.056152; Mean validation F1 0.940401; Learning rate 0.000015;\n",
    "===========================================================\n",
    "Epoch 093/096 - Mean training loss 0.080690; Mean training F1 0.939015; Mean validation loss 0.069408; Mean validation F1 0.940305; Learning rate 0.000013;\n",
    "===========================================================\n",
    "Epoch 094/096 - Mean training loss 0.080688; Mean training F1 0.939031; Mean validation loss 0.063590; Mean validation F1 0.940351; Learning rate 0.000011;\n",
    "===========================================================\n",
    "Epoch 095/096 - Mean training loss 0.080676; Mean training F1 0.939005; Mean validation loss 0.070513; Mean validation F1 0.940514; Learning rate 0.000010;\n",
    "===========================================================\n",
    "Epoch 096/096 - Mean training loss 0.080669; Mean training F1 0.939059; Mean validation loss 0.145814; Mean validation F1 0.940417; Learning rate 0.000010;\n",
    "################################################################\n",
    "Training/validation for fold 2/5;\n",
    "===========================================================\n",
    "Epoch 001/096 - Mean training loss 0.814389; Mean training F1 0.531903; Mean validation loss 0.469490; Mean validation F1 0.758003; Learning rate 0.001000;\n",
    "===========================================================\n",
    "Epoch 002/096 - Mean training loss 0.229337; Mean training F1 0.868913; Mean validation loss 0.183394; Mean validation F1 0.905002; Learning rate 0.000999;\n",
    "===========================================================\n",
    "Epoch 003/096 - Mean training loss 0.113867; Mean training F1 0.925528; Mean validation loss 0.109205; Mean validation F1 0.914826; Learning rate 0.000998;\n",
    "===========================================================\n",
    "Epoch 004/096 - Mean training loss 0.101049; Mean training F1 0.930571; Mean validation loss 0.089966; Mean validation F1 0.920528; Learning rate 0.000996;\n",
    "===========================================================\n",
    "Epoch 005/096 - Mean training loss 0.131180; Mean training F1 0.917635; Mean validation loss 0.124653; Mean validation F1 0.918180; Learning rate 0.000994;\n",
    "===========================================================\n",
    "Epoch 006/096 - Mean training loss 0.094386; Mean training F1 0.932020; Mean validation loss 0.078371; Mean validation F1 0.926392; Learning rate 0.000991;\n",
    "===========================================================\n",
    "Epoch 007/096 - Mean training loss 0.089290; Mean training F1 0.934332; Mean validation loss 0.052228; Mean validation F1 0.927983; Learning rate 0.000988;\n",
    "===========================================================\n",
    "Epoch 008/096 - Mean training loss 0.090690; Mean training F1 0.931807; Mean validation loss 0.055807; Mean validation F1 0.927376; Learning rate 0.000984;\n",
    "===========================================================\n",
    "Epoch 009/096 - Mean training loss 0.084779; Mean training F1 0.936435; Mean validation loss 0.100343; Mean validation F1 0.929321; Learning rate 0.000979;\n",
    "===========================================================\n",
    "Epoch 010/096 - Mean training loss 0.084301; Mean training F1 0.936374; Mean validation loss 0.086669; Mean validation F1 0.929630; Learning rate 0.000974;\n",
    "===========================================================\n",
    "Epoch 011/096 - Mean training loss 0.085257; Mean training F1 0.935362; Mean validation loss 0.099059; Mean validation F1 0.930134; Learning rate 0.000969;\n",
    "===========================================================\n",
    "Epoch 012/096 - Mean training loss 0.083859; Mean training F1 0.936338; Mean validation loss 0.085371; Mean validation F1 0.928816; Learning rate 0.000963;\n",
    "===========================================================\n",
    "Epoch 013/096 - Mean training loss 0.082924; Mean training F1 0.937208; Mean validation loss 0.139567; Mean validation F1 0.927707; Learning rate 0.000956;\n",
    "===========================================================\n",
    "Epoch 014/096 - Mean training loss 0.084969; Mean training F1 0.934950; Mean validation loss 0.080194; Mean validation F1 0.929985; Learning rate 0.000949;\n",
    "===========================================================\n",
    "Epoch 015/096 - Mean training loss 0.082742; Mean training F1 0.936861; Mean validation loss 0.074917; Mean validation F1 0.930847; Learning rate 0.000942;\n",
    "===========================================================\n",
    "Epoch 016/096 - Mean training loss 0.082390; Mean training F1 0.937041; Mean validation loss 0.062571; Mean validation F1 0.930967; Learning rate 0.000934;\n",
    "===========================================================\n",
    "Epoch 017/096 - Mean training loss 0.082205; Mean training F1 0.937203; Mean validation loss 0.065676; Mean validation F1 0.931359; Learning rate 0.000926;\n",
    "===========================================================\n",
    "Epoch 018/096 - Mean training loss 0.082842; Mean training F1 0.936777; Mean validation loss 0.115834; Mean validation F1 0.931166; Learning rate 0.000917;\n",
    "===========================================================\n",
    "Epoch 019/096 - Mean training loss 0.081086; Mean training F1 0.938047; Mean validation loss 0.090987; Mean validation F1 0.931935; Learning rate 0.000908;\n",
    "===========================================================\n",
    "Epoch 020/096 - Mean training loss 0.081480; Mean training F1 0.937531; Mean validation loss 0.085555; Mean validation F1 0.931450; Learning rate 0.000898;\n",
    "===========================================================\n",
    "Epoch 021/096 - Mean training loss 0.082401; Mean training F1 0.936985; Mean validation loss 0.083600; Mean validation F1 0.931103; Learning rate 0.000888;\n",
    "===========================================================\n",
    "Epoch 022/096 - Mean training loss 0.081028; Mean training F1 0.937907; Mean validation loss 0.036092; Mean validation F1 0.928091; Learning rate 0.000878;\n",
    "===========================================================\n",
    "Epoch 023/096 - Mean training loss 0.080840; Mean training F1 0.938062; Mean validation loss 0.069556; Mean validation F1 0.931138; Learning rate 0.000867;\n",
    "===========================================================\n",
    "Epoch 024/096 - Mean training loss 0.081544; Mean training F1 0.937522; Mean validation loss 0.060348; Mean validation F1 0.931748; Learning rate 0.000856;\n",
    "===========================================================\n",
    "Epoch 025/096 - Mean training loss 0.081062; Mean training F1 0.937508; Mean validation loss 0.092877; Mean validation F1 0.928435; Learning rate 0.000844;\n",
    "===========================================================\n",
    "Epoch 026/096 - Mean training loss 0.081805; Mean training F1 0.937108; Mean validation loss 0.073620; Mean validation F1 0.929279; Learning rate 0.000832;\n",
    "===========================================================\n",
    "Epoch 027/096 - Mean training loss 0.080235; Mean training F1 0.938367; Mean validation loss 0.100215; Mean validation F1 0.931710; Learning rate 0.000820;\n",
    "===========================================================\n",
    "Epoch 028/096 - Mean training loss 0.080425; Mean training F1 0.938028; Mean validation loss 0.068542; Mean validation F1 0.932531; Learning rate 0.000807;\n",
    "===========================================================\n",
    "Epoch 029/096 - Mean training loss 0.081132; Mean training F1 0.937508; Mean validation loss 0.073986; Mean validation F1 0.928988; Learning rate 0.000794;\n",
    "===========================================================\n",
    "Epoch 030/096 - Mean training loss 0.080135; Mean training F1 0.938373; Mean validation loss 0.092231; Mean validation F1 0.931973; Learning rate 0.000781;\n",
    "===========================================================\n",
    "Epoch 031/096 - Mean training loss 0.080919; Mean training F1 0.937366; Mean validation loss 0.102107; Mean validation F1 0.931652; Learning rate 0.000767;\n",
    "===========================================================\n",
    "Epoch 032/096 - Mean training loss 0.102965; Mean training F1 0.924805; Mean validation loss 0.090413; Mean validation F1 0.925046; Learning rate 0.000753;\n",
    "===========================================================\n",
    "Epoch 033/096 - Mean training loss 0.083287; Mean training F1 0.936584; Mean validation loss 0.073130; Mean validation F1 0.931546; Learning rate 0.000739;\n",
    "===========================================================\n",
    "Epoch 034/096 - Mean training loss 0.081475; Mean training F1 0.937868; Mean validation loss 0.083879; Mean validation F1 0.931535; Learning rate 0.000724;\n",
    "===========================================================\n",
    "Epoch 035/096 - Mean training loss 0.080754; Mean training F1 0.938105; Mean validation loss 0.113554; Mean validation F1 0.932476; Learning rate 0.000710;\n",
    "===========================================================\n",
    "Epoch 036/096 - Mean training loss 0.080798; Mean training F1 0.937915; Mean validation loss 0.127157; Mean validation F1 0.931912; Learning rate 0.000695;\n",
    "===========================================================\n",
    "Epoch 037/096 - Mean training loss 0.080566; Mean training F1 0.938050; Mean validation loss 0.133541; Mean validation F1 0.932722; Learning rate 0.000680;\n",
    "===========================================================\n",
    "Epoch 038/096 - Mean training loss 0.080112; Mean training F1 0.938411; Mean validation loss 0.099930; Mean validation F1 0.932990; Learning rate 0.000665;\n",
    "===========================================================\n",
    "Epoch 039/096 - Mean training loss 0.079971; Mean training F1 0.938617; Mean validation loss 0.094778; Mean validation F1 0.932231; Learning rate 0.000649;\n",
    "===========================================================\n",
    "Epoch 040/096 - Mean training loss 0.079950; Mean training F1 0.938496; Mean validation loss 0.067846; Mean validation F1 0.932266; Learning rate 0.000634;\n",
    "===========================================================\n",
    "Epoch 041/096 - Mean training loss 0.080040; Mean training F1 0.938306; Mean validation loss 0.061921; Mean validation F1 0.932465; Learning rate 0.000618;\n",
    "===========================================================\n",
    "Epoch 042/096 - Mean training loss 0.079792; Mean training F1 0.938505; Mean validation loss 0.065680; Mean validation F1 0.932296; Learning rate 0.000602;\n",
    "===========================================================\n",
    "Epoch 043/096 - Mean training loss 0.079322; Mean training F1 0.938894; Mean validation loss 0.073955; Mean validation F1 0.932765; Learning rate 0.000586;\n",
    "===========================================================\n",
    "Epoch 044/096 - Mean training loss 0.079436; Mean training F1 0.938737; Mean validation loss 0.059299; Mean validation F1 0.932662; Learning rate 0.000570;\n",
    "===========================================================\n",
    "Epoch 045/096 - Mean training loss 0.079629; Mean training F1 0.938612; Mean validation loss 0.078029; Mean validation F1 0.932894; Learning rate 0.000554;\n",
    "===========================================================\n",
    "Epoch 046/096 - Mean training loss 0.079325; Mean training F1 0.938696; Mean validation loss 0.071474; Mean validation F1 0.933127; Learning rate 0.000538;\n",
    "===========================================================\n",
    "Epoch 047/096 - Mean training loss 0.079260; Mean training F1 0.938967; Mean validation loss 0.086492; Mean validation F1 0.933014; Learning rate 0.000522;\n",
    "===========================================================\n",
    "Epoch 048/096 - Mean training loss 0.078974; Mean training F1 0.939071; Mean validation loss 0.102479; Mean validation F1 0.933121; Learning rate 0.000506;\n",
    "===========================================================\n",
    "Epoch 049/096 - Mean training loss 0.079151; Mean training F1 0.939053; Mean validation loss 0.083263; Mean validation F1 0.933374; Learning rate 0.000489;\n",
    "===========================================================\n",
    "Epoch 050/096 - Mean training loss 0.078905; Mean training F1 0.938949; Mean validation loss 0.080377; Mean validation F1 0.932698; Learning rate 0.000473;\n",
    "===========================================================\n",
    "Epoch 051/096 - Mean training loss 0.078850; Mean training F1 0.938993; Mean validation loss 0.091041; Mean validation F1 0.933201; Learning rate 0.000457;\n",
    "===========================================================\n",
    "Epoch 052/096 - Mean training loss 0.079024; Mean training F1 0.938901; Mean validation loss 0.074479; Mean validation F1 0.932685; Learning rate 0.000441;\n",
    "===========================================================\n",
    "Epoch 053/096 - Mean training loss 0.078527; Mean training F1 0.939259; Mean validation loss 0.103897; Mean validation F1 0.931995; Learning rate 0.000425;\n",
    "===========================================================\n",
    "Epoch 054/096 - Mean training loss 0.078789; Mean training F1 0.939045; Mean validation loss 0.083629; Mean validation F1 0.930530; Learning rate 0.000409;\n",
    "===========================================================\n",
    "Epoch 055/096 - Mean training loss 0.078804; Mean training F1 0.939122; Mean validation loss 0.076833; Mean validation F1 0.932250; Learning rate 0.000393;\n",
    "===========================================================\n",
    "Epoch 056/096 - Mean training loss 0.078255; Mean training F1 0.939409; Mean validation loss 0.054079; Mean validation F1 0.932542; Learning rate 0.000377;\n",
    "===========================================================\n",
    "Epoch 057/096 - Mean training loss 0.078437; Mean training F1 0.939147; Mean validation loss 0.085100; Mean validation F1 0.932961; Learning rate 0.000362;\n",
    "===========================================================\n",
    "Epoch 058/096 - Mean training loss 0.078405; Mean training F1 0.939287; Mean validation loss 0.090127; Mean validation F1 0.933377; Learning rate 0.000346;\n",
    "===========================================================\n",
    "Epoch 059/096 - Mean training loss 0.078258; Mean training F1 0.939488; Mean validation loss 0.087680; Mean validation F1 0.933167; Learning rate 0.000331;\n",
    "===========================================================\n",
    "Epoch 060/096 - Mean training loss 0.078437; Mean training F1 0.939207; Mean validation loss 0.058433; Mean validation F1 0.931630; Learning rate 0.000316;\n",
    "===========================================================\n",
    "Epoch 061/096 - Mean training loss 0.078302; Mean training F1 0.939343; Mean validation loss 0.088210; Mean validation F1 0.933377; Learning rate 0.000301;\n",
    "===========================================================\n",
    "Epoch 062/096 - Mean training loss 0.077944; Mean training F1 0.939688; Mean validation loss 0.072003; Mean validation F1 0.932343; Learning rate 0.000287;\n",
    "===========================================================\n",
    "Epoch 063/096 - Mean training loss 0.077916; Mean training F1 0.939503; Mean validation loss 0.103317; Mean validation F1 0.933154; Learning rate 0.000272;\n",
    "===========================================================\n",
    "Epoch 064/096 - Mean training loss 0.077915; Mean training F1 0.939502; Mean validation loss 0.084684; Mean validation F1 0.932724; Learning rate 0.000258;\n",
    "===========================================================\n",
    "Epoch 065/096 - Mean training loss 0.077775; Mean training F1 0.939630; Mean validation loss 0.076397; Mean validation F1 0.933297; Learning rate 0.000244;\n",
    "===========================================================\n",
    "Epoch 066/096 - Mean training loss 0.077848; Mean training F1 0.939615; Mean validation loss 0.087502; Mean validation F1 0.933143; Learning rate 0.000230;\n",
    "===========================================================\n",
    "Epoch 067/096 - Mean training loss 0.077856; Mean training F1 0.939589; Mean validation loss 0.075288; Mean validation F1 0.933097; Learning rate 0.000217;\n",
    "===========================================================\n",
    "Epoch 068/096 - Mean training loss 0.077632; Mean training F1 0.939699; Mean validation loss 0.087782; Mean validation F1 0.933138; Learning rate 0.000204;\n",
    "===========================================================\n",
    "Epoch 069/096 - Mean training loss 0.077657; Mean training F1 0.939773; Mean validation loss 0.061281; Mean validation F1 0.933625; Learning rate 0.000191;\n",
    "===========================================================\n",
    "Epoch 070/096 - Mean training loss 0.077590; Mean training F1 0.939718; Mean validation loss 0.064558; Mean validation F1 0.933205; Learning rate 0.000179;\n",
    "===========================================================\n",
    "Epoch 071/096 - Mean training loss 0.077681; Mean training F1 0.939691; Mean validation loss 0.102835; Mean validation F1 0.933340; Learning rate 0.000167;\n",
    "===========================================================\n",
    "Epoch 072/096 - Mean training loss 0.077539; Mean training F1 0.939722; Mean validation loss 0.103151; Mean validation F1 0.933731; Learning rate 0.000155;\n",
    "===========================================================\n",
    "Epoch 073/096 - Mean training loss 0.077400; Mean training F1 0.939805; Mean validation loss 0.065791; Mean validation F1 0.933652; Learning rate 0.000144;\n",
    "===========================================================\n",
    "Epoch 074/096 - Mean training loss 0.077504; Mean training F1 0.939669; Mean validation loss 0.044035; Mean validation F1 0.933470; Learning rate 0.000133;\n",
    "===========================================================\n",
    "Epoch 075/096 - Mean training loss 0.077311; Mean training F1 0.939891; Mean validation loss 0.095358; Mean validation F1 0.933585; Learning rate 0.000123;\n",
    "===========================================================\n",
    "Epoch 076/096 - Mean training loss 0.077277; Mean training F1 0.939967; Mean validation loss 0.081773; Mean validation F1 0.933500; Learning rate 0.000113;\n",
    "===========================================================\n",
    "Epoch 077/096 - Mean training loss 0.077217; Mean training F1 0.939942; Mean validation loss 0.075006; Mean validation F1 0.933440; Learning rate 0.000103;\n",
    "===========================================================\n",
    "Epoch 078/096 - Mean training loss 0.077167; Mean training F1 0.939986; Mean validation loss 0.067719; Mean validation F1 0.933680; Learning rate 0.000094;\n",
    "===========================================================\n",
    "Epoch 079/096 - Mean training loss 0.077113; Mean training F1 0.940014; Mean validation loss 0.071208; Mean validation F1 0.933854; Learning rate 0.000085;\n",
    "===========================================================\n",
    "Epoch 080/096 - Mean training loss 0.077151; Mean training F1 0.939944; Mean validation loss 0.067924; Mean validation F1 0.933608; Learning rate 0.000077;\n",
    "===========================================================\n",
    "Epoch 081/096 - Mean training loss 0.077100; Mean training F1 0.940018; Mean validation loss 0.067855; Mean validation F1 0.933891; Learning rate 0.000069;\n",
    "===========================================================\n",
    "Epoch 082/096 - Mean training loss 0.077034; Mean training F1 0.940062; Mean validation loss 0.071657; Mean validation F1 0.933785; Learning rate 0.000062;\n",
    "===========================================================\n",
    "Epoch 083/096 - Mean training loss 0.076981; Mean training F1 0.940029; Mean validation loss 0.105258; Mean validation F1 0.933713; Learning rate 0.000055;\n",
    "===========================================================\n",
    "Epoch 084/096 - Mean training loss 0.076988; Mean training F1 0.940080; Mean validation loss 0.084434; Mean validation F1 0.933791; Learning rate 0.000048;\n",
    "===========================================================\n",
    "Epoch 085/096 - Mean training loss 0.076933; Mean training F1 0.940054; Mean validation loss 0.079574; Mean validation F1 0.933899; Learning rate 0.000042;\n",
    "===========================================================\n",
    "Epoch 086/096 - Mean training loss 0.076896; Mean training F1 0.940167; Mean validation loss 0.045384; Mean validation F1 0.933878; Learning rate 0.000037;\n",
    "===========================================================\n",
    "Epoch 087/096 - Mean training loss 0.076904; Mean training F1 0.940119; Mean validation loss 0.081878; Mean validation F1 0.933837; Learning rate 0.000032;\n",
    "===========================================================\n",
    "Epoch 088/096 - Mean training loss 0.076875; Mean training F1 0.940176; Mean validation loss 0.065260; Mean validation F1 0.933873; Learning rate 0.000027;\n",
    "===========================================================\n",
    "Epoch 089/096 - Mean training loss 0.076848; Mean training F1 0.940189; Mean validation loss 0.049093; Mean validation F1 0.934078; Learning rate 0.000023;\n",
    "===========================================================\n",
    "Epoch 090/096 - Mean training loss 0.076821; Mean training F1 0.940119; Mean validation loss 0.096540; Mean validation F1 0.933974; Learning rate 0.000020;\n",
    "===========================================================\n",
    "Epoch 091/096 - Mean training loss 0.076806; Mean training F1 0.940126; Mean validation loss 0.082197; Mean validation F1 0.933973; Learning rate 0.000017;\n",
    "===========================================================\n",
    "Epoch 092/096 - Mean training loss 0.076805; Mean training F1 0.940130; Mean validation loss 0.098085; Mean validation F1 0.934011; Learning rate 0.000015;\n",
    "===========================================================\n",
    "Epoch 093/096 - Mean training loss 0.076786; Mean training F1 0.940156; Mean validation loss 0.069115; Mean validation F1 0.934013; Learning rate 0.000013;\n",
    "===========================================================\n",
    "Epoch 094/096 - Mean training loss 0.076775; Mean training F1 0.940213; Mean validation loss 0.071131; Mean validation F1 0.933997; Learning rate 0.000011;\n",
    "===========================================================\n",
    "Epoch 095/096 - Mean training loss 0.076772; Mean training F1 0.940199; Mean validation loss 0.077480; Mean validation F1 0.933967; Learning rate 0.000010;\n",
    "===========================================================\n",
    "Epoch 096/096 - Mean training loss 0.076762; Mean training F1 0.940165; Mean validation loss 0.096719; Mean validation F1 0.933915; Learning rate 0.000010;\n",
    "################################################################\n",
    "Training/validation for fold 3/5;\n",
    "===========================================================\n",
    "Epoch 001/096 - Mean training loss 0.782493; Mean training F1 0.544202; Mean validation loss 0.401294; Mean validation F1 0.733587; Learning rate 0.001000;\n",
    "===========================================================\n",
    "Epoch 002/096 - Mean training loss 0.262992; Mean training F1 0.857229; Mean validation loss 0.133050; Mean validation F1 0.882154; Learning rate 0.000999;\n",
    "===========================================================\n",
    "Epoch 003/096 - Mean training loss 0.146601; Mean training F1 0.910504; Mean validation loss 0.180128; Mean validation F1 0.892433; Learning rate 0.000998;\n",
    "===========================================================\n",
    "Epoch 004/096 - Mean training loss 0.107647; Mean training F1 0.926948; Mean validation loss 0.065302; Mean validation F1 0.929503; Learning rate 0.000996;\n",
    "===========================================================\n",
    "Epoch 005/096 - Mean training loss 0.100213; Mean training F1 0.928569; Mean validation loss 0.137271; Mean validation F1 0.932103; Learning rate 0.000994;\n",
    "===========================================================\n",
    "Epoch 006/096 - Mean training loss 0.091462; Mean training F1 0.934055; Mean validation loss 0.117434; Mean validation F1 0.934081; Learning rate 0.000991;\n",
    "===========================================================\n",
    "Epoch 007/096 - Mean training loss 0.089445; Mean training F1 0.935129; Mean validation loss 0.103406; Mean validation F1 0.934037; Learning rate 0.000988;\n",
    "===========================================================\n",
    "Epoch 008/096 - Mean training loss 0.090316; Mean training F1 0.934604; Mean validation loss 0.082860; Mean validation F1 0.934887; Learning rate 0.000984;\n",
    "===========================================================\n",
    "Epoch 009/096 - Mean training loss 0.088966; Mean training F1 0.935040; Mean validation loss 0.083001; Mean validation F1 0.935401; Learning rate 0.000979;\n",
    "===========================================================\n",
    "Epoch 010/096 - Mean training loss 0.087686; Mean training F1 0.935281; Mean validation loss 0.075496; Mean validation F1 0.933695; Learning rate 0.000974;\n",
    "===========================================================\n",
    "Epoch 011/096 - Mean training loss 0.096590; Mean training F1 0.932342; Mean validation loss 0.162001; Mean validation F1 0.886021; Learning rate 0.000969;\n",
    "===========================================================\n",
    "Epoch 012/096 - Mean training loss 0.133703; Mean training F1 0.918724; Mean validation loss 0.088729; Mean validation F1 0.934033; Learning rate 0.000963;\n",
    "===========================================================\n",
    "Epoch 013/096 - Mean training loss 0.087462; Mean training F1 0.936140; Mean validation loss 0.076404; Mean validation F1 0.935638; Learning rate 0.000956;\n",
    "===========================================================\n",
    "Epoch 014/096 - Mean training loss 0.086564; Mean training F1 0.936370; Mean validation loss 0.059902; Mean validation F1 0.935676; Learning rate 0.000949;\n",
    "===========================================================\n",
    "Epoch 015/096 - Mean training loss 0.086226; Mean training F1 0.936553; Mean validation loss 0.068301; Mean validation F1 0.935607; Learning rate 0.000942;\n",
    "===========================================================\n",
    "Epoch 016/096 - Mean training loss 0.086230; Mean training F1 0.936253; Mean validation loss 0.062677; Mean validation F1 0.935897; Learning rate 0.000934;\n",
    "===========================================================\n",
    "Epoch 017/096 - Mean training loss 0.085812; Mean training F1 0.936581; Mean validation loss 0.109349; Mean validation F1 0.929586; Learning rate 0.000926;\n",
    "===========================================================\n",
    "Epoch 018/096 - Mean training loss 0.085609; Mean training F1 0.936705; Mean validation loss 0.053000; Mean validation F1 0.933679; Learning rate 0.000917;\n",
    "===========================================================\n",
    "Epoch 019/096 - Mean training loss 0.085252; Mean training F1 0.936902; Mean validation loss 0.070629; Mean validation F1 0.934819; Learning rate 0.000908;\n",
    "===========================================================\n",
    "Epoch 020/096 - Mean training loss 0.085689; Mean training F1 0.936658; Mean validation loss 0.097579; Mean validation F1 0.936363; Learning rate 0.000898;\n",
    "===========================================================\n",
    "Epoch 021/096 - Mean training loss 0.085087; Mean training F1 0.936754; Mean validation loss 0.081257; Mean validation F1 0.936046; Learning rate 0.000888;\n",
    "===========================================================\n",
    "Epoch 022/096 - Mean training loss 0.084493; Mean training F1 0.937304; Mean validation loss 0.122982; Mean validation F1 0.934931; Learning rate 0.000878;\n",
    "===========================================================\n",
    "Epoch 023/096 - Mean training loss 0.085681; Mean training F1 0.936446; Mean validation loss 0.087449; Mean validation F1 0.935890; Learning rate 0.000867;\n",
    "===========================================================\n",
    "Epoch 024/096 - Mean training loss 0.084180; Mean training F1 0.937490; Mean validation loss 0.094693; Mean validation F1 0.933845; Learning rate 0.000856;\n",
    "===========================================================\n",
    "Epoch 025/096 - Mean training loss 0.084409; Mean training F1 0.936997; Mean validation loss 0.082594; Mean validation F1 0.934707; Learning rate 0.000844;\n",
    "===========================================================\n",
    "Epoch 026/096 - Mean training loss 0.084226; Mean training F1 0.937039; Mean validation loss 0.081024; Mean validation F1 0.936933; Learning rate 0.000832;\n",
    "===========================================================\n",
    "Epoch 027/096 - Mean training loss 0.084177; Mean training F1 0.937520; Mean validation loss 0.081629; Mean validation F1 0.936680; Learning rate 0.000820;\n",
    "===========================================================\n",
    "Epoch 028/096 - Mean training loss 0.083569; Mean training F1 0.937689; Mean validation loss 0.077791; Mean validation F1 0.936984; Learning rate 0.000807;\n",
    "===========================================================\n",
    "Epoch 029/096 - Mean training loss 0.083928; Mean training F1 0.937412; Mean validation loss 0.058227; Mean validation F1 0.937043; Learning rate 0.000794;\n",
    "===========================================================\n",
    "Epoch 030/096 - Mean training loss 0.083414; Mean training F1 0.937770; Mean validation loss 0.079162; Mean validation F1 0.935387; Learning rate 0.000781;\n",
    "===========================================================\n",
    "Epoch 031/096 - Mean training loss 0.083107; Mean training F1 0.937871; Mean validation loss 0.136871; Mean validation F1 0.936972; Learning rate 0.000767;\n",
    "===========================================================\n",
    "Epoch 032/096 - Mean training loss 0.083658; Mean training F1 0.937671; Mean validation loss 0.048453; Mean validation F1 0.936544; Learning rate 0.000753;\n",
    "===========================================================\n",
    "Epoch 033/096 - Mean training loss 0.083157; Mean training F1 0.937856; Mean validation loss 0.143647; Mean validation F1 0.937108; Learning rate 0.000739;\n",
    "===========================================================\n",
    "Epoch 034/096 - Mean training loss 0.091071; Mean training F1 0.933278; Mean validation loss 0.108225; Mean validation F1 0.918146; Learning rate 0.000724;\n",
    "===========================================================\n",
    "Epoch 035/096 - Mean training loss 0.090199; Mean training F1 0.932510; Mean validation loss 0.047702; Mean validation F1 0.936039; Learning rate 0.000710;\n",
    "===========================================================\n",
    "Epoch 036/096 - Mean training loss 0.083373; Mean training F1 0.937591; Mean validation loss 0.125080; Mean validation F1 0.936968; Learning rate 0.000695;\n",
    "===========================================================\n",
    "Epoch 037/096 - Mean training loss 0.083208; Mean training F1 0.937701; Mean validation loss 0.075552; Mean validation F1 0.935433; Learning rate 0.000680;\n",
    "===========================================================\n",
    "Epoch 038/096 - Mean training loss 0.084524; Mean training F1 0.937205; Mean validation loss 0.050859; Mean validation F1 0.936109; Learning rate 0.000665;\n",
    "===========================================================\n",
    "Epoch 039/096 - Mean training loss 0.082892; Mean training F1 0.937797; Mean validation loss 0.111532; Mean validation F1 0.936948; Learning rate 0.000649;\n",
    "===========================================================\n",
    "Epoch 040/096 - Mean training loss 0.082621; Mean training F1 0.937919; Mean validation loss 0.083878; Mean validation F1 0.936567; Learning rate 0.000634;\n",
    "===========================================================\n",
    "Epoch 041/096 - Mean training loss 0.082919; Mean training F1 0.937822; Mean validation loss 0.054776; Mean validation F1 0.936696; Learning rate 0.000618;\n",
    "===========================================================\n",
    "Epoch 042/096 - Mean training loss 0.082403; Mean training F1 0.938209; Mean validation loss 0.063617; Mean validation F1 0.937292; Learning rate 0.000602;\n",
    "===========================================================\n",
    "Epoch 043/096 - Mean training loss 0.082304; Mean training F1 0.938092; Mean validation loss 0.078510; Mean validation F1 0.935149; Learning rate 0.000586;\n",
    "===========================================================\n",
    "Epoch 044/096 - Mean training loss 0.081912; Mean training F1 0.938470; Mean validation loss 0.089584; Mean validation F1 0.936425; Learning rate 0.000570;\n",
    "===========================================================\n",
    "Epoch 045/096 - Mean training loss 0.082013; Mean training F1 0.938369; Mean validation loss 0.077729; Mean validation F1 0.937098; Learning rate 0.000554;\n",
    "===========================================================\n",
    "Epoch 046/096 - Mean training loss 0.081915; Mean training F1 0.938402; Mean validation loss 0.088013; Mean validation F1 0.936987; Learning rate 0.000538;\n",
    "===========================================================\n",
    "Epoch 047/096 - Mean training loss 0.081976; Mean training F1 0.938242; Mean validation loss 0.099995; Mean validation F1 0.937063; Learning rate 0.000522;\n",
    "===========================================================\n",
    "Epoch 048/096 - Mean training loss 0.082056; Mean training F1 0.938372; Mean validation loss 0.089368; Mean validation F1 0.936857; Learning rate 0.000506;\n",
    "===========================================================\n",
    "Epoch 049/096 - Mean training loss 0.081852; Mean training F1 0.938564; Mean validation loss 0.090396; Mean validation F1 0.937009; Learning rate 0.000489;\n",
    "===========================================================\n",
    "Epoch 050/096 - Mean training loss 0.081517; Mean training F1 0.938693; Mean validation loss 0.077386; Mean validation F1 0.937537; Learning rate 0.000473;\n",
    "===========================================================\n",
    "Epoch 051/096 - Mean training loss 0.081692; Mean training F1 0.938636; Mean validation loss 0.080793; Mean validation F1 0.937396; Learning rate 0.000457;\n",
    "===========================================================\n",
    "Epoch 052/096 - Mean training loss 0.081732; Mean training F1 0.938349; Mean validation loss 0.075692; Mean validation F1 0.937569; Learning rate 0.000441;\n",
    "===========================================================\n",
    "Epoch 053/096 - Mean training loss 0.081613; Mean training F1 0.938369; Mean validation loss 0.070050; Mean validation F1 0.937173; Learning rate 0.000425;\n",
    "===========================================================\n",
    "Epoch 054/096 - Mean training loss 0.081205; Mean training F1 0.938778; Mean validation loss 0.118138; Mean validation F1 0.937453; Learning rate 0.000409;\n",
    "===========================================================\n",
    "Epoch 055/096 - Mean training loss 0.081467; Mean training F1 0.938482; Mean validation loss 0.094309; Mean validation F1 0.937133; Learning rate 0.000393;\n",
    "===========================================================\n",
    "Epoch 056/096 - Mean training loss 0.086437; Mean training F1 0.936480; Mean validation loss 0.064855; Mean validation F1 0.936255; Learning rate 0.000377;\n",
    "===========================================================\n",
    "Epoch 057/096 - Mean training loss 0.082353; Mean training F1 0.938133; Mean validation loss 0.091763; Mean validation F1 0.937093; Learning rate 0.000362;\n",
    "===========================================================\n",
    "Epoch 058/096 - Mean training loss 0.081544; Mean training F1 0.938538; Mean validation loss 0.078550; Mean validation F1 0.936079; Learning rate 0.000346;\n",
    "===========================================================\n",
    "Epoch 059/096 - Mean training loss 0.081438; Mean training F1 0.938482; Mean validation loss 0.083750; Mean validation F1 0.936812; Learning rate 0.000331;\n",
    "===========================================================\n",
    "Epoch 060/096 - Mean training loss 0.081142; Mean training F1 0.938734; Mean validation loss 0.071596; Mean validation F1 0.937215; Learning rate 0.000316;\n",
    "===========================================================\n",
    "Epoch 061/096 - Mean training loss 0.081091; Mean training F1 0.938737; Mean validation loss 0.090903; Mean validation F1 0.937271; Learning rate 0.000301;\n",
    "===========================================================\n",
    "Epoch 062/096 - Mean training loss 0.081071; Mean training F1 0.938872; Mean validation loss 0.079269; Mean validation F1 0.936950; Learning rate 0.000287;\n",
    "===========================================================\n",
    "Epoch 063/096 - Mean training loss 0.080803; Mean training F1 0.938905; Mean validation loss 0.102092; Mean validation F1 0.937298; Learning rate 0.000272;\n",
    "===========================================================\n",
    "Epoch 064/096 - Mean training loss 0.080830; Mean training F1 0.938910; Mean validation loss 0.048447; Mean validation F1 0.937402; Learning rate 0.000258;\n",
    "===========================================================\n",
    "Epoch 065/096 - Mean training loss 0.080713; Mean training F1 0.939002; Mean validation loss 0.069231; Mean validation F1 0.937410; Learning rate 0.000244;\n",
    "===========================================================\n",
    "Epoch 066/096 - Mean training loss 0.080777; Mean training F1 0.939110; Mean validation loss 0.022250; Mean validation F1 0.937201; Learning rate 0.000230;\n",
    "===========================================================\n",
    "Epoch 067/096 - Mean training loss 0.080597; Mean training F1 0.938996; Mean validation loss 0.121557; Mean validation F1 0.937668; Learning rate 0.000217;\n",
    "===========================================================\n",
    "Epoch 068/096 - Mean training loss 0.080652; Mean training F1 0.939002; Mean validation loss 0.077767; Mean validation F1 0.937410; Learning rate 0.000204;\n",
    "===========================================================\n",
    "Epoch 069/096 - Mean training loss 0.080528; Mean training F1 0.939222; Mean validation loss 0.079333; Mean validation F1 0.937827; Learning rate 0.000191;\n",
    "===========================================================\n",
    "Epoch 070/096 - Mean training loss 0.080431; Mean training F1 0.939145; Mean validation loss 0.102696; Mean validation F1 0.937742; Learning rate 0.000179;\n",
    "===========================================================\n",
    "Epoch 071/096 - Mean training loss 0.080382; Mean training F1 0.939151; Mean validation loss 0.051782; Mean validation F1 0.937327; Learning rate 0.000167;\n",
    "===========================================================\n",
    "Epoch 072/096 - Mean training loss 0.080223; Mean training F1 0.939165; Mean validation loss 0.088175; Mean validation F1 0.937420; Learning rate 0.000155;\n",
    "===========================================================\n",
    "Epoch 073/096 - Mean training loss 0.080152; Mean training F1 0.939222; Mean validation loss 0.074692; Mean validation F1 0.937532; Learning rate 0.000144;\n",
    "===========================================================\n",
    "Epoch 074/096 - Mean training loss 0.080204; Mean training F1 0.939253; Mean validation loss 0.061612; Mean validation F1 0.937397; Learning rate 0.000133;\n",
    "===========================================================\n",
    "Epoch 075/096 - Mean training loss 0.080188; Mean training F1 0.939253; Mean validation loss 0.058376; Mean validation F1 0.937639; Learning rate 0.000123;\n",
    "===========================================================\n",
    "Epoch 076/096 - Mean training loss 0.080020; Mean training F1 0.939317; Mean validation loss 0.061067; Mean validation F1 0.937526; Learning rate 0.000113;\n",
    "===========================================================\n",
    "Epoch 077/096 - Mean training loss 0.080082; Mean training F1 0.939319; Mean validation loss 0.093733; Mean validation F1 0.937905; Learning rate 0.000103;\n",
    "===========================================================\n",
    "Epoch 078/096 - Mean training loss 0.079944; Mean training F1 0.939454; Mean validation loss 0.063296; Mean validation F1 0.937838; Learning rate 0.000094;\n",
    "===========================================================\n",
    "Epoch 079/096 - Mean training loss 0.079950; Mean training F1 0.939478; Mean validation loss 0.102829; Mean validation F1 0.937843; Learning rate 0.000085;\n",
    "===========================================================\n",
    "Epoch 080/096 - Mean training loss 0.079866; Mean training F1 0.939468; Mean validation loss 0.049421; Mean validation F1 0.937895; Learning rate 0.000077;\n",
    "===========================================================\n",
    "Epoch 081/096 - Mean training loss 0.079865; Mean training F1 0.939437; Mean validation loss 0.099754; Mean validation F1 0.937680; Learning rate 0.000069;\n",
    "===========================================================\n",
    "Epoch 082/096 - Mean training loss 0.079842; Mean training F1 0.939449; Mean validation loss 0.061354; Mean validation F1 0.937676; Learning rate 0.000062;\n",
    "===========================================================\n",
    "Epoch 083/096 - Mean training loss 0.079786; Mean training F1 0.939440; Mean validation loss 0.105710; Mean validation F1 0.937826; Learning rate 0.000055;\n",
    "===========================================================\n",
    "Epoch 084/096 - Mean training loss 0.079785; Mean training F1 0.939424; Mean validation loss 0.084792; Mean validation F1 0.937769; Learning rate 0.000048;\n",
    "===========================================================\n",
    "Epoch 085/096 - Mean training loss 0.079697; Mean training F1 0.939495; Mean validation loss 0.069863; Mean validation F1 0.937617; Learning rate 0.000042;\n",
    "===========================================================\n",
    "Epoch 086/096 - Mean training loss 0.079680; Mean training F1 0.939475; Mean validation loss 0.079715; Mean validation F1 0.937792; Learning rate 0.000037;\n",
    "===========================================================\n",
    "Epoch 087/096 - Mean training loss 0.079659; Mean training F1 0.939521; Mean validation loss 0.080199; Mean validation F1 0.937857; Learning rate 0.000032;\n",
    "===========================================================\n",
    "Epoch 088/096 - Mean training loss 0.079660; Mean training F1 0.939607; Mean validation loss 0.106754; Mean validation F1 0.937877; Learning rate 0.000027;\n",
    "===========================================================\n",
    "Epoch 089/096 - Mean training loss 0.079622; Mean training F1 0.939565; Mean validation loss 0.067934; Mean validation F1 0.937813; Learning rate 0.000023;\n",
    "===========================================================\n",
    "Epoch 090/096 - Mean training loss 0.079588; Mean training F1 0.939588; Mean validation loss 0.072644; Mean validation F1 0.937859; Learning rate 0.000020;\n",
    "===========================================================\n",
    "Epoch 091/096 - Mean training loss 0.079564; Mean training F1 0.939592; Mean validation loss 0.095518; Mean validation F1 0.937819; Learning rate 0.000017;\n",
    "===========================================================\n",
    "Epoch 092/096 - Mean training loss 0.079557; Mean training F1 0.939600; Mean validation loss 0.054131; Mean validation F1 0.937639; Learning rate 0.000015;\n",
    "===========================================================\n",
    "Epoch 093/096 - Mean training loss 0.079545; Mean training F1 0.939586; Mean validation loss 0.090699; Mean validation F1 0.937932; Learning rate 0.000013;\n",
    "===========================================================\n",
    "Epoch 094/096 - Mean training loss 0.079538; Mean training F1 0.939592; Mean validation loss 0.070184; Mean validation F1 0.937879; Learning rate 0.000011;\n",
    "===========================================================\n",
    "Epoch 095/096 - Mean training loss 0.079528; Mean training F1 0.939629; Mean validation loss 0.117663; Mean validation F1 0.937814; Learning rate 0.000010;\n",
    "===========================================================\n",
    "Epoch 096/096 - Mean training loss 0.079521; Mean training F1 0.939615; Mean validation loss 0.074538; Mean validation F1 0.937898; Learning rate 0.000010;\n",
    "################################################################\n",
    "Training/validation for fold 4/5;\n",
    "===========================================================\n",
    "Epoch 001/096 - Mean training loss 0.822519; Mean training F1 0.530989; Mean validation loss 0.445897; Mean validation F1 0.730828; Learning rate 0.001000;\n",
    "===========================================================\n",
    "Epoch 002/096 - Mean training loss 0.264794; Mean training F1 0.857394; Mean validation loss 0.093873; Mean validation F1 0.909696; Learning rate 0.000999;\n",
    "===========================================================\n",
    "Epoch 003/096 - Mean training loss 0.115820; Mean training F1 0.924304; Mean validation loss 0.056774; Mean validation F1 0.912253; Learning rate 0.000998;\n",
    "===========================================================\n",
    "Epoch 004/096 - Mean training loss 0.133397; Mean training F1 0.912247; Mean validation loss 0.049406; Mean validation F1 0.933318; Learning rate 0.000996;\n",
    "===========================================================\n",
    "Epoch 005/096 - Mean training loss 0.094990; Mean training F1 0.932585; Mean validation loss 0.075137; Mean validation F1 0.935069; Learning rate 0.000994;\n",
    "===========================================================\n",
    "Epoch 006/096 - Mean training loss 0.091940; Mean training F1 0.934010; Mean validation loss 0.064380; Mean validation F1 0.936279; Learning rate 0.000991;\n",
    "===========================================================\n",
    "Epoch 007/096 - Mean training loss 0.090694; Mean training F1 0.934358; Mean validation loss 0.101871; Mean validation F1 0.937280; Learning rate 0.000988;\n",
    "===========================================================\n",
    "Epoch 008/096 - Mean training loss 0.090273; Mean training F1 0.934148; Mean validation loss 0.063539; Mean validation F1 0.937144; Learning rate 0.000984;\n",
    "===========================================================\n",
    "Epoch 009/096 - Mean training loss 0.089273; Mean training F1 0.935022; Mean validation loss 0.099953; Mean validation F1 0.936848; Learning rate 0.000979;\n",
    "===========================================================\n",
    "Epoch 010/096 - Mean training loss 0.089439; Mean training F1 0.934600; Mean validation loss 0.046611; Mean validation F1 0.936660; Learning rate 0.000974;\n",
    "===========================================================\n",
    "Epoch 011/096 - Mean training loss 0.088122; Mean training F1 0.935528; Mean validation loss 0.119006; Mean validation F1 0.937472; Learning rate 0.000969;\n",
    "===========================================================\n",
    "Epoch 012/096 - Mean training loss 0.088051; Mean training F1 0.935539; Mean validation loss 0.102438; Mean validation F1 0.932474; Learning rate 0.000963;\n",
    "===========================================================\n",
    "Epoch 013/096 - Mean training loss 0.088719; Mean training F1 0.934667; Mean validation loss 0.081210; Mean validation F1 0.938717; Learning rate 0.000956;\n",
    "===========================================================\n",
    "Epoch 014/096 - Mean training loss 0.087019; Mean training F1 0.936055; Mean validation loss 0.082832; Mean validation F1 0.938435; Learning rate 0.000949;\n",
    "===========================================================\n",
    "Epoch 015/096 - Mean training loss 0.087194; Mean training F1 0.935832; Mean validation loss 0.059346; Mean validation F1 0.933041; Learning rate 0.000942;\n",
    "===========================================================\n",
    "Epoch 016/096 - Mean training loss 0.089861; Mean training F1 0.933525; Mean validation loss 0.103517; Mean validation F1 0.938718; Learning rate 0.000934;\n",
    "===========================================================\n",
    "Epoch 017/096 - Mean training loss 0.213311; Mean training F1 0.882328; Mean validation loss 0.141293; Mean validation F1 0.924765; Learning rate 0.000926;\n",
    "===========================================================\n",
    "Epoch 018/096 - Mean training loss 0.097972; Mean training F1 0.931997; Mean validation loss 0.110097; Mean validation F1 0.937012; Learning rate 0.000917;\n",
    "===========================================================\n",
    "Epoch 019/096 - Mean training loss 0.091416; Mean training F1 0.934689; Mean validation loss 0.110658; Mean validation F1 0.937342; Learning rate 0.000908;\n",
    "===========================================================\n",
    "Epoch 020/096 - Mean training loss 0.089847; Mean training F1 0.935405; Mean validation loss 0.100610; Mean validation F1 0.937368; Learning rate 0.000898;\n",
    "===========================================================\n",
    "Epoch 021/096 - Mean training loss 0.088566; Mean training F1 0.935989; Mean validation loss 0.084553; Mean validation F1 0.937752; Learning rate 0.000888;\n",
    "===========================================================\n",
    "Epoch 022/096 - Mean training loss 0.087879; Mean training F1 0.936132; Mean validation loss 0.096647; Mean validation F1 0.938165; Learning rate 0.000878;\n",
    "===========================================================\n",
    "Epoch 023/096 - Mean training loss 0.088076; Mean training F1 0.935789; Mean validation loss 0.105056; Mean validation F1 0.937295; Learning rate 0.000867;\n",
    "===========================================================\n",
    "Epoch 024/096 - Mean training loss 0.087483; Mean training F1 0.936162; Mean validation loss 0.061106; Mean validation F1 0.938419; Learning rate 0.000856;\n",
    "===========================================================\n",
    "Epoch 025/096 - Mean training loss 0.087507; Mean training F1 0.936125; Mean validation loss 0.076394; Mean validation F1 0.938113; Learning rate 0.000844;\n",
    "===========================================================\n",
    "Epoch 026/096 - Mean training loss 0.087012; Mean training F1 0.936319; Mean validation loss 0.087110; Mean validation F1 0.938662; Learning rate 0.000832;\n",
    "===========================================================\n",
    "Epoch 027/096 - Mean training loss 0.086535; Mean training F1 0.936600; Mean validation loss 0.072907; Mean validation F1 0.936811; Learning rate 0.000820;\n",
    "===========================================================\n",
    "Epoch 028/096 - Mean training loss 0.086596; Mean training F1 0.936418; Mean validation loss 0.100030; Mean validation F1 0.937927; Learning rate 0.000807;\n",
    "===========================================================\n",
    "Epoch 029/096 - Mean training loss 0.086567; Mean training F1 0.936360; Mean validation loss 0.075460; Mean validation F1 0.938363; Learning rate 0.000794;\n",
    "===========================================================\n",
    "Epoch 030/096 - Mean training loss 0.085845; Mean training F1 0.936918; Mean validation loss 0.103722; Mean validation F1 0.938518; Learning rate 0.000781;\n",
    "===========================================================\n",
    "Epoch 031/096 - Mean training loss 0.085847; Mean training F1 0.937014; Mean validation loss 0.069801; Mean validation F1 0.938474; Learning rate 0.000767;\n",
    "===========================================================\n",
    "Epoch 032/096 - Mean training loss 0.085826; Mean training F1 0.936757; Mean validation loss 0.081526; Mean validation F1 0.939117; Learning rate 0.000753;\n",
    "===========================================================\n",
    "Epoch 033/096 - Mean training loss 0.085418; Mean training F1 0.936856; Mean validation loss 0.073682; Mean validation F1 0.938280; Learning rate 0.000739;\n",
    "===========================================================\n",
    "Epoch 034/096 - Mean training loss 0.085842; Mean training F1 0.936951; Mean validation loss 0.059058; Mean validation F1 0.938661; Learning rate 0.000724;\n",
    "===========================================================\n",
    "Epoch 035/096 - Mean training loss 0.085270; Mean training F1 0.937010; Mean validation loss 0.105829; Mean validation F1 0.935960; Learning rate 0.000710;\n",
    "===========================================================\n",
    "Epoch 036/096 - Mean training loss 0.085323; Mean training F1 0.937019; Mean validation loss 0.106179; Mean validation F1 0.938798; Learning rate 0.000695;\n",
    "===========================================================\n",
    "Epoch 037/096 - Mean training loss 0.085159; Mean training F1 0.936943; Mean validation loss 0.078706; Mean validation F1 0.938760; Learning rate 0.000680;\n",
    "===========================================================\n",
    "Epoch 038/096 - Mean training loss 0.084785; Mean training F1 0.937345; Mean validation loss 0.076454; Mean validation F1 0.938995; Learning rate 0.000665;\n",
    "===========================================================\n",
    "Epoch 039/096 - Mean training loss 0.085259; Mean training F1 0.936890; Mean validation loss 0.099968; Mean validation F1 0.938250; Learning rate 0.000649;\n",
    "===========================================================\n",
    "Epoch 040/096 - Mean training loss 0.085374; Mean training F1 0.936797; Mean validation loss 0.106219; Mean validation F1 0.939014; Learning rate 0.000634;\n",
    "===========================================================\n",
    "Epoch 041/096 - Mean training loss 0.084444; Mean training F1 0.937437; Mean validation loss 0.100536; Mean validation F1 0.938931; Learning rate 0.000618;\n",
    "===========================================================\n",
    "Epoch 042/096 - Mean training loss 0.084212; Mean training F1 0.937416; Mean validation loss 0.080354; Mean validation F1 0.938933; Learning rate 0.000602;\n",
    "===========================================================\n",
    "Epoch 043/096 - Mean training loss 0.084138; Mean training F1 0.937495; Mean validation loss 0.082283; Mean validation F1 0.938386; Learning rate 0.000586;\n",
    "===========================================================\n",
    "Epoch 044/096 - Mean training loss 0.084215; Mean training F1 0.937474; Mean validation loss 0.080872; Mean validation F1 0.939190; Learning rate 0.000570;\n",
    "===========================================================\n",
    "Epoch 045/096 - Mean training loss 0.084531; Mean training F1 0.937001; Mean validation loss 0.074062; Mean validation F1 0.938015; Learning rate 0.000554;\n",
    "===========================================================\n",
    "Epoch 046/096 - Mean training loss 0.084136; Mean training F1 0.937216; Mean validation loss 0.110348; Mean validation F1 0.939059; Learning rate 0.000538;\n",
    "===========================================================\n",
    "Epoch 047/096 - Mean training loss 0.083689; Mean training F1 0.937807; Mean validation loss 0.115245; Mean validation F1 0.939128; Learning rate 0.000522;\n",
    "===========================================================\n",
    "Epoch 048/096 - Mean training loss 0.083603; Mean training F1 0.937765; Mean validation loss 0.042995; Mean validation F1 0.939499; Learning rate 0.000506;\n",
    "===========================================================\n",
    "Epoch 049/096 - Mean training loss 0.083639; Mean training F1 0.937682; Mean validation loss 0.062571; Mean validation F1 0.939501; Learning rate 0.000489;\n",
    "===========================================================\n",
    "Epoch 050/096 - Mean training loss 0.083571; Mean training F1 0.937692; Mean validation loss 0.086298; Mean validation F1 0.939349; Learning rate 0.000473;\n",
    "===========================================================\n",
    "Epoch 051/096 - Mean training loss 0.083255; Mean training F1 0.937992; Mean validation loss 0.091233; Mean validation F1 0.938706; Learning rate 0.000457;\n",
    "===========================================================\n",
    "Epoch 052/096 - Mean training loss 0.083129; Mean training F1 0.937923; Mean validation loss 0.098701; Mean validation F1 0.939406; Learning rate 0.000441;\n",
    "===========================================================\n",
    "Epoch 053/096 - Mean training loss 0.083340; Mean training F1 0.937798; Mean validation loss 0.080698; Mean validation F1 0.939440; Learning rate 0.000425;\n",
    "===========================================================\n",
    "Epoch 054/096 - Mean training loss 0.085059; Mean training F1 0.937020; Mean validation loss 0.127962; Mean validation F1 0.938506; Learning rate 0.000409;\n",
    "===========================================================\n",
    "Epoch 055/096 - Mean training loss 0.091389; Mean training F1 0.932591; Mean validation loss 0.070638; Mean validation F1 0.939048; Learning rate 0.000393;\n",
    "===========================================================\n",
    "Epoch 056/096 - Mean training loss 0.083472; Mean training F1 0.937907; Mean validation loss 0.077879; Mean validation F1 0.938601; Learning rate 0.000377;\n",
    "===========================================================\n",
    "Epoch 057/096 - Mean training loss 0.083411; Mean training F1 0.937698; Mean validation loss 0.117474; Mean validation F1 0.939097; Learning rate 0.000362;\n",
    "===========================================================\n",
    "Epoch 058/096 - Mean training loss 0.082986; Mean training F1 0.938028; Mean validation loss 0.088841; Mean validation F1 0.939406; Learning rate 0.000346;\n",
    "===========================================================\n",
    "Epoch 059/096 - Mean training loss 0.082693; Mean training F1 0.938167; Mean validation loss 0.070878; Mean validation F1 0.939470; Learning rate 0.000331;\n",
    "===========================================================\n",
    "Epoch 060/096 - Mean training loss 0.082845; Mean training F1 0.938089; Mean validation loss 0.071650; Mean validation F1 0.939736; Learning rate 0.000316;\n",
    "===========================================================\n",
    "Epoch 061/096 - Mean training loss 0.082518; Mean training F1 0.938217; Mean validation loss 0.094209; Mean validation F1 0.939674; Learning rate 0.000301;\n",
    "===========================================================\n",
    "Epoch 062/096 - Mean training loss 0.082536; Mean training F1 0.938295; Mean validation loss 0.077001; Mean validation F1 0.938921; Learning rate 0.000287;\n",
    "===========================================================\n",
    "Epoch 063/096 - Mean training loss 0.082385; Mean training F1 0.938311; Mean validation loss 0.115901; Mean validation F1 0.939729; Learning rate 0.000272;\n",
    "===========================================================\n",
    "Epoch 064/096 - Mean training loss 0.082330; Mean training F1 0.938258; Mean validation loss 0.065644; Mean validation F1 0.939205; Learning rate 0.000258;\n",
    "===========================================================\n",
    "Epoch 065/096 - Mean training loss 0.082262; Mean training F1 0.938404; Mean validation loss 0.078993; Mean validation F1 0.939725; Learning rate 0.000244;\n",
    "===========================================================\n",
    "Epoch 066/096 - Mean training loss 0.082111; Mean training F1 0.938502; Mean validation loss 0.104914; Mean validation F1 0.939851; Learning rate 0.000230;\n",
    "===========================================================\n",
    "Epoch 067/096 - Mean training loss 0.082164; Mean training F1 0.938454; Mean validation loss 0.078586; Mean validation F1 0.939426; Learning rate 0.000217;\n",
    "===========================================================\n",
    "Epoch 068/096 - Mean training loss 0.081918; Mean training F1 0.938573; Mean validation loss 0.100956; Mean validation F1 0.939756; Learning rate 0.000204;\n",
    "===========================================================\n",
    "Epoch 069/096 - Mean training loss 0.081737; Mean training F1 0.938670; Mean validation loss 0.089187; Mean validation F1 0.939258; Learning rate 0.000191;\n",
    "===========================================================\n",
    "Epoch 070/096 - Mean training loss 0.081851; Mean training F1 0.938428; Mean validation loss 0.076911; Mean validation F1 0.939334; Learning rate 0.000179;\n",
    "===========================================================\n",
    "Epoch 071/096 - Mean training loss 0.081753; Mean training F1 0.938614; Mean validation loss 0.076705; Mean validation F1 0.939634; Learning rate 0.000167;\n",
    "===========================================================\n",
    "Epoch 072/096 - Mean training loss 0.081697; Mean training F1 0.938570; Mean validation loss 0.090505; Mean validation F1 0.939937; Learning rate 0.000155;\n",
    "===========================================================\n",
    "Epoch 073/096 - Mean training loss 0.081561; Mean training F1 0.938662; Mean validation loss 0.101635; Mean validation F1 0.939785; Learning rate 0.000144;\n",
    "===========================================================\n",
    "Epoch 074/096 - Mean training loss 0.081506; Mean training F1 0.938715; Mean validation loss 0.050533; Mean validation F1 0.939725; Learning rate 0.000133;\n",
    "===========================================================\n",
    "Epoch 075/096 - Mean training loss 0.081544; Mean training F1 0.938570; Mean validation loss 0.029284; Mean validation F1 0.939963; Learning rate 0.000123;\n",
    "===========================================================\n",
    "Epoch 076/096 - Mean training loss 0.081374; Mean training F1 0.938792; Mean validation loss 0.096688; Mean validation F1 0.939941; Learning rate 0.000113;\n",
    "===========================================================\n",
    "Epoch 077/096 - Mean training loss 0.081339; Mean training F1 0.938745; Mean validation loss 0.126693; Mean validation F1 0.939720; Learning rate 0.000103;\n",
    "===========================================================\n",
    "Epoch 078/096 - Mean training loss 0.081255; Mean training F1 0.938814; Mean validation loss 0.083306; Mean validation F1 0.939724; Learning rate 0.000094;\n",
    "===========================================================\n",
    "Epoch 079/096 - Mean training loss 0.081197; Mean training F1 0.938844; Mean validation loss 0.100287; Mean validation F1 0.939706; Learning rate 0.000085;\n",
    "===========================================================\n",
    "Epoch 080/096 - Mean training loss 0.081214; Mean training F1 0.938910; Mean validation loss 0.065948; Mean validation F1 0.939782; Learning rate 0.000077;\n",
    "===========================================================\n",
    "Epoch 081/096 - Mean training loss 0.081152; Mean training F1 0.938853; Mean validation loss 0.098222; Mean validation F1 0.939866; Learning rate 0.000069;\n",
    "===========================================================\n",
    "Epoch 082/096 - Mean training loss 0.081075; Mean training F1 0.938941; Mean validation loss 0.087187; Mean validation F1 0.939992; Learning rate 0.000062;\n",
    "===========================================================\n",
    "Epoch 083/096 - Mean training loss 0.081061; Mean training F1 0.938991; Mean validation loss 0.076269; Mean validation F1 0.939861; Learning rate 0.000055;\n",
    "===========================================================\n",
    "Epoch 084/096 - Mean training loss 0.081002; Mean training F1 0.938970; Mean validation loss 0.044130; Mean validation F1 0.940034; Learning rate 0.000048;\n",
    "===========================================================\n",
    "Epoch 085/096 - Mean training loss 0.080961; Mean training F1 0.939067; Mean validation loss 0.042221; Mean validation F1 0.939727; Learning rate 0.000042;\n",
    "===========================================================\n",
    "Epoch 086/096 - Mean training loss 0.080898; Mean training F1 0.939034; Mean validation loss 0.073411; Mean validation F1 0.940034; Learning rate 0.000037;\n",
    "===========================================================\n",
    "Epoch 087/096 - Mean training loss 0.080865; Mean training F1 0.939105; Mean validation loss 0.062924; Mean validation F1 0.939794; Learning rate 0.000032;\n",
    "===========================================================\n",
    "Epoch 088/096 - Mean training loss 0.080852; Mean training F1 0.939055; Mean validation loss 0.077422; Mean validation F1 0.939945; Learning rate 0.000027;\n",
    "===========================================================\n",
    "Epoch 089/096 - Mean training loss 0.080818; Mean training F1 0.939089; Mean validation loss 0.090327; Mean validation F1 0.939961; Learning rate 0.000023;\n",
    "===========================================================\n",
    "Epoch 090/096 - Mean training loss 0.080810; Mean training F1 0.939066; Mean validation loss 0.069376; Mean validation F1 0.939744; Learning rate 0.000020;\n",
    "===========================================================\n",
    "Epoch 091/096 - Mean training loss 0.080805; Mean training F1 0.939095; Mean validation loss 0.070339; Mean validation F1 0.939833; Learning rate 0.000017;\n",
    "===========================================================\n",
    "Epoch 092/096 - Mean training loss 0.080767; Mean training F1 0.939049; Mean validation loss 0.099036; Mean validation F1 0.939859; Learning rate 0.000015;\n",
    "===========================================================\n",
    "Epoch 093/096 - Mean training loss 0.080753; Mean training F1 0.939150; Mean validation loss 0.093636; Mean validation F1 0.939911; Learning rate 0.000013;\n",
    "===========================================================\n",
    "Epoch 094/096 - Mean training loss 0.080742; Mean training F1 0.939191; Mean validation loss 0.106249; Mean validation F1 0.939901; Learning rate 0.000011;\n",
    "===========================================================\n",
    "Epoch 095/096 - Mean training loss 0.080727; Mean training F1 0.939220; Mean validation loss 0.085172; Mean validation F1 0.939880; Learning rate 0.000010;\n",
    "===========================================================\n",
    "Epoch 096/096 - Mean training loss 0.080722; Mean training F1 0.939191; Mean validation loss 0.087238; Mean validation F1 0.940008; Learning rate 0.000010;\n",
    "################################################################\n",
    "Training/validation for fold 5/5;\n",
    "===========================================================\n",
    "Epoch 001/096 - Mean training loss 0.827274; Mean training F1 0.517451; Mean validation loss 0.593668; Mean validation F1 0.738054; Learning rate 0.001000;\n",
    "===========================================================\n",
    "Epoch 002/096 - Mean training loss 0.294775; Mean training F1 0.839073; Mean validation loss 0.140100; Mean validation F1 0.902356; Learning rate 0.000999;\n",
    "===========================================================\n",
    "Epoch 003/096 - Mean training loss 0.162022; Mean training F1 0.904254; Mean validation loss 0.134153; Mean validation F1 0.903391; Learning rate 0.000998;\n",
    "===========================================================\n",
    "Epoch 004/096 - Mean training loss 0.118569; Mean training F1 0.921312; Mean validation loss 0.114595; Mean validation F1 0.934263; Learning rate 0.000996;\n",
    "===========================================================\n",
    "Epoch 005/096 - Mean training loss 0.096956; Mean training F1 0.932416; Mean validation loss 0.075877; Mean validation F1 0.935579; Learning rate 0.000994;\n",
    "===========================================================\n",
    "Epoch 006/096 - Mean training loss 0.094709; Mean training F1 0.933285; Mean validation loss 0.056622; Mean validation F1 0.934732; Learning rate 0.000991;\n",
    "===========================================================\n",
    "Epoch 007/096 - Mean training loss 0.093118; Mean training F1 0.933999; Mean validation loss 0.116652; Mean validation F1 0.930786; Learning rate 0.000988;\n",
    "===========================================================\n",
    "Epoch 008/096 - Mean training loss 0.092685; Mean training F1 0.933822; Mean validation loss 0.115539; Mean validation F1 0.936847; Learning rate 0.000984;\n",
    "===========================================================\n",
    "Epoch 009/096 - Mean training loss 0.093442; Mean training F1 0.932689; Mean validation loss 0.092979; Mean validation F1 0.935918; Learning rate 0.000979;\n",
    "===========================================================\n",
    "Epoch 010/096 - Mean training loss 0.093409; Mean training F1 0.933040; Mean validation loss 0.099291; Mean validation F1 0.937067; Learning rate 0.000974;\n",
    "===========================================================\n",
    "Epoch 011/096 - Mean training loss 0.088909; Mean training F1 0.935795; Mean validation loss 0.087002; Mean validation F1 0.936677; Learning rate 0.000969;\n",
    "===========================================================\n",
    "Epoch 012/096 - Mean training loss 0.106072; Mean training F1 0.924450; Mean validation loss 0.078938; Mean validation F1 0.911981; Learning rate 0.000963;\n",
    "===========================================================\n",
    "Epoch 013/096 - Mean training loss 0.090792; Mean training F1 0.934220; Mean validation loss 0.077556; Mean validation F1 0.932718; Learning rate 0.000956;\n",
    "===========================================================\n",
    "Epoch 014/096 - Mean training loss 0.088881; Mean training F1 0.935340; Mean validation loss 0.111562; Mean validation F1 0.936717; Learning rate 0.000949;\n",
    "===========================================================\n",
    "Epoch 015/096 - Mean training loss 0.088976; Mean training F1 0.934892; Mean validation loss 0.083797; Mean validation F1 0.936537; Learning rate 0.000942;\n",
    "===========================================================\n",
    "Epoch 016/096 - Mean training loss 0.088104; Mean training F1 0.935750; Mean validation loss 0.115846; Mean validation F1 0.937465; Learning rate 0.000934;\n",
    "===========================================================\n",
    "Epoch 017/096 - Mean training loss 0.086868; Mean training F1 0.936470; Mean validation loss 0.096767; Mean validation F1 0.937514; Learning rate 0.000926;\n",
    "===========================================================\n",
    "Epoch 018/096 - Mean training loss 0.087369; Mean training F1 0.936160; Mean validation loss 0.052579; Mean validation F1 0.936040; Learning rate 0.000917;\n",
    "===========================================================\n",
    "Epoch 019/096 - Mean training loss 0.086322; Mean training F1 0.936642; Mean validation loss 0.127808; Mean validation F1 0.937293; Learning rate 0.000908;\n",
    "===========================================================\n",
    "Epoch 020/096 - Mean training loss 0.086991; Mean training F1 0.936146; Mean validation loss 0.062237; Mean validation F1 0.937336; Learning rate 0.000898;\n",
    "===========================================================\n",
    "Epoch 021/096 - Mean training loss 0.086576; Mean training F1 0.936364; Mean validation loss 0.115117; Mean validation F1 0.937773; Learning rate 0.000888;\n",
    "===========================================================\n",
    "Epoch 022/096 - Mean training loss 0.086494; Mean training F1 0.936232; Mean validation loss 0.076762; Mean validation F1 0.936126; Learning rate 0.000878;\n",
    "===========================================================\n",
    "Epoch 023/096 - Mean training loss 0.085842; Mean training F1 0.936633; Mean validation loss 0.094994; Mean validation F1 0.937437; Learning rate 0.000867;\n",
    "===========================================================\n",
    "Epoch 024/096 - Mean training loss 0.085778; Mean training F1 0.936632; Mean validation loss 0.091814; Mean validation F1 0.935948; Learning rate 0.000856;\n",
    "===========================================================\n",
    "Epoch 025/096 - Mean training loss 0.086623; Mean training F1 0.936021; Mean validation loss 0.096353; Mean validation F1 0.937438; Learning rate 0.000844;\n",
    "===========================================================\n",
    "Epoch 026/096 - Mean training loss 0.085557; Mean training F1 0.936502; Mean validation loss 0.072107; Mean validation F1 0.937369; Learning rate 0.000832;\n",
    "===========================================================\n",
    "Epoch 027/096 - Mean training loss 0.085411; Mean training F1 0.936810; Mean validation loss 0.078653; Mean validation F1 0.937742; Learning rate 0.000820;\n",
    "===========================================================\n",
    "Epoch 028/096 - Mean training loss 0.087348; Mean training F1 0.935638; Mean validation loss 0.082748; Mean validation F1 0.936637; Learning rate 0.000807;\n",
    "===========================================================\n",
    "Epoch 029/096 - Mean training loss 0.084719; Mean training F1 0.937442; Mean validation loss 0.092786; Mean validation F1 0.937310; Learning rate 0.000794;\n",
    "===========================================================\n",
    "Epoch 030/096 - Mean training loss 0.085424; Mean training F1 0.936302; Mean validation loss 0.098501; Mean validation F1 0.935848; Learning rate 0.000781;\n",
    "===========================================================\n",
    "Epoch 031/096 - Mean training loss 0.084424; Mean training F1 0.937250; Mean validation loss 0.067925; Mean validation F1 0.936808; Learning rate 0.000767;\n",
    "===========================================================\n",
    "Epoch 032/096 - Mean training loss 0.084434; Mean training F1 0.937386; Mean validation loss 0.035691; Mean validation F1 0.933655; Learning rate 0.000753;\n",
    "===========================================================\n",
    "Epoch 033/096 - Mean training loss 0.084134; Mean training F1 0.937279; Mean validation loss 0.078826; Mean validation F1 0.937281; Learning rate 0.000739;\n",
    "===========================================================\n",
    "Epoch 034/096 - Mean training loss 0.084686; Mean training F1 0.937032; Mean validation loss 0.075029; Mean validation F1 0.937477; Learning rate 0.000724;\n",
    "===========================================================\n",
    "Epoch 035/096 - Mean training loss 0.083862; Mean training F1 0.937689; Mean validation loss 0.055692; Mean validation F1 0.938299; Learning rate 0.000710;\n",
    "===========================================================\n",
    "Epoch 036/096 - Mean training loss 0.084489; Mean training F1 0.937176; Mean validation loss 0.065795; Mean validation F1 0.938324; Learning rate 0.000695;\n",
    "===========================================================\n",
    "Epoch 037/096 - Mean training loss 0.083960; Mean training F1 0.937372; Mean validation loss 0.092055; Mean validation F1 0.936470; Learning rate 0.000680;\n",
    "===========================================================\n",
    "Epoch 038/096 - Mean training loss 0.084323; Mean training F1 0.937339; Mean validation loss 0.064044; Mean validation F1 0.936499; Learning rate 0.000665;\n",
    "===========================================================\n",
    "Epoch 039/096 - Mean training loss 0.084029; Mean training F1 0.937169; Mean validation loss 0.087501; Mean validation F1 0.937564; Learning rate 0.000649;\n",
    "===========================================================\n",
    "Epoch 040/096 - Mean training loss 0.083565; Mean training F1 0.937474; Mean validation loss 0.082408; Mean validation F1 0.938423; Learning rate 0.000634;\n",
    "===========================================================\n",
    "Epoch 041/096 - Mean training loss 0.082978; Mean training F1 0.937925; Mean validation loss 0.056467; Mean validation F1 0.938518; Learning rate 0.000618;\n",
    "===========================================================\n",
    "Epoch 042/096 - Mean training loss 0.083232; Mean training F1 0.937664; Mean validation loss 0.076483; Mean validation F1 0.937867; Learning rate 0.000602;\n",
    "===========================================================\n",
    "Epoch 043/096 - Mean training loss 0.083087; Mean training F1 0.937665; Mean validation loss 0.052899; Mean validation F1 0.933242; Learning rate 0.000586;\n",
    "===========================================================\n",
    "Epoch 044/096 - Mean training loss 0.083226; Mean training F1 0.937756; Mean validation loss 0.094815; Mean validation F1 0.938343; Learning rate 0.000570;\n",
    "===========================================================\n",
    "Epoch 045/096 - Mean training loss 0.082711; Mean training F1 0.938026; Mean validation loss 0.068244; Mean validation F1 0.938517; Learning rate 0.000554;\n",
    "===========================================================\n",
    "Epoch 046/096 - Mean training loss 0.082890; Mean training F1 0.937690; Mean validation loss 0.086999; Mean validation F1 0.938097; Learning rate 0.000538;\n",
    "===========================================================\n",
    "Epoch 047/096 - Mean training loss 0.082894; Mean training F1 0.937832; Mean validation loss 0.057548; Mean validation F1 0.938172; Learning rate 0.000522;\n",
    "===========================================================\n",
    "Epoch 048/096 - Mean training loss 0.082551; Mean training F1 0.938054; Mean validation loss 0.081196; Mean validation F1 0.938678; Learning rate 0.000506;\n",
    "===========================================================\n",
    "Epoch 049/096 - Mean training loss 0.081792; Mean training F1 0.938489; Mean validation loss 0.089212; Mean validation F1 0.937576; Learning rate 0.000489;\n",
    "===========================================================\n",
    "Epoch 050/096 - Mean training loss 0.081982; Mean training F1 0.938345; Mean validation loss 0.067466; Mean validation F1 0.936085; Learning rate 0.000473;\n",
    "===========================================================\n",
    "Epoch 051/096 - Mean training loss 0.082085; Mean training F1 0.938261; Mean validation loss 0.090342; Mean validation F1 0.938562; Learning rate 0.000457;\n",
    "===========================================================\n",
    "Epoch 052/096 - Mean training loss 0.081884; Mean training F1 0.938264; Mean validation loss 0.073756; Mean validation F1 0.938353; Learning rate 0.000441;\n",
    "===========================================================\n",
    "Epoch 053/096 - Mean training loss 0.081903; Mean training F1 0.938282; Mean validation loss 0.083281; Mean validation F1 0.938310; Learning rate 0.000425;\n",
    "===========================================================\n",
    "Epoch 054/096 - Mean training loss 0.081667; Mean training F1 0.938637; Mean validation loss 0.067067; Mean validation F1 0.938380; Learning rate 0.000409;\n",
    "===========================================================\n",
    "Epoch 055/096 - Mean training loss 0.081524; Mean training F1 0.938653; Mean validation loss 0.099746; Mean validation F1 0.938331; Learning rate 0.000393;\n",
    "===========================================================\n",
    "Epoch 056/096 - Mean training loss 0.083392; Mean training F1 0.937793; Mean validation loss 0.070869; Mean validation F1 0.938436; Learning rate 0.000377;\n",
    "===========================================================\n",
    "Epoch 057/096 - Mean training loss 0.081650; Mean training F1 0.938494; Mean validation loss 0.113015; Mean validation F1 0.938723; Learning rate 0.000362;\n",
    "===========================================================\n",
    "Epoch 058/096 - Mean training loss 0.081312; Mean training F1 0.938656; Mean validation loss 0.074541; Mean validation F1 0.937918; Learning rate 0.000346;\n",
    "===========================================================\n",
    "Epoch 059/096 - Mean training loss 0.081447; Mean training F1 0.938642; Mean validation loss 0.094846; Mean validation F1 0.938344; Learning rate 0.000331;\n",
    "===========================================================\n",
    "Epoch 060/096 - Mean training loss 0.081094; Mean training F1 0.938770; Mean validation loss 0.051933; Mean validation F1 0.938703; Learning rate 0.000316;\n",
    "===========================================================\n",
    "Epoch 061/096 - Mean training loss 0.081002; Mean training F1 0.938981; Mean validation loss 0.052066; Mean validation F1 0.938656; Learning rate 0.000301;\n",
    "===========================================================\n",
    "Epoch 062/096 - Mean training loss 0.080772; Mean training F1 0.938989; Mean validation loss 0.082560; Mean validation F1 0.938769; Learning rate 0.000287;\n",
    "===========================================================\n",
    "Epoch 063/096 - Mean training loss 0.080733; Mean training F1 0.939009; Mean validation loss 0.100314; Mean validation F1 0.938599; Learning rate 0.000272;\n",
    "===========================================================\n",
    "Epoch 064/096 - Mean training loss 0.080770; Mean training F1 0.939014; Mean validation loss 0.071681; Mean validation F1 0.938520; Learning rate 0.000258;\n",
    "===========================================================\n",
    "Epoch 065/096 - Mean training loss 0.080785; Mean training F1 0.939026; Mean validation loss 0.072974; Mean validation F1 0.938747; Learning rate 0.000244;\n",
    "===========================================================\n",
    "Epoch 066/096 - Mean training loss 0.080590; Mean training F1 0.939029; Mean validation loss 0.104715; Mean validation F1 0.938525; Learning rate 0.000230;\n",
    "===========================================================\n",
    "Epoch 067/096 - Mean training loss 0.080629; Mean training F1 0.939065; Mean validation loss 0.071216; Mean validation F1 0.938651; Learning rate 0.000217;\n",
    "===========================================================\n",
    "Epoch 068/096 - Mean training loss 0.080665; Mean training F1 0.939102; Mean validation loss 0.073136; Mean validation F1 0.938676; Learning rate 0.000204;\n",
    "===========================================================\n",
    "Epoch 069/096 - Mean training loss 0.080444; Mean training F1 0.939239; Mean validation loss 0.112388; Mean validation F1 0.938692; Learning rate 0.000191;\n",
    "===========================================================\n",
    "Epoch 070/096 - Mean training loss 0.080253; Mean training F1 0.939405; Mean validation loss 0.085884; Mean validation F1 0.938930; Learning rate 0.000179;\n",
    "===========================================================\n",
    "Epoch 071/096 - Mean training loss 0.080278; Mean training F1 0.939324; Mean validation loss 0.088168; Mean validation F1 0.938761; Learning rate 0.000167;\n",
    "===========================================================\n",
    "Epoch 072/096 - Mean training loss 0.080264; Mean training F1 0.939122; Mean validation loss 0.074711; Mean validation F1 0.938411; Learning rate 0.000155;\n",
    "===========================================================\n",
    "Epoch 073/096 - Mean training loss 0.080135; Mean training F1 0.939430; Mean validation loss 0.077903; Mean validation F1 0.938568; Learning rate 0.000144;\n",
    "===========================================================\n",
    "Epoch 074/096 - Mean training loss 0.080092; Mean training F1 0.939321; Mean validation loss 0.077710; Mean validation F1 0.938820; Learning rate 0.000133;\n",
    "===========================================================\n",
    "Epoch 075/096 - Mean training loss 0.080005; Mean training F1 0.939497; Mean validation loss 0.075597; Mean validation F1 0.938876; Learning rate 0.000123;\n",
    "===========================================================\n",
    "Epoch 076/096 - Mean training loss 0.079995; Mean training F1 0.939450; Mean validation loss 0.069082; Mean validation F1 0.938829; Learning rate 0.000113;\n",
    "===========================================================\n",
    "Epoch 077/096 - Mean training loss 0.079928; Mean training F1 0.939423; Mean validation loss 0.082381; Mean validation F1 0.938865; Learning rate 0.000103;\n",
    "===========================================================\n",
    "Epoch 078/096 - Mean training loss 0.079909; Mean training F1 0.939352; Mean validation loss 0.094431; Mean validation F1 0.938791; Learning rate 0.000094;\n",
    "===========================================================\n",
    "Epoch 079/096 - Mean training loss 0.079848; Mean training F1 0.939467; Mean validation loss 0.059024; Mean validation F1 0.938882; Learning rate 0.000085;\n",
    "===========================================================\n",
    "Epoch 080/096 - Mean training loss 0.079760; Mean training F1 0.939598; Mean validation loss 0.075146; Mean validation F1 0.939071; Learning rate 0.000077;\n",
    "===========================================================\n",
    "Epoch 081/096 - Mean training loss 0.079704; Mean training F1 0.939604; Mean validation loss 0.090981; Mean validation F1 0.939008; Learning rate 0.000069;\n",
    "===========================================================\n",
    "Epoch 082/096 - Mean training loss 0.079674; Mean training F1 0.939583; Mean validation loss 0.082462; Mean validation F1 0.938890; Learning rate 0.000062;\n",
    "===========================================================\n",
    "Epoch 083/096 - Mean training loss 0.079657; Mean training F1 0.939685; Mean validation loss 0.106357; Mean validation F1 0.938837; Learning rate 0.000055;\n",
    "===========================================================\n",
    "Epoch 084/096 - Mean training loss 0.079610; Mean training F1 0.939586; Mean validation loss 0.115958; Mean validation F1 0.938954; Learning rate 0.000048;\n",
    "===========================================================\n",
    "Epoch 085/096 - Mean training loss 0.079543; Mean training F1 0.939671; Mean validation loss 0.099545; Mean validation F1 0.939023; Learning rate 0.000042;\n",
    "===========================================================\n",
    "Epoch 086/096 - Mean training loss 0.079544; Mean training F1 0.939669; Mean validation loss 0.074048; Mean validation F1 0.938931; Learning rate 0.000037;\n",
    "===========================================================\n",
    "Epoch 087/096 - Mean training loss 0.079528; Mean training F1 0.939656; Mean validation loss 0.067933; Mean validation F1 0.938853; Learning rate 0.000032;\n",
    "===========================================================\n",
    "Epoch 088/096 - Mean training loss 0.079477; Mean training F1 0.939666; Mean validation loss 0.102361; Mean validation F1 0.938822; Learning rate 0.000027;\n",
    "===========================================================\n",
    "Epoch 089/096 - Mean training loss 0.079443; Mean training F1 0.939791; Mean validation loss 0.095576; Mean validation F1 0.939020; Learning rate 0.000023;\n",
    "===========================================================\n",
    "Epoch 090/096 - Mean training loss 0.079443; Mean training F1 0.939697; Mean validation loss 0.068304; Mean validation F1 0.939013; Learning rate 0.000020;\n",
    "===========================================================\n",
    "Epoch 091/096 - Mean training loss 0.079419; Mean training F1 0.939731; Mean validation loss 0.113775; Mean validation F1 0.938985; Learning rate 0.000017;\n",
    "===========================================================\n",
    "Epoch 092/096 - Mean training loss 0.079393; Mean training F1 0.939788; Mean validation loss 0.073489; Mean validation F1 0.938988; Learning rate 0.000015;\n",
    "===========================================================\n",
    "Epoch 093/096 - Mean training loss 0.079398; Mean training F1 0.939736; Mean validation loss 0.049179; Mean validation F1 0.938898; Learning rate 0.000013;\n",
    "===========================================================\n",
    "Epoch 094/096 - Mean training loss 0.079379; Mean training F1 0.939751; Mean validation loss 0.055850; Mean validation F1 0.939055; Learning rate 0.000011;\n",
    "===========================================================\n",
    "Epoch 095/096 - Mean training loss 0.079357; Mean training F1 0.939751; Mean validation loss 0.065192; Mean validation F1 0.938986; Learning rate 0.000010;\n",
    "===========================================================\n",
    "Epoch 096/096 - Mean training loss 0.079357; Mean training F1 0.939769; Mean validation loss 0.095364; Mean validation F1 0.938918; Learning rate 0.000010;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
