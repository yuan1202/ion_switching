{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import hann\n",
    "from scipy.signal import convolve\n",
    "from scipy.signal import welch, find_peaks\n",
    "from scipy import stats\n",
    "from scipy.special import entr\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import percentileofscore\n",
    "from tsfresh.feature_extraction import feature_calculators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_trn = pd.read_csv('../input/train_clean.csv')\n",
    "pdf_tst = pd.read_csv('../input/test_clean.csv')\n",
    "\n",
    "with open('../input/batch_ids_trn.pkl', 'rb') as f:\n",
    "    batch_id_trn = pickle.load(f)\n",
    "with open('../input/batch_ids_tst.pkl', 'rb') as f:\n",
    "    batch_id_tst = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dat = pdf_trn['signal'].values\n",
    "trb_lbl = pdf_trn['open_channels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g0_dat = np.concatenate([trn_dat[batch_id_trn[i]] for i in (0, 1,)], axis=0)\n",
    "g0_lbl = np.concatenate([trb_lbl[batch_id_trn[i]] for i in (0, 1,)], axis=0)\n",
    "g1_dat = np.concatenate([trn_dat[batch_id_trn[i]] for i in (2, 6,)], axis=0)\n",
    "g1_lbl = np.concatenate([trb_lbl[batch_id_trn[i]] for i in (2, 6,)], axis=0)\n",
    "g2_dat = np.concatenate([trn_dat[batch_id_trn[i]] for i in (3, 7,)], axis=0)\n",
    "g2_lbl = np.concatenate([trb_lbl[batch_id_trn[i]] for i in (3, 7,)], axis=0)\n",
    "g3_dat = np.concatenate([trn_dat[batch_id_trn[i]] for i in (5, 8,)], axis=0)\n",
    "g3_lbl = np.concatenate([trb_lbl[batch_id_trn[i]] for i in (5, 8,)], axis=0)\n",
    "g4_dat = np.concatenate([trn_dat[batch_id_trn[i]] for i in (4, 9,)], axis=0)\n",
    "g4_lbl = np.concatenate([trb_lbl[batch_id_trn[i]] for i in (4, 9,)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encode_dict = {}\n",
    "for i, (g_d, g_l) in enumerate(zip([g0_dat, g1_dat, g2_dat, g3_dat, g4_dat], [g0_lbl, g1_lbl, g2_lbl, g3_lbl, g4_lbl])):\n",
    "    unq_ls = np.unique(g_l)\n",
    "    for l in unq_ls:\n",
    "        target_encode_dict.update({str(i) + '_' + str(l): g_d[g_l == l]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/target_codes.pkl', 'wb') as f:\n",
    "    pickle.dump(target_encode_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pdf = pdf_trn.iloc[batch_id_trn[0]].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pdf['100cuts'] = pd.qcut(g_pdf['signal'], 100, labels=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stats.percentileofscore([1, 2, 3, 4], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_adjacent_stat(data, group, adjacent):\n",
    "    '''\n",
    "    data: pandas series of raw signal.\n",
    "    group: group info for each signal, i.e. same length as data.\n",
    "    skip: how adjacent to take stat.\n",
    "    '''\n",
    "    dat_wip = data.copy().reset_index(drop=True)\n",
    "    grp_wip = group.copy().reset_index(drop=True)\n",
    "    \n",
    "    skip_collection = defaultdict(lambda: np.nan)\n",
    "    for i in np.unique(grp_wip):\n",
    "        skip_collection.update(\n",
    "            {\n",
    "                i: dat_wip.shift(adjacent).loc[grp_wip == i].values\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    \n",
    "    return pd.Series([stats.percentileofscore(skip_collection[v], k) for k, v in zip(data.shift(adjacent).values, grp_wip.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = group_adjacent_stat(g_pdf['signal'], g_pdf['100cuts'], 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "backward_1_states = {}\n",
    "\n",
    "for i in range(100):\n",
    "    backward_1_states.update(\n",
    "        {\n",
    "            i: g_pdf.shift(1).loc[g_pdf['100cuts'] == i, 'signal'].values\n",
    "        }\n",
    "    )\n",
    "    \n",
    "backward_2_states = {}\n",
    "\n",
    "for i in range(100):\n",
    "    backward_2_states.update(\n",
    "        {\n",
    "            i: g_pdf.shift(2).loc[g_pdf['100cuts'] == i, 'signal'].values\n",
    "        }\n",
    "    )\n",
    "\n",
    "backward_3_states = {}\n",
    "for i in range(100):\n",
    "    backward_3_states.update(\n",
    "        {\n",
    "            i: g_pdf.shift(3).loc[g_pdf['100cuts'] == i, 'signal'].values\n",
    "        }\n",
    "    )\n",
    "    \n",
    "backward_4_states = {}\n",
    "for i in range(100):\n",
    "    backward_4_states.update(\n",
    "        {\n",
    "            i: g_pdf.shift(4).loc[g_pdf['100cuts'] == i, 'signal'].values\n",
    "        }\n",
    "    )\n",
    "    \n",
    "backward_5_states = {}\n",
    "for i in range(100):\n",
    "    backward_5_states.update(\n",
    "        {\n",
    "            i: g_pdf.shift(5).loc[g_pdf['100cuts'] == i, 'signal'].values\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bp.pack_ndarray_to_file(pre_train[features].values, '../input/trn_dat_orig_all.bp')\n",
    "bp.pack_ndarray_to_file(pre_train[target].values, '../input/trn_lbl_orig_all.bp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
