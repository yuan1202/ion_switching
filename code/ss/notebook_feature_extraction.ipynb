{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import psutil\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from functools import partial, update_wrapper\n",
    "from itertools import product\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import bloscpack as bp\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from YSMLT import utils as g_utils\n",
    "from YSMLT.series import utils as ts_utils\n",
    "\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import hann\n",
    "from scipy.signal import convolve\n",
    "from scipy.signal import welch, find_peaks\n",
    "from scipy import stats\n",
    "from scipy.special import entr\n",
    "from scipy.stats import entropy\n",
    "from tsfresh.feature_extraction import feature_calculators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_trn = pd.read_csv('../input/train_clean.csv')\n",
    "pdf_tst = pd.read_csv('../input/test_clean.csv')\n",
    "\n",
    "with open('../input/batch_ids_trn.pkl', 'rb') as f:\n",
    "    batch_id_trn = pickle.load(f)\n",
    "with open('../input/batch_ids_tst.pkl', 'rb') as f:\n",
    "    batch_id_tst = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "wndw = 500\n",
    "\n",
    "def maximum(srs): return srs.max()\n",
    "def minimum(srs): return srs.min()\n",
    "def average(srs): return srs.mean()\n",
    "def standard_deviation(srs): return srs.std()\n",
    "def mean_change_abs(srs): return srs.diff().mean()\n",
    "\n",
    "def change_rate(x):\n",
    "    if np.any(np.isnan(x)):\n",
    "        return np.nan\n",
    "    else:\n",
    "        change = (np.diff(x) / x[:-1]).values\n",
    "        change = change[np.nonzero(change)[0]]\n",
    "        change = change[~np.isnan(change)]\n",
    "        change = change[change != -np.inf]\n",
    "        change = change[change != np.inf]\n",
    "        return np.mean(change)\n",
    "\n",
    "def std_F50p(srs): return srs[:(srs.shape[0] // 2)].std()\n",
    "def std_L50p(srs): return srs[-(srs.shape[0] // 2):].std()\n",
    "def std_F10p(srs): return srs[:(srs.shape[0] // 10)].std()\n",
    "def std_L10p(srs): return srs[-(srs.shape[0] // 10):].std()\n",
    "\n",
    "def avg_F50p(srs): return srs[:(srs.shape[0] // 2)].mean()\n",
    "def avg_L50p(srs): return srs[-(srs.shape[0] // 2):].mean()\n",
    "def avg_F10p(srs): return srs[:(srs.shape[0] // 10)].mean()\n",
    "def avg_L10p(srs): return srs[-(srs.shape[0] // 10):].mean()\n",
    "\n",
    "def min_F50p(srs): return srs[:(srs.shape[0] // 2)].min()\n",
    "def min_L50p(srs): return srs[-(srs.shape[0] // 2):].min()\n",
    "def min_F10p(srs): return srs[:(srs.shape[0] // 10)].min()\n",
    "def min_L10p(srs): return srs[-(srs.shape[0] // 10):].min()\n",
    "\n",
    "def max_F50p(srs): return srs[:(srs.shape[0] // 2)].max()\n",
    "def max_L50p(srs): return srs[-(srs.shape[0] // 2):].max()\n",
    "def max_F10p(srs): return srs[:(srs.shape[0] // 10)].max()\n",
    "def max_L10p(srs): return srs[-(srs.shape[0] // 10):].max()\n",
    "\n",
    "def ratio_maxmin(srs): return np.abs(srs.max()) / max(np.abs(srs.min()), 1e-6)\n",
    "def diff_maxmin(srs): return srs.max() - srs.min()\n",
    "\n",
    "def total(srs): return srs.sum()\n",
    "def count_mid(srs): return (srs > .5 * (srs.max() + srs.min())).sum()\n",
    "\n",
    "def change_rate_F50p(srs): return change_rate(srs[:(srs.shape[0] // 2)])\n",
    "def change_rate_L50p(srs): return change_rate(srs[-(srs.shape[0] // 2):])\n",
    "def change_rate_F10p(srs): return change_rate(srs[:(srs.shape[0] // 10)])\n",
    "def change_rate_L10p(srs): return change_rate(srs[-(srs.shape[0] // 10):])\n",
    "\n",
    "def q99(srs): return srs.quantile(.99)\n",
    "def q90(srs): return srs.quantile(.90)\n",
    "def q75(srs): return srs.quantile(.75)\n",
    "def q25(srs): return srs.quantile(.25)\n",
    "def q10(srs): return srs.quantile(.10)\n",
    "def q01(srs): return srs.quantile(.01)\n",
    "\n",
    "def abs_q99(srs): return srs.abs().quantile(.99)\n",
    "def abs_q90(srs): return srs.abs().quantile(.90)\n",
    "def abs_q75(srs): return srs.abs().quantile(.75)\n",
    "def abs_q25(srs): return srs.abs().quantile(.25)\n",
    "def abs_q10(srs): return srs.abs().quantile(.10)\n",
    "def abs_q01(srs): return srs.abs().quantile(.01)\n",
    "\n",
    "def trend(srs, abs_values=False):\n",
    "    if np.any(np.isnan(srs)):\n",
    "        return np.nan\n",
    "    else:\n",
    "        srs_cp = srs.copy(deep=True)\n",
    "        srs_cp.fillna(0, inplace=True)\n",
    "        ndx = np.array(range(srs_cp.shape[0])).reshape(-1, 1)\n",
    "        if abs_values:\n",
    "            arr = srs_cp.abs().values.reshape(-1, 1)\n",
    "        else:\n",
    "            arr = srs_cp.values.reshape(-1, 1)\n",
    "\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(ndx, arr)\n",
    "\n",
    "        return lr.coef_[0][0]\n",
    "\n",
    "def abs_trend(srs): return trend(srs)\n",
    "\n",
    "def abs_average(srs): return srs.abs().mean()\n",
    "def abs_standard_deviation(srs): return srs.abs().std()\n",
    "\n",
    "def mad(srs): return srs.mad()\n",
    "def kurt(srs): return srs.kurtosis()\n",
    "def skew(srs): return srs.skew()\n",
    "def med(srs): return srs.median()\n",
    "\n",
    "def hilbert_mean(srs): return np.abs(hilbert(srs.fillna(0))).mean()\n",
    "def hann_wndw_mean(srs): return (convolve(srs, hann(150), mode='same') / sum(hann(150))).mean()\n",
    "\n",
    "def classic_sta_lta(x, length_sta, length_lta):\n",
    "    \n",
    "    x_nonan = x.fillna(-5)\n",
    "    \n",
    "    sta = np.cumsum(x_nonan ** 2)\n",
    "\n",
    "    # Convert to float\n",
    "    sta = np.require(sta, dtype=np.float)\n",
    "\n",
    "    # Copy for LTA\n",
    "    lta = sta.copy()\n",
    "\n",
    "    # Compute the STA and the LTA\n",
    "    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
    "    sta /= length_sta\n",
    "    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "    lta /= length_lta\n",
    "\n",
    "    # Pad zeros\n",
    "    sta[:length_lta - 1] = 0\n",
    "\n",
    "    # Avoid division by zero by setting zero values to tiny float\n",
    "    idx = lta < np.finfo(0.0).tiny\n",
    "    lta[idx] = np.nan\n",
    "\n",
    "    return sta / lta\n",
    "\n",
    "def sta_lta_40_2_mean(srs): return np.nanmean(classic_sta_lta(srs, (wndw // 40), (wndw // 2)))\n",
    "def sta_lta_20_4_mean(srs): return np.nanmean(classic_sta_lta(srs, (wndw // 20), (wndw // 4)))\n",
    "def sta_lta_10_8_mean(srs): return np.nanmean(classic_sta_lta(srs, (wndw // 10), (wndw // 8)))\n",
    "def sta_lta_40_30_mean(srs): return np.nanmean(classic_sta_lta(srs, (wndw // 40), (wndw // 30)))\n",
    "def sta_lta_20_15_mean(srs): return np.nanmean(classic_sta_lta(srs, (wndw // 20), (wndw // 15)))\n",
    "def sta_lta_10_5_mean(srs): return np.nanmean(classic_sta_lta(srs, (wndw // 10), (wndw // 5)))\n",
    "def sta_lta_5_2_mean(srs): return np.nanmean(classic_sta_lta(srs, (wndw // 5), (wndw // 2)))\n",
    "def sta_lta_10_1_mean(srs): return np.nanmean(classic_sta_lta(srs, (wndw // 10), (wndw // 1)))\n",
    "\n",
    "# replaced by new functions\n",
    "def exp_MA_wndw_avg(srs): return srs.ewm(span=wndw).mean().mean(skipna=True)\n",
    "def exp_MA_halfwndw_avg(srs): return srs.ewm(span=wndw//2).mean().mean(skipna=True)\n",
    "def exp_MA_quaterwndw_avg(srs): return srs.ewm(span=wndw//4).mean().mean(skipna=True)\n",
    "\n",
    "def MA_5th_wndw_avg(srs): return srs.rolling(window=wndw//5).mean().mean(skipna=True)\n",
    "def MA_5th_wndw_std_avg(srs): return srs.rolling(window=wndw//5).std().mean()\n",
    "def MA_5th_wndw_BBhigh_avg(srs): return MA_5th_wndw_avg(srs) + 3 * MA_5th_wndw_std_avg(srs)\n",
    "def MA_5th_wndw_BBlow_avg(srs): return MA_5th_wndw_avg(srs) - 3 * MA_5th_wndw_std_avg(srs)\n",
    "\n",
    "def MA_2nd_wndw_avg(srs): return srs.rolling(window=wndw//2).mean().mean(skipna=True)\n",
    "def MA_2nd_wndw_std_avg(srs): return srs.rolling(window=wndw//2).std().mean()\n",
    "def MA_2nd_wndw_BBhigh_avg(srs): return MA_2nd_wndw_avg(srs) + 3 * MA_2nd_wndw_std_avg(srs)\n",
    "def MA_2nd_wndw_BBlow_avg(srs): return MA_2nd_wndw_avg(srs) - 3 * MA_2nd_wndw_std_avg(srs)\n",
    "\n",
    "def MA_10th_wndw_avg(srs): return srs.rolling(window=wndw//10).mean().mean(skipna=True)\n",
    "def MA_10th_wndw_std_avg(srs): return srs.rolling(window=wndw//10).std().mean()\n",
    "def MA_10th_wndw_BBhigh_avg(srs): return MA_10th_wndw_avg(srs) + 3 * MA_10th_wndw_std_avg(srs)\n",
    "def MA_10th_wndw_BBlow_avg(srs): return MA_10th_wndw_avg(srs) - 3 * MA_10th_wndw_std_avg(srs)\n",
    "\n",
    "def iqr(srs): return np.subtract(*np.percentile(srs, [75, 25]))\n",
    "\n",
    "def q999(srs): return srs.quantile(.999)\n",
    "def q001(srs): return srs.quantile(.001)\n",
    "def ave10(srs): return stats.trim_mean(srs, 0.1)\n",
    "    \n",
    "def roll_std_avg(srs, window): return srs.rolling(window).std().mean()\n",
    "def roll_std_std(srs, window): return srs.rolling(window).std().std()\n",
    "def roll_std_max(srs, window): return srs.rolling(window).std().max()\n",
    "def roll_std_min(srs, window): return srs.rolling(window).std().min()\n",
    "\n",
    "# replaced by new functions\n",
    "def roll_std_q01(srs, window): return srs.rolling(window).std().dropna().quantile(.01)\n",
    "def roll_std_q05(srs, window): return srs.rolling(window).std().dropna().quantile(.05)\n",
    "def roll_std_q95(srs, window): return srs.rolling(window).std().dropna().quantile(.95)\n",
    "def roll_std_q99(srs, window): return srs.rolling(window).std().dropna().quantile(.99)\n",
    "\n",
    "def roll_std_avg_change(srs, window): return srs.rolling(window).std().diff().mean()\n",
    "def roll_std_avg_change_rate(srs, window): \n",
    "    std_ = srs.rolling(window).std().dropna()\n",
    "    std_demoniator = std_.values[:-1]\n",
    "    rate_ = std_.diff().values[1:][std_demoniator!=0] / std_demoniator[std_demoniator!=0]\n",
    "    return np.nanmean(rate_)\n",
    "\n",
    "def roll_std_abs_max(srs, window): return srs.rolling(window).std().abs().max()\n",
    "\n",
    "def roll_avg_avg(srs, window): return srs.rolling(window).mean().mean()\n",
    "def roll_avg_std(srs, window): return srs.rolling(window).mean().std()\n",
    "def roll_avg_max(srs, window): return srs.rolling(window).mean().max()\n",
    "def roll_avg_min(srs, window): return srs.rolling(window).mean().min()\n",
    "\n",
    "# replaced by new functions\n",
    "def roll_avg_q01(srs, window): return srs.rolling(window).mean().dropna().quantile(.01)\n",
    "def roll_avg_q05(srs, window): return srs.rolling(window).mean().dropna().quantile(.05)\n",
    "def roll_avg_q95(srs, window): return srs.rolling(window).mean().dropna().quantile(.95)\n",
    "def roll_avg_q99(srs, window): return srs.rolling(window).mean().dropna().quantile(.99)\n",
    "\n",
    "def roll_avg_avg_change(srs, window): return srs.rolling(window).mean().diff().mean()\n",
    "def roll_avg_avg_change_rate(srs, window): \n",
    "    std_ = srs.rolling(window).mean().dropna()\n",
    "    std_demoniator = std_.values[:-1]\n",
    "    rate_ = std_.diff().values[1:][std_demoniator!=0] / std_demoniator[std_demoniator!=0]\n",
    "    return np.nanmean(rate_)\n",
    "\n",
    "def roll_avg_abs_max(srs, window): return srs.rolling(window).mean().abs().max()\n",
    "\n",
    "def energy_density_peaks(srs): #\n",
    "    # frequency bounds in which peaks and energy levels are taken\n",
    "    bounds = (\n",
    "        (0, 200), (200, 400), (400, 600), (600, 800), (800, 1000),\n",
    "        (1000, 1200), (1000, 1400), (1400, 1600), (1600, 1800), (1800, 2000),\n",
    "        (2000, 2200), (2200, 2400), (2400, 2600), (2600, 2800), (2800, 3000),\n",
    "        (3000, 3200), (3200, 3400), (3400, 3600), (3600, 3800), (3800, 4000),\n",
    "        (4000, 4200), (4200, 4400), (4400, 4600), (4600, 4800), (4800, 5000),\n",
    "    )\n",
    "    # get energy density distribution\n",
    "    freqs, Es = welch(srs, fs=10000, nperseg=wndw)\n",
    "    # get the indicese of the maximum energy for each bound\n",
    "    bins = [np.where((freqs >= bound[0]) & (freqs < bound[1])) for bound in bounds]\n",
    "    # get corresponding bins of frequencies and energy levels\n",
    "    freq_bins = [freqs[ndcs] for ndcs in bins]\n",
    "    E_bins = [Es[ndcs] for ndcs in bins]\n",
    "    # get max energy level and corresponding frequency for each bin\n",
    "    max_ndcs = [np.argmax(E_bin) for E_bin in E_bins]\n",
    "    max_freqs = [freq_bin[ndx] for freq_bin, ndx in zip(freq_bins, max_ndcs)]\n",
    "    max_Es = [E_bin[ndx] for E_bin, ndx in zip(E_bins, max_ndcs)]\n",
    "    E_integration = [np.trapz(e, f) for f, e in zip(freq_bins, E_bins)]\n",
    "    return max_freqs + max_Es + E_integration\n",
    "\n",
    "# ---------------------\n",
    "# new\n",
    "#def hmean(srs): return stats.hmean(np.abs(srs[np.nonzero(srs)[0]]))\n",
    "def hmean(srs): return stats.hmean(srs[srs != 0].abs().fillna(0).values)\n",
    "#def gmean(srs): return stats.gmean(np.abs(srs[np.nonzero(srs)[0]]))\n",
    "def gmean(srs): return stats.gmean(srs[srs != 0].abs().fillna(0).values)\n",
    "\n",
    "def std_Fhalf(srs): return srs[:wndw//2].std()\n",
    "def std_Lhalf(srs): return srs[-wndw//2:].std()\n",
    "def avg_Fhalf(srs): return srs[:wndw//2].mean()\n",
    "def avg_Lhalf(srs): return srs[-wndw//2:].mean()\n",
    "def min_Fhalf(srs): return srs[:wndw//2].min()\n",
    "def min_Lhalf(srs): return srs[-wndw//2:].min()\n",
    "def max_Fhalf(srs): return srs[:wndw//2].max()\n",
    "def max_Lhalf(srs): return srs[-wndw//2:].max()\n",
    "\n",
    "def hann_wndw_mean_4thwndw(srs): return (convolve(srs, hann(wndw//4), mode='same') / sum(hann(150))).mean()\n",
    "def hann_wndw_mean_8thwndw(srs): return (convolve(srs, hann(wndw//8), mode='same') / sum(hann(150))).mean()\n",
    "def hann_wndw_mean_16thwndw(srs): return (convolve(srs, hann(wndw//16), mode='same') / sum(hann(150))).mean()\n",
    "\n",
    "def iqrl(srs): return np.subtract(*np.percentile(srs, [95, 5]))\n",
    "\n",
    "def rng_n6n4(srs): return feature_calculators.range_count(srs, -6, -4)\n",
    "def rng_n4n2(srs): return feature_calculators.range_count(srs, -4, -2)\n",
    "def rng_n20(srs): return feature_calculators.range_count(srs, -2, 0)\n",
    "def rng_0p2(srs): return feature_calculators.range_count(srs, 0, 2)\n",
    "def rng_p2p4(srs): return feature_calculators.range_count(srs, 2, 4)\n",
    "def rng_p4p6(srs): return feature_calculators.range_count(srs, 4, 6)\n",
    "def rng_p6p8(srs): return feature_calculators.range_count(srs, 6, 8)\n",
    "def rng_p8p10(srs): return feature_calculators.range_count(srs, 8, 10)\n",
    "\n",
    "def num_crossing_mean(srs): return feature_calculators.number_crossing_m(srs, srs.mean())\n",
    "\n",
    "def roll_std_qntl(srs, window, qntl): return srs.rolling(wndw).std().dropna().quantile(qntl)\n",
    "def roll_avg_qntl(srs, window, qntl): return srs.rolling(wndw).mean().dropna().quantile(qntl)\n",
    "\n",
    "# feat_func_list = []\n",
    "\n",
    "feat_func_list = [\n",
    "    maximum, minimum, standard_deviation, # average, \n",
    "    mean_change_abs, # abs_maximum, abs_minimum, abs_std, change_rate, \n",
    "    \n",
    "    std_F50p, std_L50p, std_F10p, std_L10p, \n",
    "    avg_F50p, avg_L50p, avg_F10p, avg_L10p, \n",
    "    min_F50p, min_L50p, min_F10p, min_L10p, \n",
    "    max_F50p, max_L50p, max_F10p, max_L10p,\n",
    "    \n",
    "    ratio_maxmin, diff_maxmin, total, count_mid,\n",
    "    \n",
    "    change_rate_F50p, change_rate_L50p, change_rate_F10p, change_rate_L10p,\n",
    "    \n",
    "    q99, q90, q75, q25, q10, q01, abs_q99, abs_q90, abs_q75, abs_q25, abs_q10, abs_q01,\n",
    "    \n",
    "    abs_average, abs_standard_deviation, # trend, abs_trend, \n",
    "    mad, kurt, skew, med,\n",
    "    hilbert_mean, hann_wndw_mean,\n",
    "    \n",
    "    sta_lta_40_2_mean, sta_lta_20_4_mean, sta_lta_10_8_mean,\n",
    "    sta_lta_40_30_mean, sta_lta_20_15_mean, sta_lta_10_5_mean,\n",
    "    sta_lta_5_2_mean, sta_lta_10_1_mean,\n",
    "    \n",
    "    MA_5th_wndw_std_avg, MA_5th_wndw_BBhigh_avg, MA_5th_wndw_BBlow_avg, \n",
    "    MA_2nd_wndw_std_avg, MA_2nd_wndw_BBhigh_avg, MA_2nd_wndw_BBlow_avg, \n",
    "    MA_10th_wndw_std_avg, MA_10th_wndw_BBhigh_avg, MA_10th_wndw_BBlow_avg, \n",
    "    \n",
    "    iqr, q999, q001, ave10,\n",
    "\n",
    "    energy_density_peaks,\n",
    "    hmean, gmean, \n",
    "    std_Fhalf, std_Lhalf, avg_Fhalf, avg_Lhalf, min_Fhalf, min_Lhalf, max_Fhalf, max_Lhalf, \n",
    "    hann_wndw_mean_4thwndw, hann_wndw_mean_8thwndw, hann_wndw_mean_16thwndw,\n",
    "    iqrl, num_crossing_mean,\n",
    "    rng_n6n4, rng_n4n2, rng_n20, rng_0p2, rng_p2p4, rng_p4p6, rng_p6p8, rng_p8p10,\n",
    "]\n",
    "\n",
    "# test_func_list = [\n",
    "#     iqrl, num_crossing_mean,\n",
    "#     rng_n6n4, rng_n4n2, rng_n20, rng_0p2, rng_p2p4, rng_p4p6, rng_p6p8, rng_p8p10\n",
    "# ]\n",
    "\n",
    "for wd in (wndw//5, wndw//10, wndw//20):\n",
    "    feat_func_list.append(g_utils.named_partial(roll_std_avg, window=wd))\n",
    "    feat_func_list.append(g_utils.named_partial(roll_std_std, window=wd))\n",
    "    feat_func_list.append(g_utils.named_partial(roll_std_max, window=wd))\n",
    "    feat_func_list.append(g_utils.named_partial(roll_std_min, window=wd))\n",
    "    feat_func_list.append(g_utils.named_partial(roll_std_avg_change, window=wd))\n",
    "    feat_func_list.append(g_utils.named_partial(roll_std_avg_change_rate, window=wd))\n",
    "    feat_func_list.append(g_utils.named_partial(roll_std_abs_max, window=wd))\n",
    "    feat_func_list.append(g_utils.named_partial(roll_avg_avg, window=wd))\n",
    "    feat_func_list.append(g_utils.named_partial(roll_avg_std, window=wd))\n",
    "    feat_func_list.append(g_utils.named_partial(roll_avg_max, window=wd))\n",
    "    feat_func_list.append(g_utils.named_partial(roll_avg_min, window=wd))\n",
    "    feat_func_list.append(g_utils.named_partial(roll_avg_avg_change, window=wd))\n",
    "    feat_func_list.append(g_utils.named_partial(roll_avg_avg_change_rate, window=wd))\n",
    "    feat_func_list.append(g_utils.named_partial(roll_avg_abs_max, window=wd))\n",
    "    \n",
    "    for p in [1, 5, 10, 20, 25, 30, 40, 50, 60, 70, 75, 80, 90, 95, 99]:\n",
    "        feat_func_list.append(g_utils.named_partial(roll_std_qntl, window=wd, qntl=p/100))\n",
    "        feat_func_list.append(g_utils.named_partial(roll_avg_qntl, window=wd, qntl=p/100))\n",
    "\n",
    "# ---------------------\n",
    "for i in range(1, 5):\n",
    "    feat_func_list.append(g_utils.named_partial(stats.kstat, n=i))\n",
    "    feat_func_list.append(g_utils.named_partial(stats.moment, moment=i))\n",
    "    \n",
    "for i in [1, 2]:\n",
    "    feat_func_list.append(g_utils.named_partial(stats.kstatvar, n=i))\n",
    "    \n",
    "def first_change_rate(srs, length): return change_rate(srs[:length])\n",
    "def last_change_rate(srs, length): return change_rate(srs[-length:]) \n",
    "    \n",
    "for slice_length in [wndw//20, wndw//10, wndw//5, wndw//2]:\n",
    "    feat_func_list.append(g_utils.named_partial(first_change_rate, length=slice_length))\n",
    "    feat_func_list.append(g_utils.named_partial(last_change_rate, length=slice_length))\n",
    "    \n",
    "def pctl(srs, p): return srs.quantile(p)\n",
    "def abs_pctl(srs, p): return srs.abs().quantile(p)\n",
    "\n",
    "for p in [1, 10, 25, 50, 75, 90, 99]:\n",
    "    feat_func_list.append(g_utils.named_partial(pctl, p=p/100))\n",
    "    feat_func_list.append(g_utils.named_partial(abs_pctl, p=p/100))\n",
    "    \n",
    "def exp_MA_avg(srs, span): return srs.ewm(span=span).mean(skipna=True).mean(skipna=True)\n",
    "def exp_MA_std(srs, span): return srs.ewm(span=span).mean(skipna=True).std(skipna=True)\n",
    "def exp_MS_avg(srs, span): return srs.ewm(span=span).std(skipna=True).mean(skipna=True)\n",
    "def exp_MS_std(srs, span): return srs.ewm(span=span).std(skipna=True).std(skipna=True)\n",
    "\n",
    "for s in [wndw//20, wndw//10, wndw//5, wndw//2]:\n",
    "    feat_func_list.append(g_utils.named_partial(exp_MA_avg, span=s))\n",
    "    feat_func_list.append(g_utils.named_partial(exp_MA_std, span=s))\n",
    "    feat_func_list.append(g_utils.named_partial(exp_MS_avg, span=s))\n",
    "    feat_func_list.append(g_utils.named_partial(exp_MS_std, span=s))\n",
    "\n",
    "borders = np.linspace(-5, 8, 10)\n",
    "for i, j in zip(borders[:-1], borders[1:]):\n",
    "    feat_func_list.append(g_utils.named_partial(feature_calculators.range_count, min=i, max=j))\n",
    "    \n",
    "for autocorr_lag in [wndw//20, wndw//10, wndw//5, wndw//2]:\n",
    "    feat_func_list.append(g_utils.named_partial(feature_calculators.autocorrelation, lag=autocorr_lag))\n",
    "    feat_func_list.append(g_utils.named_partial(feature_calculators.c3, lag=autocorr_lag))\n",
    "    \n",
    "def binned_entropy_nonan(srs, p):\n",
    "    srs_nonan = srs.copy().dropna()\n",
    "    return feature_calculators.binned_entropy(srs_nonan, max_bins=p)\n",
    "    \n",
    "for p in [10, 25, 50, 75, 90]:\n",
    "    feat_func_list.append(g_utils.named_partial(binned_entropy_nonan, p=p))\n",
    "    \n",
    "# for peak in [wndw//20, wndw//10, wndw//5]:\n",
    "#     feat_func_list.append(g_utils.named_partial(feature_calculators.number_peaks, n=peak))\n",
    "\n",
    "for c in [wndw//20, wndw//10, wndw//5, wndw//2]:\n",
    "    feat_func_list.append(g_utils.named_partial(feature_calculators.time_reversal_asymmetry_statistic, lag=c))\n",
    "\n",
    "def wrapped_spkt_welch_density(srs, c): return list(feature_calculators.spkt_welch_density(srs, [{'coeff': c}]))[0][1]\n",
    "\n",
    "for c in range(-120, 130, 10):\n",
    "    feat_func_list.append(g_utils.named_partial(wrapped_spkt_welch_density, c=c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgnl_ndcs = batch_id_trn[0]\n",
    "sgnl = pdf_trn['signal'].iloc[sgnl_ndcs]\n",
    "trgt = pdf_trn['open_channels'].iloc[sgnl_ndcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgnl_L = pd.concat([pd.Series([np.nan] * (wndw-1)), sgnl])\n",
    "sgnl_R = pd.concat([sgnl, pd.Series([np.nan] * (wndw-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction_ndcs = list(\n",
    "    ts_utils.index_sampler((0, sgnl_L.shape[0]), block_size=500, sample_ratio=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(ndx, period=500):\n",
    "    serie = sgnl_L.iloc[ndx:ndx+period]\n",
    "\n",
    "    # make sure sampling does go out of bound\n",
    "    #if ndx+period > sgnl_L.shape[0]:\n",
    "    #    return None, None, None\n",
    "\n",
    "    # get feature and name\n",
    "    feat, dat = ts_utils.feature_runner(serie, feat_func_list)\n",
    "\n",
    "    # collect label\n",
    "    lbl = trgt.iloc[ndx]\n",
    "    \n",
    "    return feat, dat, lbl, ndx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuan/miniconda3/envs/ML/lib/python3.7/site-packages/scipy/stats/stats.py:338: RuntimeWarning: divide by zero encountered in log\n",
      "  log_a = np.log(a)\n",
      "/home/yuan/miniconda3/envs/ML/lib/python3.7/site-packages/scipy/stats/stats.py:338: RuntimeWarning: divide by zero encountered in log\n",
      "  log_a = np.log(a)\n",
      "/home/yuan/miniconda3/envs/ML/lib/python3.7/site-packages/scipy/stats/stats.py:338: RuntimeWarning: divide by zero encountered in log\n",
      "  log_a = np.log(a)\n",
      "/home/yuan/miniconda3/envs/ML/lib/python3.7/site-packages/ipykernel_launcher.py:173: RuntimeWarning: Mean of empty slice\n",
      "/home/yuan/miniconda3/envs/ML/lib/python3.7/site-packages/ipykernel_launcher.py:193: RuntimeWarning: Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "with Pool(processes=14) as P:\n",
    "    trn_dat_fe = P.map_async(runner, feature_extraction_ndcs[:10000]).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.array([lst[1] for lst in trn_dat_fe])\n",
    "lbl = np.array([lst[2] for lst in trn_dat_fe])\n",
    "feat = np.array(trn_dat_fe[0][0])\n",
    "rank = np.array([lst[-1] for lst in trn_dat_fe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = np.argsort(rank)\n",
    "dat = dat[sort]\n",
    "lbl = lbl[sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 398)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp.pack_ndarray_to_file(dat, '../input/trn_dat_g8R_w500.bp')\n",
    "bp.pack_ndarray_to_file(lbl, '../input/trn_lbl_g8R_w500.bp')\n",
    "bp.pack_ndarray_to_file(feat, '../input/trn_feat_g8R_w500.bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner_v2(ndx, serie, period):\n",
    "    sr_slice = serie.iloc[ndx:ndx+period]\n",
    "    # get feature and name\n",
    "    feat, dat = ts_utils.feature_runner(serie, feat_func_list)\n",
    "    # collect label\n",
    "    lbl = trgt.iloc[ndx]\n",
    "    return feat, dat, lbl, ndx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    sgnl_ndcs = batch_id_trn[i]\n",
    "    sgnl = pdf_trn['signal'].iloc[sgnl_ndcs]\n",
    "    trgt = pdf_trn['open_channels'].iloc[sgnl_ndcs]\n",
    "    \n",
    "    sgnl_L = pd.concat([pd.Series([np.nan] * (wndw-1)), sgnl])\n",
    "    sgnl_R = pd.concat([sgnl, pd.Series([np.nan] * (wndw-1))])\n",
    "    del sngl\n",
    "    \n",
    "    # ------------------------\n",
    "    # run left\n",
    "    with Pool(processes=12) as P:\n",
    "        extracted = P.map_async(partial(runner_v2, serie=sgnl_L, period=wndw), feature_extraction_ndcs).get()\n",
    "        \n",
    "    # post-process\n",
    "    feat_L = [n + '_L' for n in extracted[0][0]]\n",
    "    dat_L = np.array([lst[1] for lst in extracted])\n",
    "    lbl_L = np.array([lst[2] for lst in extracted])\n",
    "    rank_L = np.array([lst[-1] for lst in extracted])\n",
    "    \n",
    "    sort_L = np.argsort(rank_L)\n",
    "    dat_L = dat_L[sort_L]\n",
    "    lbl_L = lbl_L[sort_L]\n",
    "    \n",
    "    del extracted\n",
    "    \n",
    "    # ------------------------\n",
    "    # run right\n",
    "    with Pool(processes=12) as P:\n",
    "        extracted = P.map_async(partial(runner_v2, serie=sgnl_R, period=wndw), feature_extraction_ndcs).get()\n",
    "        \n",
    "    # post-process\n",
    "    feat_R = [n + '_R' for n in extracted[0][0]]\n",
    "    dat_R = np.array([lst[1] for lst in extracted])\n",
    "    lbl_R = np.array([lst[2] for lst in extracted])\n",
    "    rank_R = np.array([lst[-1] for lst in extracted])\n",
    "    \n",
    "    sort_R = np.argsort(rank_R)\n",
    "    dat_R = dat_L[sort_R]\n",
    "    lbl_R = lbl_L[sort_R]\n",
    "    \n",
    "    # ------------------------\n",
    "    # some checks\n",
    "    assert np.all(lbl_L == lbl_R)\n",
    "    feat = np.array(feat_L + feat_R)\n",
    "    dat = np.concatenate([dat_L, dat_R], axis=1)\n",
    "    lbl = lbl_L\n",
    "    \n",
    "    # ------------------------\n",
    "    # save and clean up\n",
    "    bp.pack_ndarray_to_file(feat, '../input/trn_feat_g{:d}_w{:d}.bp'.format(i, wndw))\n",
    "    bp.pack_ndarray_to_file(dat, '../input/trn_dat_g{:d}_w{:d}.bp'.format(i, wndw))\n",
    "    bp.pack_ndarray_to_file(lbl, '../input/trn_lbl_g{:d}_w{:d}.bp'.format(i, wndw))\n",
    "    \n",
    "    del feat_L, dat_L, lbl_L, rank_L, sort_L\n",
    "    del feat_R, dat_R, lbl_R, rank_R, sort_R\n",
    "    del feat, dat, lbl\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tst_func0 = lambda sr: [sr.min(), sr.max()]\n",
    "tst_func1 = lambda sr: sr.var()\n",
    "a = ts_utils.serie_stacker(sgnl_R, step_size=1, block_size=wndw, feature_extractor=[tst_func0, tst_func1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "freqs, Es = welch(sgnl_L[5000:6000], fs=10000, nperseg=1000)\n",
    "plt.bar(freqs, Es)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pdf_all = pd.concat([pdf_trn, pdf_tst], axis=0).reset_index(drop=True)\n",
    "pdf_all['signal_clean_norm'] = (pdf_all['signal_clean'] - pdf_all['signal_clean'].mean()) / pdf_all['signal_clean'].std()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_id_trn = {i: range(i*500000, (i+1)*500000) for i in range(10)}\n",
    "batch_id_tst = {i: range(i*500000, (i+1)*500000) for i in range(4)}\n",
    "print(batch_id_trn, batch_id_tst)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../input/batch_ids_trn.pkl', 'wb') as f:\n",
    "    pickle.dump(batch_id_trn, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../input/batch_ids_tst.pkl', 'wb') as f:\n",
    "    pickle.dump(batch_id_tst, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = ts_utils.serie_stacker(pdf_trn['signal_clean'], step_size=1, block_size=20, feature_extractor=[])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def maximum(srs): return srs.max()\n",
    "def minimum(srs): return srs.min()\n",
    "def average(srs): return srs.mean()\n",
    "def standard_deviation(srs): return srs.std()\n",
    "def mean_change_abs(srs): return srs.diff().mean()\n",
    "\n",
    "# def change_rate(x):\n",
    "#     change = (np.diff(x) / x[:-1]).values\n",
    "#     change = change[np.nonzero(change)[0]]\n",
    "#     change = change[~np.isnan(change)]\n",
    "#     change = change[change != -np.inf]\n",
    "#     change = change[change != np.inf]\n",
    "#     return np.mean(change)\n",
    "\n",
    "# def abs_maximum(srs): return srs.abs().max()\n",
    "# def abs_minimum(srs): return srs.abs().min()\n",
    "# def abs_std(srs): return srs.abs().std()\n",
    "\n",
    "def std_F50p(srs): return srs[:(srs.shape[0] // 2)].std()\n",
    "def std_L50p(srs): return srs[-(srs.shape[0] // 2):].std()\n",
    "def std_F10p(srs): return srs[:(srs.shape[0] // 10)].std()\n",
    "def std_L10p(srs): return srs[-(srs.shape[0] // 10):].std()\n",
    "\n",
    "def avg_F50p(srs): return srs[:(srs.shape[0] // 2)].mean()\n",
    "def avg_L50p(srs): return srs[-(srs.shape[0] // 2):].mean()\n",
    "def avg_F10p(srs): return srs[:(srs.shape[0] // 10)].mean()\n",
    "def avg_L10p(srs): return srs[-(srs.shape[0] // 10):].mean()\n",
    "\n",
    "def min_F50p(srs): return srs[:(srs.shape[0] // 2)].min()\n",
    "def min_L50p(srs): return srs[-(srs.shape[0] // 2):].min()\n",
    "def min_F10p(srs): return srs[:(srs.shape[0] // 10)].min()\n",
    "def min_L10p(srs): return srs[-(srs.shape[0] // 10):].min()\n",
    "\n",
    "def max_F50p(srs): return srs[:(srs.shape[0] // 2)].max()\n",
    "def max_L50p(srs): return srs[-(srs.shape[0] // 2):].max()\n",
    "def max_F10p(srs): return srs[:(srs.shape[0] // 10)].max()\n",
    "def max_L10p(srs): return srs[-(srs.shape[0] // 10):].max()\n",
    "\n",
    "def ratio_maxmin(srs): return np.abs(srs.max()) / max(np.abs(srs.min()), 1e-6)\n",
    "def diff_maxmin(srs): return srs.max() - srs.min()\n",
    "\n",
    "def total(srs): return srs.sum()\n",
    "def count_mid(srs): return (srs > .5 * (srs.max() + srs.min())).sum()\n",
    "\n",
    "# def change_rate_F50p(srs): return change_rate(srs[:(srs.shape[0] // 2)])\n",
    "# def change_rate_L50p(srs): return change_rate(srs[-(srs.shape[0] // 2):])\n",
    "# def change_rate_F10p(srs): return change_rate(srs[:(srs.shape[0] // 10)])\n",
    "# def change_rate_L10p(srs): return change_rate(srs[-(srs.shape[0] // 10):])\n",
    "\n",
    "def q99(srs): return srs.quantile(.99)\n",
    "def q90(srs): return srs.quantile(.90)\n",
    "def q75(srs): return srs.quantile(.75)\n",
    "def q25(srs): return srs.quantile(.25)\n",
    "def q10(srs): return srs.quantile(.10)\n",
    "def q01(srs): return srs.quantile(.01)\n",
    "\n",
    "def abs_q99(srs): return srs.abs().quantile(.99)\n",
    "def abs_q90(srs): return srs.abs().quantile(.90)\n",
    "def abs_q75(srs): return srs.abs().quantile(.75)\n",
    "def abs_q25(srs): return srs.abs().quantile(.25)\n",
    "def abs_q10(srs): return srs.abs().quantile(.10)\n",
    "def abs_q01(srs): return srs.abs().quantile(.01)\n",
    "\n",
    "# def trend(srs, abs_values=False):\n",
    "#     srs_cp = srs.copy(deep=True)\n",
    "#     srs_cp.fillna(0, inplace=True)\n",
    "#     ndx = np.array(range(srs_cp.shape[0])).reshape(-1, 1)\n",
    "#     if abs_values:\n",
    "#         arr = srs_cp.abs().values.reshape(-1, 1)\n",
    "#     else:\n",
    "#         arr = srs_cp.values.reshape(-1, 1)\n",
    "        \n",
    "#     lr = LinearRegression()\n",
    "#     lr.fit(ndx, arr)\n",
    "    \n",
    "#     return lr.coef_[0][0]\n",
    "\n",
    "# def abs_trend(srs): return trend(srs)\n",
    "\n",
    "def abs_average(srs): return srs.abs().mean()\n",
    "def abs_standard_deviation(srs): return srs.abs().std()\n",
    "\n",
    "def mad(srs): return srs.mad()\n",
    "def kurt(srs): return srs.kurtosis()\n",
    "def skew(srs): return srs.skew()\n",
    "def med(srs): return srs.median()\n",
    "\n",
    "def hilbert_mean(srs): return np.abs(hilbert(srs.fillna(0))).mean()\n",
    "def hann_wndw_mean(srs): return (convolve(srs, hann(150), mode='same') / sum(hann(150))).mean()\n",
    "\n",
    "def classic_sta_lta(x, length_sta, length_lta):\n",
    "    \n",
    "    x_nonan = x.fillna(-5)\n",
    "    \n",
    "    sta = np.cumsum(x_nonan ** 2)\n",
    "\n",
    "    # Convert to float\n",
    "    sta = np.require(sta, dtype=np.float)\n",
    "\n",
    "    # Copy for LTA\n",
    "    lta = sta.copy()\n",
    "\n",
    "    # Compute the STA and the LTA\n",
    "    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
    "    sta /= length_sta\n",
    "    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "    lta /= length_lta\n",
    "\n",
    "    # Pad zeros\n",
    "    sta[:length_lta - 1] = 0\n",
    "\n",
    "    # Avoid division by zero by setting zero values to tiny float\n",
    "    idx = lta < np.finfo(0.0).tiny\n",
    "    lta[idx] = np.nan\n",
    "\n",
    "    return sta / lta\n",
    "\n",
    "def sta_lta_20_2_mean(srs): return np.nanmean(classic_sta_lta(srs, (srs.shape[0] // 20), (srs.shape[0] // 2)))\n",
    "def sta_lta_30_3_mean(srs): return np.nanmean(classic_sta_lta(srs, (srs.shape[0] // 30), (srs.shape[0] // 3)))\n",
    "def sta_lta_10_4_mean(srs): return np.nanmean(classic_sta_lta(srs, (srs.shape[0] // 10), (srs.shape[0] // 4)))\n",
    "def sta_lta_5_2_mean(srs): return np.nanmean(classic_sta_lta(srs, (srs.shape[0] // 5), (srs.shape[0] // 2)))\n",
    "def sta_lta_50_20_mean(srs): return np.nanmean(classic_sta_lta(srs, (srs.shape[0] // 50), (srs.shape[0] // 20)))\n",
    "def sta_lta_80_70_mean(srs): return np.nanmean(classic_sta_lta(srs, (srs.shape[0] // 80), (srs.shape[0] // 70)))\n",
    "def sta_lta_30_15_mean(srs): return np.nanmean(classic_sta_lta(srs, (srs.shape[0] // 30), (srs.shape[0] // 15)))\n",
    "def sta_lta_200_100_mean(srs): return np.nanmean(classic_sta_lta(srs, (srs.shape[0] // 200), (srs.shape[0] // 100)))\n",
    "\n",
    "# replaced by new functions\n",
    "#def exp_MA_300_avg(srs): return srs.ewm(span=300).mean().mean(skipna=True)\n",
    "#def exp_MA_3k_avg(srs): return srs.ewm(span=3000).mean().mean(skipna=True)\n",
    "#def exp_MA_30k_avg(srs): return srs.ewm(span=30000).mean().mean(skipna=True)\n",
    "\n",
    "# def MA_500_avg(srs): return srs.rolling(window=500).mean().mean(skipna=True)\n",
    "# def MA_500_std_avg(srs): return srs.rolling(window=500).std().mean()\n",
    "# def MA_500_BBhigh_avg(srs): return MA_500_avg(srs) + 3 * MA_500_std_avg(srs)\n",
    "# def MA_500_BBlow_avg(srs): return MA_500_avg(srs) - 3 * MA_500_std_avg(srs)\n",
    "\n",
    "# def MA_300_avg(srs): return srs.rolling(window=300).mean().mean(skipna=True)\n",
    "# def MA_300_std_avg(srs): return srs.rolling(window=300).std().mean()\n",
    "# def MA_300_BBhigh_avg(srs): return MA_300_avg(srs) + 3 * MA_300_std_avg(srs)\n",
    "# def MA_300_BBlow_avg(srs): return MA_300_avg(srs) - 3 * MA_300_std_avg(srs)\n",
    "\n",
    "# def MA_100_avg(srs): return srs.rolling(window=100).mean().mean(skipna=True)\n",
    "# def MA_100_std_avg(srs): return srs.rolling(window=100).std().mean()\n",
    "# def MA_100_BBhigh_avg(srs): return MA_100_avg(srs) + 3 * MA_100_std_avg(srs)\n",
    "# def MA_100_BBlow_avg(srs): return MA_100_avg(srs) - 3 * MA_100_std_avg(srs)\n",
    "\n",
    "# def MA_1k_std_avg(srs): return srs.rolling(window=1000).std().mean()\n",
    "\n",
    "def iqr(srs): return np.subtract(*np.percentile(srs, [75, 25]))\n",
    "\n",
    "def q999(srs): return srs.quantile(.999)\n",
    "def q001(srs): return srs.quantile(.001)\n",
    "def ave10(srs): return stats.trim_mean(srs, 0.1)\n",
    "    \n",
    "def roll_std_avg(srs, wndw): return srs.rolling(wndw).std().mean()\n",
    "def roll_std_std(srs, wndw): return srs.rolling(wndw).std().std()\n",
    "def roll_std_max(srs, wndw): return srs.rolling(wndw).std().max()\n",
    "def roll_std_min(srs, wndw): return srs.rolling(wndw).std().min()\n",
    "\n",
    "# replaced by new functions\n",
    "#def roll_std_q01(srs, wndw): return srs.rolling(wndw).std().dropna().quantile(.01)\n",
    "#def roll_std_q05(srs, wndw): return srs.rolling(wndw).std().dropna().quantile(.05)\n",
    "#def roll_std_q95(srs, wndw): return srs.rolling(wndw).std().dropna().quantile(.95)\n",
    "#def roll_std_q99(srs, wndw): return srs.rolling(wndw).std().dropna().quantile(.99)\n",
    "\n",
    "def roll_std_avg_change(srs, wndw): return srs.rolling(wndw).std().diff().mean()\n",
    "def roll_std_avg_change_rate(srs, wndw): \n",
    "    std_ = srs.rolling(wndw).std().dropna()\n",
    "    std_demoniator = std_.values[:-1]\n",
    "    rate_ = std_.diff().values[1:][std_demoniator!=0] / std_demoniator[std_demoniator!=0]\n",
    "    return np.nanmean(rate_)\n",
    "\n",
    "def roll_std_abs_max(srs, wndw): return srs.rolling(wndw).std().abs().max()\n",
    "\n",
    "def roll_avg_avg(srs, wndw): return srs.rolling(wndw).mean().mean()\n",
    "def roll_avg_std(srs, wndw): return srs.rolling(wndw).mean().std()\n",
    "def roll_avg_max(srs, wndw): return srs.rolling(wndw).mean().max()\n",
    "def roll_avg_min(srs, wndw): return srs.rolling(wndw).mean().min()\n",
    "\n",
    "# replaced by new functions\n",
    "#def roll_avg_q01(srs, wndw): return srs.rolling(wndw).mean().dropna().quantile(.01)\n",
    "#def roll_avg_q05(srs, wndw): return srs.rolling(wndw).mean().dropna().quantile(.05)\n",
    "#def roll_avg_q95(srs, wndw): return srs.rolling(wndw).mean().dropna().quantile(.95)\n",
    "#def roll_avg_q99(srs, wndw): return srs.rolling(wndw).mean().dropna().quantile(.99)\n",
    "\n",
    "def roll_avg_avg_change(srs, wndw): return srs.rolling(wndw).mean().diff().mean()\n",
    "def roll_avg_avg_change_rate(srs, wndw): \n",
    "    std_ = srs.rolling(wndw).mean().dropna()\n",
    "    std_demoniator = std_.values[:-1]\n",
    "    rate_ = std_.diff().values[1:][std_demoniator!=0] / std_demoniator[std_demoniator!=0]\n",
    "    return np.nanmean(rate_)\n",
    "\n",
    "def roll_avg_abs_max(srs, wndw): return srs.rolling(wndw).mean().abs().max()\n",
    "\n",
    "def energy_density_peaks(srs): #\n",
    "    # frequency bounds in which peaks and energy levels are taken\n",
    "    bounds = (\n",
    "        (0, 200), (200, 400), (400, 600), (600, 800), (800, 1000),\n",
    "        (1000, 1200), (1000, 1400), (1400, 1600), (1600, 1800), (1800, 2000),\n",
    "        (2000, 2200), (2200, 2400), (2400, 2600), (2600, 2800), (2800, 3000),\n",
    "        (3000, 3200), (3200, 3400), (3400, 3600), (3600, 3800), (3800, 4000),\n",
    "        (4000, 4200), (4200, 4400), (4400, 4600), (4600, 4800), (4800, 5000),\n",
    "    )\n",
    "    # get energy density distribution\n",
    "    freqs, Es = welch(srs, fs=10000, nperseg=500)\n",
    "    # get the indicese of the maximum energy for each bound\n",
    "    bins = [np.where((freqs >= bound[0]) & (freqs < bound[1])) for bound in bounds]\n",
    "    # get corresponding bins of frequencies and energy levels\n",
    "    freq_bins = [freqs[ndcs] for ndcs in bins]\n",
    "    E_bins = [Es[ndcs] for ndcs in bins]\n",
    "    # get max energy level and corresponding frequency for each bin\n",
    "    max_ndcs = [np.argmax(E_bin) for E_bin in E_bins]\n",
    "    max_freqs = [freq_bin[ndx] for freq_bin, ndx in zip(freq_bins, max_ndcs)]\n",
    "    max_Es = [E_bin[ndx] for E_bin, ndx in zip(E_bins, max_ndcs)]\n",
    "    E_integration = [np.trapz(e, f) for f, e in zip(freq_bins, E_bins)]\n",
    "    return max_freqs + max_Es + E_integration\n",
    "\n",
    "# ---------------------\n",
    "# new\n",
    "#def hmean(srs): return stats.hmean(np.abs(srs[np.nonzero(srs)[0]]))\n",
    "def hmean(srs): return stats.hmean(srs[srs != 0].abs().fillna(0).values)\n",
    "#def gmean(srs): return stats.gmean(np.abs(srs[np.nonzero(srs)[0]]))\n",
    "def gmean(srs): return stats.gmean(srs[srs != 0].abs().fillna(0).values)\n",
    "\n",
    "def std_F500(srs): return srs[:500].std()\n",
    "def std_L500(srs): return srs[-500:].std()\n",
    "def avg_F500(srs): return srs[:500].mean()\n",
    "def avg_L500(srs): return srs[-500:].mean()\n",
    "def min_F500(srs): return srs[:500].min()\n",
    "def min_L500(srs): return srs[-500:].min()\n",
    "def max_F500(srs): return srs[:500].max()\n",
    "def max_L500(srs): return srs[-500:].max()\n",
    "\n",
    "def hann_wndw_mean_50(srs): return (convolve(srs, hann(50), mode='same') / sum(hann(150))).mean()\n",
    "def hann_wndw_mean_250(srs): return (convolve(srs, hann(250), mode='same') / sum(hann(150))).mean()\n",
    "def hann_wndw_mean_500(srs): return (convolve(srs, hann(500), mode='same') / sum(hann(150))).mean()\n",
    "\n",
    "def iqrl(srs): return np.subtract(*np.percentile(srs, [95, 5]))\n",
    "\n",
    "def rng_n6n4(srs): return feature_calculators.range_count(srs, -6, -4)\n",
    "def rng_n4n2(srs): return feature_calculators.range_count(srs, -4, -2)\n",
    "def rng_n20(srs): return feature_calculators.range_count(srs, -2, 0)\n",
    "def rng_0p2(srs): return feature_calculators.range_count(srs, 0, 2)\n",
    "def rng_p2p4(srs): return feature_calculators.range_count(srs, 2, 4)\n",
    "def rng_p4p6(srs): return feature_calculators.range_count(srs, 4, 6)\n",
    "def rng_p6p8(srs): return feature_calculators.range_count(srs, 6, 8)\n",
    "def rng_p8p10(srs): return feature_calculators.range_count(srs, 8, 10)\n",
    "\n",
    "def num_crossing_mean(srs): return feature_calculators.number_crossing_m(srs, srs.mean())\n",
    "\n",
    "def roll_std_qntl(srs, wndw, qntl): return srs.rolling(wndw).std().dropna().quantile(qntl)\n",
    "def roll_avg_qntl(srs, wndw, qntl): return srs.rolling(wndw).mean().dropna().quantile(qntl)\n",
    "\n",
    "\n",
    "feat_func_list = [\n",
    "    maximum, minimum, standard_deviation, # average, \n",
    "    mean_change_abs, # abs_maximum, abs_minimum, abs_std, change_rate, \n",
    "    std_F50p, std_L50p, std_F10p, std_L10p, # avg_F50k, avg_L50k,\n",
    "    min_F50p, min_L50p, min_F10p, min_L10p, # avg_F10k, avg_L10k, \n",
    "    max_F50p, max_L50p, max_F10p, max_L10p, \n",
    "    ratio_maxmin, diff_maxmin, total, count_mid,\n",
    "    #change_rate_F50k, change_rate_L50k, change_rate_F10k, change_rate_L10k,\n",
    "    q99, q90, q75, q25, q10, q01, abs_q99, abs_q90, abs_q75, abs_q25, abs_q10, abs_q01,\n",
    "    abs_average, abs_standard_deviation, # trend, abs_trend, \n",
    "    mad, kurt, skew, med,\n",
    "    hilbert_mean, hann_wndw_mean,\n",
    "    sta_lta_20_2_mean, sta_lta_30_3_mean, sta_lta_10_4_mean,\n",
    "    sta_lta_5_2_mean, sta_lta_50_20_mean, sta_lta_80_70_mean,\n",
    "    sta_lta_30_15_mean, sta_lta_200_100_mean,\n",
    "    #MA_500_std_avg, MA_500_BBhigh_avg, MA_500_BBlow_avg, \n",
    "    #MA_300_std_avg, MA_300_BBhigh_avg, MA_300_BBlow_avg, \n",
    "    #MA_100_std_avg, MA_100_BBhigh_avg, MA_100_BBlow_avg, \n",
    "    #MA_1k_std_avg,\n",
    "    iqr, q999, q001, ave10,\n",
    "\n",
    "    energy_density_peaks,\n",
    "    hmean, gmean, std_F500, std_L500, min_F500, min_L500, max_F500, max_L500, # avg_F1k, avg_L1k, \n",
    "    hann_wndw_mean_50, hann_wndw_mean_250, hann_wndw_mean_500,\n",
    "    iqrl, num_crossing_mean,\n",
    "    rng_n6n4, rng_n4n2, rng_n20, rng_0p2, rng_p2p4, rng_p4p6, rng_p6p8, rng_p8p10\n",
    "]\n",
    "\n",
    "# test_func_list = [\n",
    "#     iqrl, num_crossing_mean,\n",
    "#     rng_n6n4, rng_n4n2, rng_n20, rng_0p2, rng_p2p4, rng_p4p6, rng_p6p8, rng_p8p10\n",
    "# ]\n",
    "\n",
    "# for wd in (10, 100, 1000):\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_std_avg, wndw=wd))\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_std_std, wndw=wd))\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_std_max, wndw=wd))\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_std_min, wndw=wd))\n",
    "#     #feat_func_list.append(g_utils.named_partial(roll_std_q01, wndw=wd))\n",
    "#     #feat_func_list.append(g_utils.named_partial(roll_std_q05, wndw=wd))\n",
    "#     #feat_func_list.append(g_utils.named_partial(roll_std_q95, wndw=wd))\n",
    "#     #feat_func_list.append(g_utils.named_partial(roll_std_q99, wndw=wd))\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_std_avg_change, wndw=wd))\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_std_avg_change_rate, wndw=wd))\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_std_abs_max, wndw=wd))\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_avg_avg, wndw=wd))\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_avg_std, wndw=wd))\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_avg_max, wndw=wd))\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_avg_min, wndw=wd))\n",
    "#     #feat_func_list.append(named_partial(roll_avg_q01, wndw=wd))\n",
    "#     #feat_func_list.append(named_partial(roll_avg_q05, wndw=wd))\n",
    "#     #feat_func_list.append(named_partial(roll_avg_q95, wndw=wd))\n",
    "#     #feat_func_list.append(named_partial(roll_avg_q99, wndw=wd))\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_avg_avg_change, wndw=wd))\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_avg_avg_change_rate, wndw=wd))\n",
    "#     feat_func_list.append(g_utils.named_partial(roll_avg_abs_max, wndw=wd))\n",
    "    \n",
    "#     for p in [1, 5, 10, 20, 25, 30, 40, 50, 60, 70, 75, 80, 90, 95, 99]:\n",
    "#         feat_func_list.append(g_utils.named_partial(roll_std_qntl, wndw=wd, qntl=p/100))\n",
    "#         feat_func_list.append(g_utils.named_partial(roll_avg_qntl, wndw=wd, qntl=p/100))\n",
    "\n",
    "# # ---------------------\n",
    "# # new\n",
    "# for i in range(1, 5):\n",
    "#     feat_func_list.append(g_utils.named_partial(stats.kstat, n=i))\n",
    "#     feat_func_list.append(g_utils.named_partial(stats.moment, moment=i))\n",
    "    \n",
    "# for i in [1, 2]:\n",
    "#     feat_func_list.append(g_utils.named_partial(stats.kstatvar, n=i))\n",
    "    \n",
    "# def first_change_rate(srs, length): return change_rate(srs[:length])\n",
    "# def last_change_rate(srs, length): return change_rate(srs[-length:]) \n",
    "    \n",
    "# for slice_length in [1000, 10000, 50000]:\n",
    "#     feat_func_list.append(g_utils.named_partial(first_change_rate, length=slice_length))\n",
    "#     feat_func_list.append(g_utils.named_partial(last_change_rate, length=slice_length))\n",
    "    \n",
    "# def pctl(srs, p): return srs.quantile(p)\n",
    "# def abs_pctl(srs, p): return srs.abs().quantile(p)\n",
    "\n",
    "# for p in [1, 5, 10, 20, 25, 30, 40, 50, 60, 70, 75, 80, 90, 95, 99]:\n",
    "#     feat_func_list.append(g_utils.named_partial(pctl, p=p/100))\n",
    "#     feat_func_list.append(g_utils.named_partial(abs_pctl, p=p/100))\n",
    "    \n",
    "# def exp_MA_avg(srs, span): return srs.ewm(span=span).mean(skipna=True).mean(skipna=True)\n",
    "# def exp_MA_std(srs, span): return srs.ewm(span=span).mean(skipna=True).std(skipna=True)\n",
    "# def exp_MS_avg(srs, span): return srs.ewm(span=span).std(skipna=True).mean(skipna=True)\n",
    "# def exp_MS_std(srs, span): return srs.ewm(span=span).std(skipna=True).std(skipna=True)\n",
    "\n",
    "# for s in [300, 3000, 30000, 50000]:\n",
    "#     feat_func_list.append(g_utils.named_partial(exp_MA_avg, span=s))\n",
    "#     feat_func_list.append(g_utils.named_partial(exp_MA_std, span=s))\n",
    "#     feat_func_list.append(g_utils.named_partial(exp_MS_avg, span=s))\n",
    "#     feat_func_list.append(g_utils.named_partial(exp_MS_std, span=s))\n",
    "\n",
    "# def count_big_threshold(srs, length, threshold): return (np.abs(srs[-length:]) > threshold).sum()\n",
    "# def count_big_less_threshold(srs, length, threshold): return (np.abs(srs[-length:]) < threshold).sum()\n",
    "\n",
    "# for slice_length, threshold in product([50000, 100000, 150000], [5, 10, 20, 50, 100]):\n",
    "#     feat_func_list.append(g_utils.named_partial(count_big_threshold, length=slice_length, threshold=threshold))\n",
    "#     feat_func_list.append(g_utils.named_partial(count_big_less_threshold, length=slice_length, threshold=threshold))\n",
    "    \n",
    "# borders = list(range(-4000, 4001, 1000))\n",
    "# for i, j in zip(borders, borders[1:]):\n",
    "#     feat_func_list.append(g_utils.named_partial(feature_calculators.range_count, min=i, max=j))\n",
    "    \n",
    "# for autocorr_lag in [5, 10, 50, 100, 500, 1000, 5000, 10000]:\n",
    "#     feat_func_list.append(g_utils.named_partial(feature_calculators.autocorrelation, lag=autocorr_lag))\n",
    "#     feat_func_list.append(g_utils.named_partial(feature_calculators.c3, lag=autocorr_lag))\n",
    "    \n",
    "# for p in [1, 5, 10, 20, 25, 30, 40, 50, 60, 70, 75, 80, 90, 95, 99]:\n",
    "#     feat_func_list.append(g_utils.named_partial(feature_calculators.binned_entropy, max_bins=p))\n",
    "    \n",
    "# for peak in [10, 20, 50, 100]:\n",
    "#     feat_func_list.append(g_utils.named_partial(feature_calculators.number_peaks, n=peak))\n",
    "\n",
    "# def wrapped_spkt_welch_density(srs, c): return list(feature_calculators.spkt_welch_density(srs, [{'coeff': c}]))[0][1]\n",
    "\n",
    "# for c in [1, 5, 10, 50, 100]:\n",
    "#     feat_func_list.append(g_utils.named_partial(feature_calculators.time_reversal_asymmetry_statistic, lag=c))\n",
    "    \n",
    "# for c in range(-120, 130, 10):\n",
    "#     feat_func_list.append(g_utils.named_partial(wrapped_spkt_welch_density, c=c))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
